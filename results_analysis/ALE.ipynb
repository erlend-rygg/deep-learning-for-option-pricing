{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_colab = False\n",
    "lags = 140\n",
    "training_size = 991232\n",
    "random_seed = 0\n",
    "\n",
    "bs_vars = ['Underlying_last', 'Strike', 'TTM', 'R']\n",
    "underlying_lags = [f'Underlying_{i}' for i in range(lags - 1, 0, -1)] + ['Underlying_return']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if google_colab:\n",
    "    import tensorflow as tf\n",
    "    # Print info\n",
    "    gpu_info = !nvidia-smi\n",
    "    gpu_info = '\\n'.join(gpu_info)\n",
    "    if gpu_info.find('failed') >= 0:\n",
    "        print('Not connected to a GPU')\n",
    "    else:\n",
    "        print(gpu_info)\n",
    "    \n",
    "    from psutil import virtual_memory\n",
    "    ram_gb = virtual_memory().total / 1e9\n",
    "    print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "    if ram_gb < 20:\n",
    "        print('Not using a high-RAM runtime')\n",
    "    else:\n",
    "        print('You are using a high-RAM runtime!')\n",
    "\n",
    "    # Code to read csv file into Colaboratory:\n",
    "    !pip install -U -q PyDrive\n",
    "    from pydrive.auth import GoogleAuth\n",
    "    from pydrive.drive import GoogleDrive\n",
    "    from google.colab import auth\n",
    "    from oauth2client.client import GoogleCredentials\n",
    "    # Authenticate and create the PyDrive client.\n",
    "    auth.authenticate_user()\n",
    "    gauth = GoogleAuth()\n",
    "    gauth.credentials = GoogleCredentials.get_application_default()\n",
    "    drive = GoogleDrive(gauth)\n",
    "    id = \"1Doyuyo_VDOmJf0CLo5kl9XzMTfhGtxiR\"\n",
    "    downloaded = drive.CreateFile({'id':id}) \n",
    "    downloaded.GetContentFile('2010-2023_NSS_filtered_vF.csv')  \n",
    "    df_read = pd.read_csv('2010-2023_NSS_filtered_vF.csv')\n",
    "else:\n",
    "    file = \"../data/processed_data/2010-2023_NSS_filtered_vF.csv\"\n",
    "    df_read = pd.read_csv(file)\n",
    "\n",
    "display(df_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_read\n",
    "del df_read\n",
    "\n",
    "df = df[(df[\"Quote_date\"] >= \"2019-06-01\")]\n",
    "\n",
    "\n",
    "# Group the data by Quote Date and calculate the mean for Underlying Price\n",
    "df_agg = df.groupby('Quote_date').mean().reset_index()\n",
    "df_agg = df_agg[['Quote_date', 'Underlying_last']]\n",
    "\n",
    "# Values to returns\n",
    "df_agg[\"Underlying_return\"] = df_agg[\"Underlying_last\"].pct_change()\n",
    "\n",
    "# Add the Underlying Price Lag column\n",
    "for i in range(1, lags + 1):\n",
    "    df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
    "\n",
    "df = pd.merge(df, df_agg[['Quote_date', 'Underlying_return'] + ['Underlying_' + str(i) for i in range(1, lags + 1)]], on='Quote_date', how='left')\n",
    "del df_agg\n",
    "\n",
    "# Filter df between 2011-09-01\n",
    "df = df[(df[\"Quote_date\"] >= \"2011-12-01\")]\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format settings\n",
    "max_timesteps = lags\n",
    "\n",
    "def create_rw_dataset(window_number = 0, df = None, shap = False):\n",
    "    '''Creates dataset for a single rolling window period offsett by the window number'''\n",
    "\n",
    "    # Create train, validation and test set split points\n",
    "    test_months = 1\n",
    "    train_start = datetime(2011,12,1) + relativedelta(months=window_number * test_months)\n",
    "    val_start = train_start + relativedelta(months=3*12)\n",
    "    test_start = val_start + relativedelta(months = 1)\n",
    "    test_end = test_start + relativedelta(months=test_months)\n",
    "    train_start = str(train_start.date())\n",
    "    val_start = str(val_start.date())\n",
    "    test_start = str(test_start.date())\n",
    "    test_end = str(test_end.date())\n",
    "        \n",
    "    # Split train and validation data\n",
    "    df_train = df[(df['Quote_date'] >= train_start) & (df['Quote_date'] < val_start)]\n",
    "    df_val = df[(df['Quote_date'] >= val_start) & (df['Quote_date'] < test_start)]\n",
    "    df_test = df[(df['Quote_date'] >= test_start) & (df['Quote_date'] < test_end)]\n",
    "\n",
    "    del df\n",
    "\n",
    "    # Extract target values\n",
    "    train_y = df_train['Price'].to_numpy()\n",
    "    val_y = df_val['Price'].to_numpy()\n",
    "    test_y = df_test['Price'].to_numpy()\n",
    "\n",
    "    # Print earliest and latest date in every dataframe used\n",
    "    print(\"--------------Dataframe dates--------------\")\n",
    "    print(f\"Train: {df_train['Quote_date'].min()} - {df_train['Quote_date'].max()}\")\n",
    "    print(f\"Val: {df_val['Quote_date'].min()} - {df_val['Quote_date'].max()}\")\n",
    "    print(f\"Test: {df_test['Quote_date'].min()} - {df_test['Quote_date'].max()}\")\n",
    "    print(\"-------------------------------------------\")\n",
    "\n",
    "    # Convert dataframes to numpy arrays\n",
    "    train_x = [df_train[underlying_lags].to_numpy(), df_train[bs_vars].to_numpy()]\n",
    "    val_x = [df_val[underlying_lags].to_numpy(), df_val[bs_vars].to_numpy()]\n",
    "    test_x = [df_test[underlying_lags].to_numpy(), df_test[bs_vars].to_numpy()]\n",
    "\n",
    "    del df_train\n",
    "    del df_val\n",
    "\n",
    "    # Scale features based on training set\n",
    "    underlying_scaler = MinMaxScaler()\n",
    "    train_x[0] = underlying_scaler.fit_transform(train_x[0].flatten().reshape(-1, 1)).reshape(train_x[0].shape)\n",
    "    val_x[0] = underlying_scaler.transform(val_x[0].flatten().reshape(-1,1)).reshape(val_x[0].shape)\n",
    "    test_x[0] = underlying_scaler.transform(test_x[0].flatten().reshape(-1,1)).reshape(test_x[0].shape)\n",
    "\n",
    "    bs_scaler = MinMaxScaler()\n",
    "    train_x[1] = bs_scaler.fit_transform(train_x[1])\n",
    "    val_x[1] = bs_scaler.transform(val_x[1])\n",
    "    test_x[1] = bs_scaler.transform(test_x[1])\n",
    "\n",
    "\n",
    "    # Shuffle training set\n",
    "    np.random.seed(random_seed)\n",
    "    shuffle = np.random.permutation(len(train_x[0]))\n",
    "    train_x = [train_x[0][shuffle], train_x[1][shuffle]]\n",
    "    train_y = train_y[shuffle]\n",
    "\n",
    "    # Extract training set\n",
    "    train_x = [train_x[0][:training_size], train_x[1][:training_size]]\n",
    "    train_y = train_y[:training_size]\n",
    "\n",
    "\n",
    "    # Reshape data to fit LSTM\n",
    "    train_x = [train_x[0].reshape(len(train_x[0]), max_timesteps, 1), train_x[1]]\n",
    "    val_x = [val_x[0].reshape(len(val_x[0]), max_timesteps, 1), val_x[1]]\n",
    "    test_x = [test_x[0].reshape(len(test_x[0]), max_timesteps, 1), test_x[1]]\n",
    "\n",
    "    print(f'Train shape: {train_x[0].shape}, {train_x[1].shape}')\n",
    "    print(f'Val shape: {val_x[0].shape}, {val_x[1].shape}')\n",
    "    print(f'Test shape: {test_x[0].shape}, {test_x[1].shape}')\n",
    "\n",
    "    return train_x, train_y, val_x, val_y, test_x, test_y, train_start, val_start, test_start, df_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelClass:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def predict(self, x_2d):\n",
    "        x_3d = [x_2d[:, :lags].reshape(len(x_2d[:, :lags]), lags, 1), x_2d[:, lags:]]\n",
    "        return np.array(self.model(x_3d))\n",
    "\n",
    "def create_ale(x_2d, model_object):\n",
    "    ale_values = np.zeros((len(x_2d), len(x_2d[0])))\n",
    "    for i in range(len(x_2d[0])):\n",
    "        x_2d_copy = x_2d.copy()\n",
    "        np.random.shuffle(x_2d_copy[:, i])\n",
    "        ale_values[:, i] = model_object.predict(x_2d_copy).flatten()\n",
    "    print(ale_values.shape)\n",
    "    print(ale_values)\n",
    "    return ale_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = 99\n",
    "start_window = 99\n",
    "window_interval = 1\n",
    "checkpoint_time = '11.05 1 mnd test sett full model run'\n",
    "\n",
    "df_shap_combined = pd.DataFrame()\n",
    "\n",
    "model = None\n",
    "\n",
    "timestamp = datetime.now()\n",
    "timestamp = timestamp.strftime(\"%m-%d_%H-%M\")\n",
    "\n",
    "for window in range(start_window-1, windows, window_interval):\n",
    "    # Load data\n",
    "    train_x, test_x, train_x_org, train_start, val_start, test_start = create_rw_dataset(df=df, window_number=window, shap = True)\n",
    "\n",
    "    if google_colab:\n",
    "        checkpoint_path = f'/content/drive/MyDrive/01. Masters Thesis - Shared/05. Checkpoints/{checkpoint_time}/{train_start}/'\n",
    "    else:\n",
    "        checkpoint_path = f'../checkpoints/{checkpoint_time}/{train_start}/'\n",
    "\n",
    "    c_model = load_model(checkpoint_path + \".h5\")\n",
    "\n",
    "    train_x_2d_input = np.hstack((train_x[0], train_x[1]))\n",
    "    test_x_2d_input = np.hstack((test_x[0], test_x[1]))\n",
    "    \n",
    "    model = ModelClass(c_model)\n",
    "\n",
    "    ale_values = create_ale(test_x_2d_input, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
