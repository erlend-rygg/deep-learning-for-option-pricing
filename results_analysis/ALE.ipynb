{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from PyALE import ale\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_colab = True\n",
    "lags = 140\n",
    "training_size = 991232\n",
    "random_seed = 0\n",
    "\n",
    "bs_vars = ['Underlying_last', 'Strike', 'TTM', 'R']\n",
    "underlying_lags = [f'Underlying_{i}' for i in range(lags - 1, 0, -1)] + ['Underlying_return']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if google_colab:\n",
    "    import tensorflow as tf\n",
    "    # Print info\n",
    "    gpu_info = !nvidia-smi\n",
    "    gpu_info = '\\n'.join(gpu_info)\n",
    "    if gpu_info.find('failed') >= 0:\n",
    "        print('Not connected to a GPU')\n",
    "    else:\n",
    "        print(gpu_info)\n",
    "    \n",
    "    from psutil import virtual_memory\n",
    "    ram_gb = virtual_memory().total / 1e9\n",
    "    print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "    if ram_gb < 20:\n",
    "        print('Not using a high-RAM runtime')\n",
    "    else:\n",
    "        print('You are using a high-RAM runtime!')\n",
    "\n",
    "    # Code to read csv file into Colaboratory:\n",
    "    !pip install -U -q PyDrive\n",
    "    from pydrive.auth import GoogleAuth\n",
    "    from pydrive.drive import GoogleDrive\n",
    "    from google.colab import auth\n",
    "    from oauth2client.client import GoogleCredentials\n",
    "    # Authenticate and create the PyDrive client.\n",
    "    auth.authenticate_user()\n",
    "    gauth = GoogleAuth()\n",
    "    gauth.credentials = GoogleCredentials.get_application_default()\n",
    "    drive = GoogleDrive(gauth)\n",
    "    id = \"1Doyuyo_VDOmJf0CLo5kl9XzMTfhGtxiR\"\n",
    "    downloaded = drive.CreateFile({'id':id}) \n",
    "    downloaded.GetContentFile('2010-2023_NSS_filtered_vF.csv')  \n",
    "    df_read = pd.read_csv('2010-2023_NSS_filtered_vF.csv')\n",
    "else:\n",
    "    file = \"../data/processed_data/2010-2023_NSS_filtered_vF.csv\"\n",
    "    df_read = pd.read_csv(file)\n",
    "\n",
    "display(df_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_read\n",
    "del df_read\n",
    "\n",
    "df = df[(df[\"Quote_date\"] >= \"2019-06-01\")]\n",
    "\n",
    "\n",
    "# Group the data by Quote Date and calculate the mean for Underlying Price\n",
    "df_agg = df.groupby('Quote_date').mean().reset_index()\n",
    "df_agg = df_agg[['Quote_date', 'Underlying_last']]\n",
    "\n",
    "# Values to returns\n",
    "df_agg[\"Underlying_return\"] = df_agg[\"Underlying_last\"].pct_change()\n",
    "\n",
    "# Add the Underlying Price Lag column\n",
    "for i in range(1, lags + 1):\n",
    "    df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
    "\n",
    "df = pd.merge(df, df_agg[['Quote_date', 'Underlying_return'] + ['Underlying_' + str(i) for i in range(1, lags + 1)]], on='Quote_date', how='left')\n",
    "del df_agg\n",
    "\n",
    "# Filter df between 2011-09-01\n",
    "df = df[(df[\"Quote_date\"] >= \"2020-02-01\")]\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format settings\n",
    "max_timesteps = lags\n",
    "\n",
    "def create_rw_dataset(window_number = 0, df = None, shap = False):\n",
    "    '''Creates dataset for a single rolling window period offsett by the window number'''\n",
    "\n",
    "    # Create train, validation and test set split points\n",
    "    test_months = 1\n",
    "    train_start = datetime(2011,12,1) + relativedelta(months=window_number * test_months)\n",
    "    val_start = train_start + relativedelta(months=3*12)\n",
    "    train_start = str(train_start.date())\n",
    "    val_start = str(val_start.date())\n",
    "        \n",
    "    # Split train and validation data\n",
    "    df_train = df[(df['Quote_date'] >= train_start) & (df['Quote_date'] < val_start)]\n",
    "\n",
    "    # Extract target values\n",
    "    train_y = df_train['Price'].to_numpy()\n",
    "\n",
    "    # Print earliest and latest date in every dataframe used\n",
    "    print(\"--------------Dataframe dates--------------\")\n",
    "    print(f\"Train: {df_train['Quote_date'].min()} - {df_train['Quote_date'].max()}\")\n",
    "    print(\"-------------------------------------------\")\n",
    "\n",
    "    # Convert dataframes to numpy arrays\n",
    "    train_x = [df_train[underlying_lags].to_numpy(), df_train[bs_vars].to_numpy()]\n",
    "\n",
    "    del df_train\n",
    "\n",
    "    # Scale features based on training set\n",
    "    underlying_scaler = MinMaxScaler()\n",
    "    train_x[0] = underlying_scaler.fit_transform(train_x[0].flatten().reshape(-1, 1)).reshape(train_x[0].shape)\n",
    "\n",
    "    bs_scaler = MinMaxScaler()\n",
    "    train_x[1] = bs_scaler.fit_transform(train_x[1])\n",
    "\n",
    "\n",
    "    # Shuffle training set\n",
    "    np.random.seed(random_seed)\n",
    "    shuffle = np.random.permutation(len(train_x[0]))\n",
    "    train_x = [train_x[0][shuffle], train_x[1][shuffle]]\n",
    "\n",
    "    return train_x, train_start"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelClass:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def predict(self, x_2d):\n",
    "        x_2d = x_2d.to_numpy()\n",
    "        x_3d = [x_2d[:, :lags].reshape(len(x_2d[:, :lags]), lags, 1), x_2d[:, lags:]]\n",
    "        return np.array(self.model(x_3d))\n",
    "\n",
    "def create_ale(x_2d, model_object):\n",
    "    cols = x_2d.columns\n",
    "    ale_values = np.zeros((len(cols)))\n",
    "    for i in range(len(cols)):\n",
    "        value = ale(x_2d, model_object, feature = [cols[i]])\n",
    "        ale_values[i] = abs(value['eff']).mean()\n",
    "        print(f'Finished {cols[i]}')\n",
    "    return ale_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 96\n",
    "train_x, train_start = create_rw_dataset(df=df, window_number=window, shap = True)\n",
    "\n",
    "checkpoint_time = '11.05 1 mnd test sett full model run'\n",
    "if google_colab:\n",
    "    checkpoint_path = f'/content/drive/MyDrive/01. Masters Thesis - Shared/05. Checkpoints/{checkpoint_time}/{train_start}/'\n",
    "else:\n",
    "    checkpoint_path = f'../checkpoints/{checkpoint_time}/{train_start}/'\n",
    "\n",
    "c_model = load_model(checkpoint_path + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_time = '11.05 1 mnd test sett full model run'\n",
    "sample = 100\n",
    "feature_labels = ['Returns', 'S&P500', 'Strike', 'TTM', 'R']\n",
    "\n",
    "all_ale_aggs_pct = np.array([])\n",
    "\n",
    "for window in range(96, 97, 1):\n",
    "    '''train_x, train_start = create_rw_dataset(df=df, window_number=window, shap = True)\n",
    "    \n",
    "    checkpoint_path = f'/content/drive/MyDrive/01. Masters Thesis - Shared/05. Checkpoints/{checkpoint_time}/{train_start}/'\n",
    "\n",
    "    c_model = load_model(checkpoint_path + \".h5\")'''\n",
    "    print(f'Train start: {train_start}')\n",
    "\n",
    "    train_x_sample = (train_x[0][:sample], train_x[1][:sample])\n",
    "\n",
    "    train_x_2d = np.hstack((train_x_sample[0], train_x_sample[1]))\n",
    "\n",
    "    df_ale_train = pd.DataFrame(train_x_2d, columns = [underlying_lags + bs_vars])\n",
    "\n",
    "    model = ModelClass(c_model)\n",
    "\n",
    "    ale_values = create_ale(df_ale_train, model)\n",
    "\n",
    "    save_path = f'/content/drive/MyDrive/01. Masters Thesis - Shared/05. Predictions/ALE/{checkpoint_time}/{train_start}/'\n",
    "    df_ale_values = pd.DataFrame(ale_values, columns = [underlying_lags + bs_vars])\n",
    "    df_ale_values.to_csv(save_path)\n",
    "\n",
    "    ale_agg = np.append(np.sum(ale_values[:lags]), ale_values[lags:])\n",
    "    ale_agg_pct = ale_agg / np.sum(ale_agg)\n",
    "    all_ale_agg_pct = np.append(all_ale_agg_pct, ale_agg_pct)\n",
    "    print(f'Ale agg: {ale_agg}')\n",
    "    print(f'Ale pct: {ale_agg_pct}')\n",
    "\n",
    "    df_pct = pd.DataFrame(feature_labels, ['Feature'])\n",
    "    df_pct['Importance'] = ale_agg_pct\n",
    "    df_pct = df_pct.sort_values('Importance', ascending = False)\n",
    "\n",
    "    sns.barplot(x='Feature', y='Importance', data = df_pct)\n",
    "    plt.show()\n",
    "\n",
    "    del train_x\n",
    "\n",
    "print('Combined for all models:')\n",
    "df_pct = pd.DataFrame(feature_labels, ['Feature'])\n",
    "avg_pct = np.mean(all_ale_aggs_pct, axis = 0)\n",
    "print(f'Avg pre scale {avg_pct}')\n",
    "avg_pct = avg_pct / np.sum(avg_pct)\n",
    "print(f'Avg post scale {avg_pct}')\n",
    "df_pct['Importance'] = all_ale_agg_pct\n",
    "df_pct = df_pct.sort_values('Importance', ascending = False)\n",
    "\n",
    "sns.barplot(x='Feature', y='Importance', data = df_pct)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
