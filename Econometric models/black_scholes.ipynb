{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2 Run\n",
    "This file takes in the file written as a .csv from naive.ipynb. The output is used by compare.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log, sqrt, pi, exp\n",
    "from scipy.stats import norm\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from arch import arch_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    \"\"\"Read a single file and return a dataframe\"\"\"\n",
    "    return pd.read_csv(file, skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "garch = False\n",
    "rolling_avg = False\n",
    "implied_vol = True\n",
    "test_pq = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if implied_vol:\n",
    "    file = '../data/processed_data/2010-2023_NSS_filtered_with_IV.csv'\n",
    "    df_IV = pd.read_csv(file, skipinitialspace=True)\n",
    "    df_IV = df_IV[df_IV[\"Quote_date\"] >= \"2015-01-01\"]\n",
    "else:\n",
    "    file = '../data/processed_data/2010-2023_NSS_filtered_vF.csv'\n",
    "    df = pd.read_csv(file, skipinitialspace=True)\n",
    "    df = df[df[\"Quote_date\"] >= \"2014-09-01\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Volatility"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_pq:\n",
    "    # Assuming your dataframe is named `df`\n",
    "    df_unique_dates = df[['Quote_date', 'Underlying_last']].drop_duplicates()\n",
    "\n",
    "    df_unique_dates['log_returns'] = np.log(df_unique_dates['Underlying_last']) - np.log(df_unique_dates['Underlying_last'].shift(1))\n",
    "    df_unique_dates = df_unique_dates[['Quote_date', 'log_returns']].dropna()\n",
    "\n",
    "    # Define a function to fit GARCH models and return AIC and BIC\n",
    "    def fit_garch_aic_bic(log_returns, p, q):\n",
    "        model = arch_model(log_returns, vol='Garch', p=p, q=q, dist='Normal')\n",
    "        results = model.fit(disp='off')\n",
    "        return results.aic, results.bic\n",
    "\n",
    "    # Test GARCH models with different p and q values\n",
    "    pq_values = [(1, 1), (1, 2), (2, 1), (2, 2), (1, 3), (3, 1), (2, 3), (3, 2), (3, 3), (4, 1), (1, 4), (4, 2), (2, 4), (4, 4)]\n",
    "\n",
    "    aic_bic_values = [fit_garch_aic_bic(df_unique_dates['log_returns'], p, q) for p, q in pq_values]\n",
    "    aic_values, bic_values = zip(*aic_bic_values)\n",
    "\n",
    "    # Find the best p and q values based on AIC and BIC\n",
    "    best_pq_aic = pq_values[np.argmin(aic_values)]\n",
    "    best_pq_bic = pq_values[np.argmin(bic_values)]\n",
    "    print(f\"Best GARCH model based on AIC: GARCH({best_pq_aic[0]},{best_pq_aic[1]})\")\n",
    "    print(f\"Best GARCH model based on BIC: GARCH({best_pq_bic[0]},{best_pq_bic[1]})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "if garch:\n",
    "    df_unique_dates = df[['Quote_date', 'Underlying_last']].drop_duplicates()\n",
    "\n",
    "    df_unique_dates['log_returns'] = np.log(df_unique_dates['Underlying_last']) - np.log(df_unique_dates['Underlying_last'].shift(1))\n",
    "    df_unique_dates = df_unique_dates[['Quote_date', 'log_returns']].dropna()\n",
    "\n",
    "    # Define GARCH(1,1) model\n",
    "    model = arch_model(df_unique_dates['log_returns'], vol='Garch', p=1, q=1, dist='Normal')\n",
    "\n",
    "    # Fit the model\n",
    "    results = model.fit(update_freq=5)\n",
    "\n",
    "    # Predict the volatility (annualized)\n",
    "    df_unique_dates['predicted_volatility'] =  results.conditional_volatility * np.sqrt(252)\n",
    "\n",
    "    # Merge the predicted volatility with the original dataframe\n",
    "    df = df.merge(df_unique_dates[['Quote_date', 'predicted_volatility']], on='Quote_date', how='left')\n",
    "\n",
    "    df = df[df[\"Quote_date\"] >= \"2015-01-01\"]\n",
    "    # Drop rows with NaN values\n",
    "    df = df.dropna()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rolling_avg:    \n",
    "    # Add volatility column with 30 day rolling standard deviation of Underlying_last\n",
    "\n",
    "    # New dataframe without duplicate Quote_dates\n",
    "    df2 = df.drop_duplicates(subset=['Quote_date'])\n",
    "\n",
    "    # Calculate volatility\n",
    "    df2['rolling_volatility'] = np.log(df2[\"Underlying_last\"] / df2[\"Underlying_last\"].shift()).rolling(30).std()*(252**0.5)\n",
    "\n",
    "    # Matching volatility in df2 to df\n",
    "    df['rolling_volatility'] = df['Quote_date'].map(df2.set_index('Quote_date')['rolling_volatility'])\n",
    "\n",
    "    df = df[df[\"Quote_date\"] >= \"2015-01-01\"]\n",
    "\n",
    "    # Drop rows with NaN values\n",
    "    df = df.dropna()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implied vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Black-Scholes formula for call options\n",
    "def d1(S,K,T,r,sigma):\n",
    "    x1 = S.apply(lambda x : log(x)) - K.apply(lambda x : log(x))\n",
    "    x2 = (r + ((sigma.apply(lambda x : x**2)) / 2)) * T\n",
    "    x3 = sigma * T.apply(lambda x: sqrt(x))\n",
    "    return  (x1 + x2) / x3\n",
    "\n",
    "def d2(S,K,T,r,sigma):\n",
    "    return d1(S,K,T,r,sigma) - sigma * T.apply(lambda x : sqrt(x))  \n",
    "\n",
    "def bs_call(S,K,T,r,sigma):\n",
    "    return S * d1(S,K,T,r,sigma).apply(lambda x : norm.cdf(x)) - K * (-r*T).apply(lambda x : exp(x)) * d2(S,K,T,r,sigma).apply(lambda x : norm.cdf(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "if implied_vol:    \n",
    "    # Calculate the average implied volatility for each date\n",
    "    avg_implied_vol_df = df_IV.groupby(\n",
    "        'Quote_date')['IV'].median().reset_index()\n",
    "\n",
    "    # Add a new column with the average implied volatility shifted by one row\n",
    "    avg_implied_vol_df['avg_implied_vol_t-1'] = avg_implied_vol_df['IV'].shift(1)\n",
    "\n",
    "    # Merge the avg_implied_vol_df DataFrame back to the original df DataFrame\n",
    "    df = df.merge(avg_implied_vol_df[[\n",
    "        'Quote_date', 'avg_implied_vol_t-1']], on='Quote_date', how='left')\n",
    "\n",
    "    # Drop rows with NaN values\n",
    "    df = df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Quote_date</th>\n",
       "      <th>Price</th>\n",
       "      <th>Underlying_last</th>\n",
       "      <th>Strike</th>\n",
       "      <th>TTM</th>\n",
       "      <th>R</th>\n",
       "      <th>predicted_volatility</th>\n",
       "      <th>Volatility</th>\n",
       "      <th>BS</th>\n",
       "      <th>avg_implied_vol_t-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2776</th>\n",
       "      <td>1812133</td>\n",
       "      <td>2015-01-05</td>\n",
       "      <td>970.00</td>\n",
       "      <td>2021.05</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>0.010959</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>0.125301</td>\n",
       "      <td>0.125301</td>\n",
       "      <td>971.052301</td>\n",
       "      <td>0.187670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2777</th>\n",
       "      <td>1812134</td>\n",
       "      <td>2015-01-05</td>\n",
       "      <td>920.05</td>\n",
       "      <td>2021.05</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.010959</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>0.125301</td>\n",
       "      <td>0.125301</td>\n",
       "      <td>921.052411</td>\n",
       "      <td>0.187670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2778</th>\n",
       "      <td>1812135</td>\n",
       "      <td>2015-01-05</td>\n",
       "      <td>870.05</td>\n",
       "      <td>2021.05</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>0.010959</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>0.125301</td>\n",
       "      <td>0.125301</td>\n",
       "      <td>871.052521</td>\n",
       "      <td>0.187670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2779</th>\n",
       "      <td>1812136</td>\n",
       "      <td>2015-01-05</td>\n",
       "      <td>845.05</td>\n",
       "      <td>2021.05</td>\n",
       "      <td>1175.0</td>\n",
       "      <td>0.010959</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>0.125301</td>\n",
       "      <td>0.125301</td>\n",
       "      <td>846.052575</td>\n",
       "      <td>0.187670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2780</th>\n",
       "      <td>1812137</td>\n",
       "      <td>2015-01-05</td>\n",
       "      <td>820.05</td>\n",
       "      <td>2021.05</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>0.010959</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>0.125301</td>\n",
       "      <td>0.125301</td>\n",
       "      <td>821.052630</td>\n",
       "      <td>0.187670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10539482</th>\n",
       "      <td>13739049</td>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>217.75</td>\n",
       "      <td>4109.88</td>\n",
       "      <td>4700.0</td>\n",
       "      <td>1.726027</td>\n",
       "      <td>0.04198</td>\n",
       "      <td>0.148997</td>\n",
       "      <td>0.148997</td>\n",
       "      <td>216.072717</td>\n",
       "      <td>0.178325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10539483</th>\n",
       "      <td>13739050</td>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>180.00</td>\n",
       "      <td>4109.88</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>1.726027</td>\n",
       "      <td>0.04198</td>\n",
       "      <td>0.148997</td>\n",
       "      <td>0.148997</td>\n",
       "      <td>186.281498</td>\n",
       "      <td>0.178325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10539484</th>\n",
       "      <td>13739051</td>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>146.55</td>\n",
       "      <td>4109.88</td>\n",
       "      <td>4900.0</td>\n",
       "      <td>1.726027</td>\n",
       "      <td>0.04198</td>\n",
       "      <td>0.148997</td>\n",
       "      <td>0.148997</td>\n",
       "      <td>159.937644</td>\n",
       "      <td>0.178325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10539485</th>\n",
       "      <td>13739052</td>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>118.20</td>\n",
       "      <td>4109.88</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>1.726027</td>\n",
       "      <td>0.04198</td>\n",
       "      <td>0.148997</td>\n",
       "      <td>0.148997</td>\n",
       "      <td>136.773457</td>\n",
       "      <td>0.178325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10539486</th>\n",
       "      <td>13739053</td>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>94.40</td>\n",
       "      <td>4109.88</td>\n",
       "      <td>5100.0</td>\n",
       "      <td>1.726027</td>\n",
       "      <td>0.04198</td>\n",
       "      <td>0.148997</td>\n",
       "      <td>0.148997</td>\n",
       "      <td>116.515395</td>\n",
       "      <td>0.178325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10536711 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Unnamed: 0  Quote_date   Price  Underlying_last  Strike       TTM  \\\n",
       "2776         1812133  2015-01-05  970.00          2021.05  1050.0  0.010959   \n",
       "2777         1812134  2015-01-05  920.05          2021.05  1100.0  0.010959   \n",
       "2778         1812135  2015-01-05  870.05          2021.05  1150.0  0.010959   \n",
       "2779         1812136  2015-01-05  845.05          2021.05  1175.0  0.010959   \n",
       "2780         1812137  2015-01-05  820.05          2021.05  1200.0  0.010959   \n",
       "...              ...         ...     ...              ...     ...       ...   \n",
       "10539482    13739049  2023-03-31  217.75          4109.88  4700.0  1.726027   \n",
       "10539483    13739050  2023-03-31  180.00          4109.88  4800.0  1.726027   \n",
       "10539484    13739051  2023-03-31  146.55          4109.88  4900.0  1.726027   \n",
       "10539485    13739052  2023-03-31  118.20          4109.88  5000.0  1.726027   \n",
       "10539486    13739053  2023-03-31   94.40          4109.88  5100.0  1.726027   \n",
       "\n",
       "                R  predicted_volatility  Volatility          BS  \\\n",
       "2776      0.00020              0.125301    0.125301  971.052301   \n",
       "2777      0.00020              0.125301    0.125301  921.052411   \n",
       "2778      0.00020              0.125301    0.125301  871.052521   \n",
       "2779      0.00020              0.125301    0.125301  846.052575   \n",
       "2780      0.00020              0.125301    0.125301  821.052630   \n",
       "...           ...                   ...         ...         ...   \n",
       "10539482  0.04198              0.148997    0.148997  216.072717   \n",
       "10539483  0.04198              0.148997    0.148997  186.281498   \n",
       "10539484  0.04198              0.148997    0.148997  159.937644   \n",
       "10539485  0.04198              0.148997    0.148997  136.773457   \n",
       "10539486  0.04198              0.148997    0.148997  116.515395   \n",
       "\n",
       "          avg_implied_vol_t-1  \n",
       "2776                 0.187670  \n",
       "2777                 0.187670  \n",
       "2778                 0.187670  \n",
       "2779                 0.187670  \n",
       "2780                 0.187670  \n",
       "...                       ...  \n",
       "10539482             0.178325  \n",
       "10539483             0.178325  \n",
       "10539484             0.178325  \n",
       "10539485             0.178325  \n",
       "10539486             0.178325  \n",
       "\n",
       "[10536711 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    df = df_read.copy()\n",
    "    # Filter out options with a difference in strike price of more than 4% from the underlying price and a TTM of less than 20 days\n",
    "    df = df[(df['Underlying_last'] > df['Strike'] * 0.94) & (df['Underlying_last'] < df['Strike'] * 1.06) & (df['TTM'] > 15)]\n",
    "\n",
    "    from py_vollib.black_scholes import black_scholes as bs\n",
    "    from py_vollib.black_scholes.greeks.analytical import vega\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    def implied_vol(S0, K, T, r, market_price, flag='c', tol=0.000001):\n",
    "        \"\"\"Compute the implied volatility of a European Option\n",
    "            S0: initial stock price\n",
    "            K:  strike price\n",
    "            T:  maturity\n",
    "            r:  risk-free rate\n",
    "            market_price: market observed price\n",
    "            tol: user choosen tolerance\n",
    "        \"\"\"\n",
    "        T = T/365 #converting to years\n",
    "        r = r/100\n",
    "        max_iter = 200 #max number of iterations\n",
    "        vol_old = 0.20 #initial guess\n",
    "        for k in range(max_iter):\n",
    "            bs_price = bs(flag, S0, K, T, r, vol_old)\n",
    "            Cprime =  vega(flag, S0, K, T, r, vol_old)*100\n",
    "            C = bs_price - market_price\n",
    "            vol_new = vol_old - C/Cprime\n",
    "            bs_new = bs(flag, S0, K, T, r, vol_new)\n",
    "            if (abs(vol_old - vol_new) < tol or abs(bs_new - market_price) < tol):\n",
    "                break\n",
    "            vol_old = vol_new\n",
    "        implied_vol = vol_old\n",
    "        return implied_vol\n",
    "    \n",
    "\n",
    "    # Add implied volatility column\n",
    "    df['Implied_volatility'] = df.apply(lambda x: implied_vol(x['Underlying_last'], x['Strike'], x['TTM'], x['R'], x['Price'], 'c'), axis=1)\n",
    "\n",
    "    display(df)\n",
    "    print(\"Number of rows with NaN values: \", df.isna().sum().sum())\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Calculate the average implied volatility for each date\n",
    "    avg_implied_vol_df = df.groupby('Quote_date')['Implied_volatility'].mean().reset_index()\n",
    "\n",
    "    # Add a new column with the average implied volatility shifted by one row\n",
    "    avg_implied_vol_df['avg_implied_vol_t-1'] = avg_implied_vol_df['Implied_volatility'].shift(1)\n",
    "\n",
    "    # Merge the avg_implied_vol_df DataFrame back to the original df DataFrame\n",
    "    df = df.merge(avg_implied_vol_df[['Quote_date', 'avg_implied_vol_t-1']], on='Quote_date', how='left')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    df = df_read.copy()\n",
    "    import numpy as np\n",
    "    from scipy.stats import norm\n",
    "\n",
    "    def bs_call(S, K, T, r, vol):\n",
    "        eps = 1e-8\n",
    "        d1 = np.divide((np.log(S/K) + (r + 0.5*vol**2)*T), (vol*np.sqrt(T) + eps))\n",
    "        d2 = d1 - vol * np.sqrt(T)\n",
    "        return S * norm.cdf(d1) - np.exp(-r * T) * K * norm.cdf(d2)\n",
    "\n",
    "    def bs_vega(S, K, T, r, sigma):\n",
    "        eps = 1e-8\n",
    "        d1 = np.divide((np.log(S/K) + (r + 0.5*sigma**2)*T), (sigma*np.sqrt(T) + eps))\n",
    "        return S * norm.pdf(d1) * np.sqrt(T)\n",
    "\n",
    "    def find_vol(target_values, S, K, T, r):\n",
    "        MAX_ITERATIONS = 200\n",
    "        PRECISION = 1.0e-5\n",
    "        sigmas = np.full_like(target_values, 0.5)\n",
    "\n",
    "        for _ in range(MAX_ITERATIONS):\n",
    "            prices = bs_call(S, K, T, r, sigmas)\n",
    "            vegas = bs_vega(S, K, T, r, sigmas)\n",
    "            diffs = target_values - prices\n",
    "\n",
    "            mask = np.abs(diffs) >= PRECISION\n",
    "            if not np.any(mask):\n",
    "                break\n",
    "\n",
    "            sigmas[mask] += np.divide(diffs[mask], vegas[mask], out=np.full_like(diffs[mask], np.nan), where=(vegas[mask] != 0))\n",
    "\n",
    "        return sigmas\n",
    "\n",
    "\n",
    "    df['Implied_volatility'] = find_vol(df['Price'].values, df['Underlying_last'].values, df['Strike'].values, df['TTM'].values, df['R'].values)\n",
    "\n",
    "\n",
    "    len_before = len(df)\n",
    "    df = df.dropna()\n",
    "    len_after = len(df)\n",
    "    print(f\"Number of rows dropped: {len_before - len_after}, which is {round((len_before - len_after)/len_before*100, 2)}% of the original dataframe\")\n",
    "\n",
    "    # Calculate the average implied volatility for each date\n",
    "    avg_implied_vol_df = df.groupby('Quote_date')['Implied_volatility'].mean().reset_index()\n",
    "\n",
    "    # Add a new column with the average implied volatility shifted by one row\n",
    "    avg_implied_vol_df['avg_implied_vol_t-1'] = avg_implied_vol_df['Implied_volatility'].shift(1)\n",
    "\n",
    "    # Merge the avg_implied_vol_df DataFrame back to the original df DataFrame\n",
    "    df = df.merge(avg_implied_vol_df[['Quote_date', 'avg_implied_vol_t-1']], on='Quote_date', how='left')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"Quote_date\"] >= \"2015-01-01\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rolling_avg:\n",
    "    df[\"Volatility\"] = df[\"rolling_volatility\"]\n",
    "if garch:\n",
    "    df[\"Volatility\"] = df[\"predicted_volatility\"]\n",
    "if implied_vol:\n",
    "    df[\"Volatility\"] = df[\"avg_implied_vol_t-1\"]\n",
    "    \n",
    "df[\"BS\"] = bs_call(df[\"Underlying_last\"], df[\"Strike\"], df[\"TTM\"], df[\"R\"], df[\"Volatility\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for full period 26.71753177110124\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE for full period\", np.sqrt(np.mean((df['BS'] - df['Price'])**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to file\n",
    "df.to_csv('../data/predictions/BS_IV_median.csv', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0adcc2737ebf6a4a119f135174df96668767fca1ef1112612db5ecadf2b6d608"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
