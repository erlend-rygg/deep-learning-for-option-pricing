{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "s2MNO3kPTnkd"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JlTtOWVUD6YH"
      },
      "source": [
        "### Model decisions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oOQkqhqFTnkg"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "\n",
            "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Weights and Biases\n",
        "!pip install -q wandb\n",
        "# Tensorflow\n",
        "!pip install -q tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "4PZBtROGD6YJ"
      },
      "outputs": [],
      "source": [
        "google_colab = False\n",
        "hyperparameter_search = True\n",
        "training_size = 991232\n",
        "random_seed = 0\n",
        "checkpoint_time = None # Insert checkpoint time if continuing training\n",
        "starting_window = 0 # Insert starting window if continuing training\n",
        "run_training = True\n",
        "run_shap = False\n",
        "run_prediction = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Ovc8FFP6D6YJ"
      },
      "outputs": [],
      "source": [
        "if google_colab == True:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8xVVTnPD6YJ"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Bfwvf6vGTnki"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, LSTM, Concatenate, Dense, BatchNormalization, LeakyReLU\n",
        "from keras.activations import tanh\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from tensorflow import square, reduce_mean\n",
        "from tensorflow.keras.losses import MSE\n",
        "from tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.math import multiply\n",
        "from tensorflow.keras.metrics import MeanSquaredError, RootMeanSquaredError\n",
        "from math import log\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import shap\n",
        "from pathlib import Path\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "1fduF6iTTnkj"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Erlend/.netrc\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# If running in colab, insert your wandb key here\n",
        "\n",
        "#import config\n",
        "#Erlend\n",
        "wandb.login(key='3cae81eb56be3190be5bb48c571e69933071df69')\n",
        "# Hjalmar\n",
        "#wandb.login(key=\"b47bcf387a0571c5520c58a13be35cda8ada0a99\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zJGDiGMLTnkl"
      },
      "source": [
        "# Load, split and normalize data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UI1j3mQWTnkl"
      },
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "OKcl8WWJTnkl"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Quote_date</th>\n",
              "      <th>Price</th>\n",
              "      <th>Underlying_last</th>\n",
              "      <th>Strike</th>\n",
              "      <th>TTM</th>\n",
              "      <th>R</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2010-01-04</td>\n",
              "      <td>207.490</td>\n",
              "      <td>1132.99</td>\n",
              "      <td>925.0</td>\n",
              "      <td>0.008219</td>\n",
              "      <td>0.00050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2010-01-04</td>\n",
              "      <td>182.500</td>\n",
              "      <td>1132.99</td>\n",
              "      <td>950.0</td>\n",
              "      <td>0.008219</td>\n",
              "      <td>0.00050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2010-01-04</td>\n",
              "      <td>157.500</td>\n",
              "      <td>1132.99</td>\n",
              "      <td>975.0</td>\n",
              "      <td>0.008219</td>\n",
              "      <td>0.00050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>2010-01-04</td>\n",
              "      <td>132.600</td>\n",
              "      <td>1132.99</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>0.008219</td>\n",
              "      <td>0.00050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2010-01-04</td>\n",
              "      <td>107.705</td>\n",
              "      <td>1132.99</td>\n",
              "      <td>1025.0</td>\n",
              "      <td>0.008219</td>\n",
              "      <td>0.00050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12057638</th>\n",
              "      <td>13739049</td>\n",
              "      <td>2023-03-31</td>\n",
              "      <td>217.750</td>\n",
              "      <td>4109.88</td>\n",
              "      <td>4700.0</td>\n",
              "      <td>1.726027</td>\n",
              "      <td>0.04198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12057639</th>\n",
              "      <td>13739050</td>\n",
              "      <td>2023-03-31</td>\n",
              "      <td>180.000</td>\n",
              "      <td>4109.88</td>\n",
              "      <td>4800.0</td>\n",
              "      <td>1.726027</td>\n",
              "      <td>0.04198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12057640</th>\n",
              "      <td>13739051</td>\n",
              "      <td>2023-03-31</td>\n",
              "      <td>146.550</td>\n",
              "      <td>4109.88</td>\n",
              "      <td>4900.0</td>\n",
              "      <td>1.726027</td>\n",
              "      <td>0.04198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12057641</th>\n",
              "      <td>13739052</td>\n",
              "      <td>2023-03-31</td>\n",
              "      <td>118.200</td>\n",
              "      <td>4109.88</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>1.726027</td>\n",
              "      <td>0.04198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12057642</th>\n",
              "      <td>13739053</td>\n",
              "      <td>2023-03-31</td>\n",
              "      <td>94.400</td>\n",
              "      <td>4109.88</td>\n",
              "      <td>5100.0</td>\n",
              "      <td>1.726027</td>\n",
              "      <td>0.04198</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12057643 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          Unnamed: 0  Quote_date    Price  Underlying_last  Strike       TTM   \n",
              "0                  0  2010-01-04  207.490          1132.99   925.0  0.008219  \\\n",
              "1                  1  2010-01-04  182.500          1132.99   950.0  0.008219   \n",
              "2                  2  2010-01-04  157.500          1132.99   975.0  0.008219   \n",
              "3                  3  2010-01-04  132.600          1132.99  1000.0  0.008219   \n",
              "4                  4  2010-01-04  107.705          1132.99  1025.0  0.008219   \n",
              "...              ...         ...      ...              ...     ...       ...   \n",
              "12057638    13739049  2023-03-31  217.750          4109.88  4700.0  1.726027   \n",
              "12057639    13739050  2023-03-31  180.000          4109.88  4800.0  1.726027   \n",
              "12057640    13739051  2023-03-31  146.550          4109.88  4900.0  1.726027   \n",
              "12057641    13739052  2023-03-31  118.200          4109.88  5000.0  1.726027   \n",
              "12057642    13739053  2023-03-31   94.400          4109.88  5100.0  1.726027   \n",
              "\n",
              "                R  \n",
              "0         0.00050  \n",
              "1         0.00050  \n",
              "2         0.00050  \n",
              "3         0.00050  \n",
              "4         0.00050  \n",
              "...           ...  \n",
              "12057638  0.04198  \n",
              "12057639  0.04198  \n",
              "12057640  0.04198  \n",
              "12057641  0.04198  \n",
              "12057642  0.04198  \n",
              "\n",
              "[12057643 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "if google_colab:\n",
        "    import tensorflow as tf\n",
        "    # Print info\n",
        "    gpu_info = !nvidia-smi\n",
        "    gpu_info = '\\n'.join(gpu_info)\n",
        "    if gpu_info.find('failed') >= 0:\n",
        "        print('Not connected to a GPU')\n",
        "    else:\n",
        "        print(gpu_info)\n",
        "    \n",
        "    from psutil import virtual_memory\n",
        "    ram_gb = virtual_memory().total / 1e9\n",
        "    print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "    if ram_gb < 20:\n",
        "        print('Not using a high-RAM runtime')\n",
        "    else:\n",
        "        print('You are using a high-RAM runtime!')\n",
        "\n",
        "    # Code to read csv file into Colaboratory:\n",
        "    !pip install -U -q PyDrive\n",
        "    from pydrive.auth import GoogleAuth\n",
        "    from pydrive.drive import GoogleDrive\n",
        "    from google.colab import auth\n",
        "    from oauth2client.client import GoogleCredentials\n",
        "    # Authenticate and create the PyDrive client.\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)\n",
        "    id = \"1Doyuyo_VDOmJf0CLo5kl9XzMTfhGtxiR\"\n",
        "    downloaded = drive.CreateFile({'id':id}) \n",
        "    downloaded.GetContentFile('2010-2023_NSS_filtered_vF.csv')  \n",
        "    df_read = pd.read_csv('2010-2023_NSS_filtered_vF.csv')\n",
        "else:\n",
        "    file = \"../data/processed_data/2010-2023_NSS_filtered_vF.csv\"\n",
        "    df_read = pd.read_csv(file)\n",
        "\n",
        "display(df_read)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wdbRyWt1D6YM"
      },
      "source": [
        "### Calculate volatility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "CpcmhVq6Tnkm"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          Unnamed: 0  Quote_date    Price  Underlying_last  Strike       TTM   \n",
            "342523        446309  2011-12-01  344.545          1244.09   900.0  0.002740  \\\n",
            "342524        446310  2011-12-01  319.695          1244.09   925.0  0.002740   \n",
            "342525        446311  2011-12-01  294.595          1244.09   950.0  0.002740   \n",
            "342526        446312  2011-12-01  269.610          1244.09   975.0  0.002740   \n",
            "342527        446313  2011-12-01  244.600          1244.09  1000.0  0.002740   \n",
            "...              ...         ...      ...              ...     ...       ...   \n",
            "12057638    13739049  2023-03-31  217.750          4109.88  4700.0  1.726027   \n",
            "12057639    13739050  2023-03-31  180.000          4109.88  4800.0  1.726027   \n",
            "12057640    13739051  2023-03-31  146.550          4109.88  4900.0  1.726027   \n",
            "12057641    13739052  2023-03-31  118.200          4109.88  5000.0  1.726027   \n",
            "12057642    13739053  2023-03-31   94.400          4109.88  5100.0  1.726027   \n",
            "\n",
            "                R  Volatility  \n",
            "342523    0.00020    0.301489  \n",
            "342524    0.00020    0.301489  \n",
            "342525    0.00020    0.301489  \n",
            "342526    0.00020    0.301489  \n",
            "342527    0.00020    0.301489  \n",
            "...           ...         ...  \n",
            "12057638  0.04198    0.174420  \n",
            "12057639  0.04198    0.174420  \n",
            "12057640  0.04198    0.174420  \n",
            "12057641  0.04198    0.174420  \n",
            "12057642  0.04198    0.174420  \n",
            "\n",
            "[11715120 rows x 8 columns]\n"
          ]
        }
      ],
      "source": [
        "df = df_read\n",
        "del df_read\n",
        "\n",
        "if hyperparameter_search:\n",
        "    df = df[df['Quote_date'] <= '2015-02-01']\n",
        "\n",
        "if run_shap:\n",
        "    df = df[df['Quote_date'] >= '2019-01-01']\n",
        "\n",
        "# Add volatility column with 30 day rolling standard deviation of Underlying_last\n",
        "\n",
        "# New dataframe without duplicate Quote_dates\n",
        "df2 = df.drop_duplicates(subset=['Quote_date'])\n",
        "\n",
        "# Calculate volatility\n",
        "df2['Volatility'] = np.log(df2[\"Underlying_last\"] / df2[\"Underlying_last\"].shift()).rolling(30).std()*(252**0.5)\n",
        "\n",
        "# Matching volatility in df2 to df\n",
        "df['Volatility'] = df['Quote_date'].map(df2.set_index('Quote_date')['Volatility'])\n",
        "\n",
        "# Filter df between 2011-09-01\n",
        "df = df[(df[\"Quote_date\"] >= \"2011-09-01\")] if hyperparameter_search else df[(df[\"Quote_date\"] >= \"2011-12-01\")]\n",
        "\n",
        "print(df)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "j1fuXIvmTnkn"
      },
      "source": [
        "### Format input data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "YKAFyCmTTnko"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------Dataframe dates--------------\n",
            "Train: 2011-12-01 - 2014-10-31\n",
            "Val: 2014-11-03 - 2014-12-31\n",
            "Test: 2015-01-02 - 2015-01-30\n",
            "-------------------------------------------\n",
            "Train shape: (991232, 5),\n",
            "Val shape: (115455, 5),\n",
            "Test shape: (56797, 5),\n"
          ]
        }
      ],
      "source": [
        "# Format settings\n",
        "bs_vars = ['Underlying_last', 'Strike', 'TTM', 'R', 'Volatility']\n",
        "\n",
        "def create_rw_dataset(window_number = 0, df = None, shap = False):\n",
        "    '''Creates dataset for a single rolling window period offsett by the window number'''\n",
        "\n",
        "    # Create train, validation and test set split points\n",
        "    test_months = 1\n",
        "    train_start = datetime(2011,11,1) + relativedelta(months=window_number * test_months) if hyperparameter_search else datetime(2011,12,1) + relativedelta(months=window_number * test_months)\n",
        "    val_start = train_start + relativedelta(months=3*12)\n",
        "    test_start = val_start + relativedelta(months = 1 + test_months) if hyperparameter_search else val_start + relativedelta(months = 1)\n",
        "    test_end = test_start + relativedelta(months=test_months)\n",
        "    train_start = str(train_start.date())\n",
        "    val_start = str(val_start.date())\n",
        "    test_start = str(test_start.date())\n",
        "    test_end = str(test_end.date())\n",
        "        \n",
        "    # Split train and validation data\n",
        "    df_train = df[(df['Quote_date'] >= train_start) & (df['Quote_date'] < val_start)]\n",
        "    df_val = df[(df['Quote_date'] >= val_start) & (df['Quote_date'] < test_start)]\n",
        "    df_test = df[(df['Quote_date'] >= test_start) & (df['Quote_date'] < test_end)]\n",
        "\n",
        "    del df\n",
        "\n",
        "    # Extract target values\n",
        "    train_y = df_train['Price'].to_numpy()\n",
        "    val_y = df_val['Price'].to_numpy()\n",
        "    test_y = df_test['Price'].to_numpy()\n",
        "\n",
        "    # Print earliest and latest date in every dataframe used\n",
        "    print(\"--------------Dataframe dates--------------\")\n",
        "    print(f\"Train: {df_train['Quote_date'].min()} - {df_train['Quote_date'].max()}\")\n",
        "    print(f\"Val: {df_val['Quote_date'].min()} - {df_val['Quote_date'].max()}\")\n",
        "    print(f\"Test: {df_test['Quote_date'].min()} - {df_test['Quote_date'].max()}\")\n",
        "    print(\"-------------------------------------------\")\n",
        "\n",
        "    # Convert dataframes to numpy arrays\n",
        "    train_x = df_train[bs_vars].to_numpy()\n",
        "    val_x = df_val[bs_vars].to_numpy()\n",
        "    test_x = df_test[bs_vars].to_numpy()\n",
        "\n",
        "    del df_train\n",
        "    del df_val\n",
        "\n",
        "    # Scale features based on training set\n",
        "    bs_scaler = MinMaxScaler()\n",
        "    train_x = bs_scaler.fit_transform(train_x)\n",
        "    val_x = bs_scaler.transform(val_x)\n",
        "    test_x = bs_scaler.transform(test_x)\n",
        "\n",
        "\n",
        "    # Shuffle training set\n",
        "    np.random.seed(random_seed)\n",
        "    shuffle = np.random.permutation(len(train_x))\n",
        "    train_x = train_x[shuffle]\n",
        "    train_y = train_y[shuffle]\n",
        "\n",
        "    # Extract training set\n",
        "    train_x = train_x[:training_size]\n",
        "    train_y = train_y[:training_size]\n",
        "    \n",
        "    if shap:\n",
        "        return train_x, test_x, train_start, val_start, test_start\n",
        "\n",
        "    print(f'Train shape: {train_x.shape},')\n",
        "    print(f'Val shape: {val_x.shape},')\n",
        "    print(f'Test shape: {test_x.shape},')\n",
        "\n",
        "    return train_x, train_y, val_x, val_y, test_x, test_y, train_start, val_start, test_start, df_test\n",
        "\n",
        "# Create the dataset for the first rolling window period\n",
        "if not run_shap:\n",
        "    train_x, train_y, val_x, val_y, test_x, test_y, train_start, val_start, test_start, df_test = create_rw_dataset(df=df)\n",
        "    if hyperparameter_search:\n",
        "        del df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "inutSrd8Tnko"
      },
      "source": [
        "# Model construction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "gewRG-rHTnkp"
      },
      "outputs": [],
      "source": [
        "def create_model(config):\n",
        "    '''Builds a MLP model of minimum 2 layers sequentially from a given config dictionary'''\n",
        "\n",
        "    # Input layers\n",
        "    bs_vars = Input((config.Num_features,))\n",
        "\n",
        "    # MLP layers\n",
        "    layers = Dense(config.MLP_units)(bs_vars)\n",
        "    layers = BatchNormalization(momentum=config.Bn_momentum)(layers)\n",
        "    layers = LeakyReLU(alpha=config.Leaky_relu_alpha)(layers)\n",
        "    \n",
        "    for _ in range(config.MLP_layers-1):\n",
        "        layers = Dense(config.MLP_units)(layers)\n",
        "        layers = BatchNormalization(momentum=config.Bn_momentum)(layers)\n",
        "        layers = LeakyReLU(alpha=config.Leaky_relu_alpha)(layers)\n",
        "\n",
        "    output = Dense(1, activation='relu')(layers)\n",
        "\n",
        "    # Exponential decaying learning rate\n",
        "    lr_schedule = ExponentialDecay(\n",
        "        initial_learning_rate = config.Lr,\n",
        "        decay_steps = int(len(train_x)/config.Minibatch_size),\n",
        "        decay_rate=config.Lr_decay\n",
        "    )\n",
        "\n",
        "    # Compile model\n",
        "    model = Model(inputs=bs_vars, outputs=output)\n",
        "    model.compile(loss='mse', optimizer=Adam(learning_rate=lr_schedule))\n",
        "\n",
        "    model.summary()\n",
        "    return model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AogdJkrRTnkq"
      },
      "source": [
        "# Help functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "xf-ICRDKTnkr"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "from tensorflow.keras import backend as k\n",
        "\n",
        "class ClearMemory(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        gc.collect()\n",
        "        k.clear_session()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "v4CRxFJrTnkr"
      },
      "source": [
        "## Creating trainer function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "EZsaCBmrTnkr"
      },
      "outputs": [],
      "source": [
        "def trainer(train_x = train_x, train_y = train_y, val_x = val_x, val_y = val_y, config = None, project = None, checkpoint_path = None):\n",
        "    # Initialize a new wandb run\n",
        "    with wandb.init(config=config, project = project):\n",
        "\n",
        "        # If called by wandb.agent, as below,\n",
        "        # this config will be set by Sweep Controller\n",
        "        config = wandb.config\n",
        "\n",
        "        # Build model and create callbacks\n",
        "        model = create_model(config)\n",
        "\n",
        "        early_stopping = EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            mode='min',\n",
        "            min_delta = config.Min_delta,\n",
        "            patience = config.Patience,\n",
        "        )\n",
        "        \n",
        "        wandb_callback = WandbCallback(\n",
        "            monitor='val_loss',\n",
        "            mode='min',\n",
        "            save_model=False\n",
        "        )\n",
        "        \n",
        "        # Check if the checkpoint folder exists\n",
        "        if checkpoint_path and not os.path.exists(checkpoint_path):\n",
        "            # Create the checkpoint folder if it does not exist\n",
        "            os.makedirs(checkpoint_path)\n",
        "        \n",
        "        if checkpoint_path:\n",
        "          checkpoint = ModelCheckpoint(\n",
        "              filepath=checkpoint_path + \".h5\",\n",
        "              monitor='val_loss',\n",
        "              mode='min',\n",
        "              save_best_only=True,\n",
        "              save_weights_only=False\n",
        "          )\n",
        "\n",
        "        print(f'Train shape: {train_x.shape}')\n",
        "        print(f'Val shape: {val_x.shape}')\n",
        "\n",
        "        # Train model\n",
        "        model.fit(\n",
        "            train_x,\n",
        "            train_y,\n",
        "            batch_size = config.Minibatch_size,\n",
        "            validation_data = (val_x, val_y),\n",
        "            epochs = 1000,\n",
        "            callbacks = [early_stopping, wandb_callback, checkpoint, ClearMemory()] if checkpoint_path else [early_stopping, wandb_callback, ClearMemory()],\n",
        "        )\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Rqno7WFBTnkp"
      },
      "source": [
        "# Hyperparameter search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "__KmrL9HTnkp"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Malformed sweep config detected! This may cause your sweep to behave in unexpected ways.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m To avoid this, please fix the sweep config schema violations below:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m   Violation 1. Lr uses log_uniform, where min/max specify base-e exponents. Use log_uniform_values to specify limit values.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Create sweep with ID: vony09oh\n",
            "Sweep URL: https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20MLP/sweeps/vony09oh\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ey4rbj11 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tBn_momentum: 0.99\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tLeaky_relu_alpha: 0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tLr: 0.0002872795103650739\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tLr_decay: 0.8408169239701624\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tMLP_layers: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tMLP_units: 200\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tMin_delta: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tMinibatch_size: 4096\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tNum_features: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tPatience: 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tTraining_size: 991232\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.15.3 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Erlend\\Google Drive\\NTNU\\5. Klasse\\Master\\Git-folder\\deep-learning-for-option-pricing\\LSTM-MLP_notebooks\\wandb\\run-20230524_172904-ey4rbj11</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20MLP/runs/ey4rbj11\" target=\"_blank\">denim-sweep-1</a></strong> to <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20MLP\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20MLP/sweeps/vony09oh\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20MLP/sweeps/vony09oh</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 5)]               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 200)               1200      \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 200)              800       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 200)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 200)               40200     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 200)              800       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 200)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 201       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 43,201\n",
            "Trainable params: 42,401\n",
            "Non-trainable params: 800\n",
            "_________________________________________________________________\n",
            "Train shape: (991232, 5)\n",
            "Val shape: (61356, 5)\n",
            "Epoch 1/1000\n",
            "242/242 [==============================] - 22s 85ms/step - loss: 101812.6484 - val_loss: 166041.0469\n",
            "Epoch 2/1000\n",
            "242/242 [==============================] - 21s 87ms/step - loss: 94900.6484 - val_loss: 155915.4844\n",
            "Epoch 3/1000\n",
            "242/242 [==============================] - 19s 78ms/step - loss: 89225.4531 - val_loss: 134474.7656\n",
            "Epoch 4/1000\n",
            "242/242 [==============================] - 15s 62ms/step - loss: 84108.1719 - val_loss: 121633.1406\n",
            "Epoch 5/1000\n",
            "242/242 [==============================] - 15s 61ms/step - loss: 79594.7578 - val_loss: 116541.0859\n",
            "Epoch 6/1000\n",
            "242/242 [==============================] - 17s 70ms/step - loss: 75648.3750 - val_loss: 110952.9922\n",
            "Epoch 7/1000\n",
            "242/242 [==============================] - 19s 78ms/step - loss: 72259.0156 - val_loss: 105369.9766\n",
            "Epoch 8/1000\n",
            "242/242 [==============================] - 17s 68ms/step - loss: 69406.7188 - val_loss: 102634.7812\n",
            "Epoch 9/1000\n",
            "242/242 [==============================] - 18s 73ms/step - loss: 67018.8516 - val_loss: 98110.7266\n",
            "Epoch 10/1000\n",
            "242/242 [==============================] - 22s 88ms/step - loss: 65023.4141 - val_loss: 96476.6328\n",
            "Epoch 11/1000\n",
            "242/242 [==============================] - 16s 67ms/step - loss: 63355.1406 - val_loss: 93606.7500\n",
            "Epoch 12/1000\n",
            "119/242 [=============>................] - ETA: 9s - loss: 62122.6211"
          ]
        }
      ],
      "source": [
        "# Configuring the sweep hyperparameter search space\n",
        "sweep_configuration = {\n",
        "    'method': 'random',\n",
        "    'name': 'MLP v1.0',\n",
        "    'metric': {\n",
        "        'goal': 'minimize', \n",
        "        'name': 'val_loss'\n",
        "\t\t},\n",
        "    'parameters': {\n",
        "        'MLP_units': {\n",
        "            'values': [100, 200, 400, 600]},        \n",
        "        'MLP_layers': {\n",
        "            'distribution': 'int_uniform',\n",
        "            'max': 7, 'min': 2},\n",
        "        'Bn_momentum': {\n",
        "            'values': [0.1, 0.4, 0.7, 0.99]},\n",
        "        'Lr': {\n",
        "            'distribution': 'uniform',\n",
        "            'max': 0.1, 'min': 0.001},\n",
        "        'Lr_decay': {\n",
        "            'distribution': 'uniform',\n",
        "            'max': 1, 'min': 0.8},\n",
        "        'Leaky_relu_alpha': {\n",
        "            'values': [0.01, 0.05, 0.1, 0.3]},\n",
        "        'Minibatch_size': {\n",
        "            'value': 4096},\n",
        "        'Min_delta': {\n",
        "            'value': 1},\n",
        "        'Patience': {\n",
        "            'value': 20},\n",
        "        'Num_features': {\n",
        "            'value':  5},\n",
        "        'Training_size': {\n",
        "            'value': training_size},\n",
        "    }\n",
        "}\n",
        "\n",
        "# Initialize sweep and creating sweepID\n",
        "\n",
        "# If new sweep, uncomment the line below and comment the line after it\n",
        "sweep_id = wandb.sweep(sweep=sweep_configuration, project='Deep learning for option pricing - MLP') \n",
        "#sweep_id = 'kdb4nbpi'\n",
        "\n",
        "if hyperparameter_search:\n",
        "    wandb.agent(sweep_id=sweep_id, function=trainer, project='Deep learning for option pricing - MLP', count = 1000)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zaS4BiSNTnks"
      },
      "source": [
        "# Rolling window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "ifDAjXS3Tnks"
      },
      "outputs": [],
      "source": [
        "def calculate_error(predictions, original):\n",
        "    m = MeanSquaredError()\n",
        "    m.update_state(predictions, original)\n",
        "    print(\"MSE:\", m.result().numpy())\n",
        "    m = RootMeanSquaredError()\n",
        "    m.update_state(predictions, original)\n",
        "    print(\"RMSE:\", m.result().numpy())\n",
        "\n",
        "class config_object:\n",
        "    def __init__(self, config):\n",
        "        self.MLP_units = config['MLP_units']\n",
        "        self.MLP_layers = config['MLP_layers']\n",
        "        self.Bn_momentum = config['Bn_momentum']\n",
        "        self.Lr = config['Lr']\n",
        "        self.Lr_decay = config['Lr_decay']\n",
        "        self.Minibatch_size = config['Minibatch_size']\n",
        "        self.Min_delta = config['Min_delta']\n",
        "        self.Patience = config['Patience']\n",
        "        self.Num_features = config['Num_features']\n",
        "        self.Architecture = config['Architecture']\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "lRV8wlklTnks"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint time: 05-24_16-50\n",
            "--------------Dataframe dates--------------\n",
            "Train: 2011-12-01 - 2014-11-28\n",
            "Val: 2014-12-01 - 2014-12-31\n",
            "Test: 2015-01-02 - 2015-01-30\n",
            "-------------------------------------------\n",
            "Train shape: (991232, 5),\n",
            "Val shape: (61356, 5),\n",
            "Test shape: (56797, 5),\n",
            "-------------------------------------------\n",
            "Window 1 of 99\n",
            "Training start:  2011-12-01 Validation start:  2014-12-01 Test start:  2015-01-01\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.15.3 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Erlend\\Google Drive\\NTNU\\5. Klasse\\Master\\Git-folder\\deep-learning-for-option-pricing\\LSTM-MLP_notebooks\\wandb\\run-20230524_165027-1f6iee09</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20MLP%20rw/runs/1f6iee09\" target=\"_blank\">misty-thunder-5</a></strong> to <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20MLP%20rw\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 5)]               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 200)               1200      \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 200)              800       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 200)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 200)               40200     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 200)              800       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 200)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 200)               40200     \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 200)              800       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 200)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 201       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 84,201\n",
            "Trainable params: 83,001\n",
            "Non-trainable params: 1,200\n",
            "_________________________________________________________________\n",
            "Train shape: (991232, 5)\n",
            "Val shape: (61356, 5)\n",
            "Epoch 1/1000\n",
            "242/242 [==============================] - 19s 76ms/step - loss: 4986.5962 - val_loss: 91.7042\n",
            "Epoch 2/1000\n",
            "242/242 [==============================] - 18s 75ms/step - loss: 54.5133 - val_loss: 172.0466\n",
            "Epoch 3/1000\n",
            "242/242 [==============================] - 19s 78ms/step - loss: 38.1930 - val_loss: 39.6185\n",
            "Epoch 4/1000\n",
            "242/242 [==============================] - 19s 78ms/step - loss: 33.3617 - val_loss: 113.8533\n",
            "Epoch 5/1000\n",
            "242/242 [==============================] - 19s 78ms/step - loss: 36.8523 - val_loss: 31.3585\n",
            "Epoch 6/1000\n",
            "242/242 [==============================] - 20s 83ms/step - loss: 33.6398 - val_loss: 29.4823\n",
            "Epoch 7/1000\n",
            "242/242 [==============================] - 20s 81ms/step - loss: 29.1593 - val_loss: 37.9160\n",
            "Epoch 8/1000\n",
            "242/242 [==============================] - 20s 80ms/step - loss: 33.8964 - val_loss: 51.2385\n",
            "Epoch 9/1000\n",
            "242/242 [==============================] - 19s 79ms/step - loss: 31.7593 - val_loss: 53.7912\n",
            "Epoch 10/1000\n",
            "242/242 [==============================] - 19s 77ms/step - loss: 29.6426 - val_loss: 181.1104\n",
            "Epoch 11/1000\n",
            "242/242 [==============================] - 19s 79ms/step - loss: 26.8835 - val_loss: 62.7654\n",
            "Epoch 12/1000\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 28.9462 - val_loss: 25.7908\n",
            "Epoch 13/1000\n",
            "242/242 [==============================] - 22s 91ms/step - loss: 26.7390 - val_loss: 40.7741\n",
            "Epoch 14/1000\n",
            "242/242 [==============================] - 20s 83ms/step - loss: 27.8655 - val_loss: 42.3842\n",
            "Epoch 15/1000\n",
            "242/242 [==============================] - 20s 83ms/step - loss: 26.4967 - val_loss: 47.5978\n",
            "Epoch 16/1000\n",
            "242/242 [==============================] - 20s 82ms/step - loss: 24.6737 - val_loss: 37.5812\n",
            "Epoch 17/1000\n",
            "242/242 [==============================] - 20s 81ms/step - loss: 25.6471 - val_loss: 57.3130\n",
            "Epoch 18/1000\n",
            "242/242 [==============================] - 20s 83ms/step - loss: 24.5677 - val_loss: 50.8948\n",
            "Epoch 19/1000\n",
            "242/242 [==============================] - 20s 82ms/step - loss: 22.7498 - val_loss: 95.9670\n",
            "Epoch 20/1000\n",
            "242/242 [==============================] - 19s 79ms/step - loss: 20.6032 - val_loss: 42.9982\n",
            "Epoch 21/1000\n",
            "242/242 [==============================] - 20s 84ms/step - loss: 22.7555 - val_loss: 43.3627\n",
            "Epoch 22/1000\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 27.0398 - val_loss: 35.2997\n",
            "Epoch 23/1000\n",
            "242/242 [==============================] - 20s 81ms/step - loss: 23.2831 - val_loss: 60.2538\n",
            "Epoch 24/1000\n",
            "242/242 [==============================] - 20s 81ms/step - loss: 26.4801 - val_loss: 87.5528\n",
            "Epoch 25/1000\n",
            "242/242 [==============================] - 21s 85ms/step - loss: 23.0857 - val_loss: 63.3413\n",
            "Epoch 26/1000\n",
            "242/242 [==============================] - 20s 83ms/step - loss: 21.1305 - val_loss: 53.6140\n",
            "Epoch 27/1000\n",
            "242/242 [==============================] - 25s 103ms/step - loss: 22.6647 - val_loss: 131.5404\n",
            "Epoch 28/1000\n",
            "242/242 [==============================] - 24s 100ms/step - loss: 24.3635 - val_loss: 35.5864\n",
            "Epoch 29/1000\n",
            "242/242 [==============================] - 20s 82ms/step - loss: 23.7505 - val_loss: 39.5233\n",
            "Epoch 30/1000\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 22.0218 - val_loss: 51.1801\n",
            "Epoch 31/1000\n",
            "242/242 [==============================] - 20s 83ms/step - loss: 25.0275 - val_loss: 35.3107\n",
            "Epoch 32/1000\n",
            "242/242 [==============================] - 21s 84ms/step - loss: 21.4155 - val_loss: 57.8424\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2fb448069014470388f034765b5f01fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.003 MB of 0.012 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.230645…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▄█▂▅▁▁▂▂▂█▃▁▂▂▂▂▂▂▄▂▂▁▃▄▃▂▆▁▂▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>11</td></tr><tr><td>best_val_loss</td><td>25.79076</td></tr><tr><td>epoch</td><td>31</td></tr><tr><td>loss</td><td>21.41548</td></tr><tr><td>val_loss</td><td>57.84241</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">misty-thunder-5</strong>: <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20MLP%20rw/runs/1f6iee09\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20MLP%20rw/runs/1f6iee09</a><br/>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20230524_165027-1f6iee09\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Predictions for test_start 2015-01-01 ---\n",
            "MSE: 16.829733\n",
            "RMSE: 4.1024055\n",
            "-------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------Dataframe dates--------------\n",
            "Train: 2012-01-03 - 2014-12-31\n",
            "Val: 2015-01-02 - 2015-01-30\n",
            "Test: 2015-02-02 - 2015-02-27\n",
            "-------------------------------------------\n",
            "Train shape: (991232, 5),\n",
            "Val shape: (56797, 5),\n",
            "Test shape: (57283, 5),\n",
            "-------------------------------------------\n",
            "Window 2 of 99\n",
            "Training start:  2012-01-01 Validation start:  2015-01-01 Test start:  2015-02-01\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.15.3 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Erlend\\Google Drive\\NTNU\\5. Klasse\\Master\\Git-folder\\deep-learning-for-option-pricing\\LSTM-MLP_notebooks\\wandb\\run-20230524_170124-2bk2aaj5</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20MLP%20rw/runs/2bk2aaj5\" target=\"_blank\">glad-pyramid-6</a></strong> to <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20MLP%20rw\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 5)]               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 200)               1200      \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 200)              800       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 200)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 200)               40200     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 200)              800       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 200)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 200)               40200     \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 200)              800       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 200)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 201       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 84,201\n",
            "Trainable params: 83,001\n",
            "Non-trainable params: 1,200\n",
            "_________________________________________________________________\n",
            "Train shape: (991232, 5)\n",
            "Val shape: (56797, 5)\n",
            "Epoch 1/1000\n",
            "242/242 [==============================] - 21s 82ms/step - loss: 5203.5298 - val_loss: 67.8366\n",
            "Epoch 2/1000\n",
            "242/242 [==============================] - 21s 87ms/step - loss: 45.7349 - val_loss: 37.0818\n",
            "Epoch 3/1000\n",
            "242/242 [==============================] - 21s 84ms/step - loss: 41.4523 - val_loss: 87.0830\n",
            "Epoch 4/1000\n",
            "242/242 [==============================] - 20s 81ms/step - loss: 38.7058 - val_loss: 14.3062\n",
            "Epoch 5/1000\n",
            "242/242 [==============================] - 21s 84ms/step - loss: 36.7918 - val_loss: 34.4037\n",
            "Epoch 6/1000\n",
            "242/242 [==============================] - 22s 89ms/step - loss: 33.8030 - val_loss: 31.6508\n",
            "Epoch 7/1000\n",
            "242/242 [==============================] - 21s 85ms/step - loss: 34.1720 - val_loss: 28.0992\n",
            "Epoch 8/1000\n",
            "242/242 [==============================] - 21s 87ms/step - loss: 35.0990 - val_loss: 69.3865\n",
            "Epoch 9/1000\n",
            "242/242 [==============================] - 96s 397ms/step - loss: 29.3715 - val_loss: 40.1469\n",
            "Epoch 10/1000\n",
            "242/242 [==============================] - 77s 313ms/step - loss: 30.4607 - val_loss: 22.9516\n",
            "Epoch 11/1000\n",
            "242/242 [==============================] - 64s 261ms/step - loss: 29.1896 - val_loss: 36.3194\n",
            "Epoch 12/1000\n",
            "242/242 [==============================] - 72s 291ms/step - loss: 31.0906 - val_loss: 54.1885\n",
            "Epoch 13/1000\n",
            "242/242 [==============================] - 48s 193ms/step - loss: 28.3858 - val_loss: 15.3122\n",
            "Epoch 14/1000\n",
            "242/242 [==============================] - 43s 174ms/step - loss: 29.5804 - val_loss: 29.5038\n",
            "Epoch 15/1000\n",
            "242/242 [==============================] - 48s 194ms/step - loss: 29.0229 - val_loss: 22.4223\n",
            "Epoch 16/1000\n",
            "242/242 [==============================] - 53s 218ms/step - loss: 24.8834 - val_loss: 43.4036\n",
            "Epoch 17/1000\n",
            "242/242 [==============================] - 57s 235ms/step - loss: 26.9509 - val_loss: 61.7366\n",
            "Epoch 18/1000\n",
            "242/242 [==============================] - 59s 243ms/step - loss: 26.2091 - val_loss: 23.8979\n",
            "Epoch 19/1000\n",
            "242/242 [==============================] - 57s 230ms/step - loss: 23.8271 - val_loss: 40.2101\n",
            "Epoch 20/1000\n",
            "242/242 [==============================] - 46s 189ms/step - loss: 25.8427 - val_loss: 30.1925\n",
            "Epoch 21/1000\n",
            "242/242 [==============================] - 37s 146ms/step - loss: 23.5245 - val_loss: 33.0471\n",
            "Epoch 22/1000\n",
            "242/242 [==============================] - 34s 138ms/step - loss: 24.9399 - val_loss: 36.4132\n",
            "Epoch 23/1000\n",
            "242/242 [==============================] - 32s 130ms/step - loss: 24.1792 - val_loss: 35.9207\n",
            "Epoch 24/1000\n",
            "242/242 [==============================] - 30s 124ms/step - loss: 23.7125 - val_loss: 19.9201\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8165a4549c904a8cb9423cc7df47f060",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇██</td></tr><tr><td>loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▆▃█▁▃▃▂▆▃▂▃▅▁▂▂▄▆▂▃▃▃▃▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>3</td></tr><tr><td>best_val_loss</td><td>14.30619</td></tr><tr><td>epoch</td><td>23</td></tr><tr><td>loss</td><td>23.71253</td></tr><tr><td>val_loss</td><td>19.92007</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">glad-pyramid-6</strong>: <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20MLP%20rw/runs/2bk2aaj5\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20MLP%20rw/runs/2bk2aaj5</a><br/>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20230524_170124-2bk2aaj5\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Predictions for test_start 2015-02-01 ---\n",
            "MSE: 22.372278\n",
            "RMSE: 4.729934\n",
            "-------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------Dataframe dates--------------\n",
            "Train: 2012-02-01 - 2015-01-30\n",
            "Val: 2015-02-02 - 2015-02-27\n",
            "Test: 2015-03-02 - 2015-03-31\n",
            "-------------------------------------------\n",
            "Train shape: (991232, 5),\n",
            "Val shape: (57283, 5),\n",
            "Test shape: (74992, 5),\n",
            "-------------------------------------------\n",
            "Window 3 of 99\n",
            "Training start:  2012-02-01 Validation start:  2015-02-01 Test start:  2015-03-01\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.15.3 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Erlend\\Google Drive\\NTNU\\5. Klasse\\Master\\Git-folder\\deep-learning-for-option-pricing\\LSTM-MLP_notebooks\\wandb\\run-20230524_171838-1b40zrpr</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20MLP%20rw/runs/1b40zrpr\" target=\"_blank\">robust-paper-7</a></strong> to <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20MLP%20rw\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 5)]               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 200)               1200      \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 200)              800       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 200)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 200)               40200     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 200)              800       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 200)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 200)               40200     \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 200)              800       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 200)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 201       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 84,201\n",
            "Trainable params: 83,001\n",
            "Non-trainable params: 1,200\n",
            "_________________________________________________________________\n",
            "Train shape: (991232, 5)\n",
            "Val shape: (57283, 5)\n",
            "Epoch 1/1000\n",
            "242/242 [==============================] - 33s 130ms/step - loss: 5346.0425 - val_loss: 70.9504\n",
            "Epoch 2/1000\n",
            "242/242 [==============================] - 30s 122ms/step - loss: 45.3806 - val_loss: 23.1810\n",
            "Epoch 3/1000\n",
            "242/242 [==============================] - 29s 120ms/step - loss: 51.0391 - val_loss: 31.7738\n",
            "Epoch 4/1000\n",
            "242/242 [==============================] - 30s 122ms/step - loss: 40.6908 - val_loss: 12.8830\n",
            "Epoch 5/1000\n",
            "242/242 [==============================] - 30s 121ms/step - loss: 34.7279 - val_loss: 20.4652\n",
            "Epoch 6/1000\n",
            "242/242 [==============================] - 30s 123ms/step - loss: 34.3423 - val_loss: 11.1288\n",
            "Epoch 7/1000\n",
            "242/242 [==============================] - 29s 119ms/step - loss: 38.8531 - val_loss: 13.5311\n",
            "Epoch 8/1000\n",
            "242/242 [==============================] - 31s 126ms/step - loss: 35.2822 - val_loss: 31.2983\n",
            "Epoch 9/1000\n",
            "242/242 [==============================] - 29s 121ms/step - loss: 28.0437 - val_loss: 19.3647\n",
            "Epoch 10/1000\n",
            "242/242 [==============================] - 30s 123ms/step - loss: 36.8424 - val_loss: 27.6425\n",
            "Epoch 11/1000\n",
            "242/242 [==============================] - 31s 126ms/step - loss: 32.2778 - val_loss: 15.8880\n",
            "Epoch 12/1000\n",
            "242/242 [==============================] - 32s 130ms/step - loss: 34.3144 - val_loss: 13.4526\n",
            "Epoch 13/1000\n",
            "242/242 [==============================] - 24s 98ms/step - loss: 32.3552 - val_loss: 28.0120\n",
            "Epoch 14/1000\n",
            "242/242 [==============================] - 19s 77ms/step - loss: 28.6550 - val_loss: 8.2515\n",
            "Epoch 15/1000\n",
            "242/242 [==============================] - 19s 78ms/step - loss: 29.2365 - val_loss: 26.5906\n",
            "Epoch 16/1000\n",
            "242/242 [==============================] - 19s 77ms/step - loss: 28.2182 - val_loss: 48.8276\n",
            "Epoch 17/1000\n",
            "242/242 [==============================] - 22s 90ms/step - loss: 28.4931 - val_loss: 9.4030\n",
            "Epoch 18/1000\n",
            "242/242 [==============================] - 24s 96ms/step - loss: 31.4928 - val_loss: 24.9871\n",
            "Epoch 19/1000\n",
            "242/242 [==============================] - 23s 96ms/step - loss: 26.8601 - val_loss: 10.8016\n",
            "Epoch 20/1000\n",
            "242/242 [==============================] - 22s 92ms/step - loss: 27.0083 - val_loss: 22.7594\n",
            "Epoch 21/1000\n",
            "183/242 [=====================>........] - ETA: 5s - loss: 27.7315"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f68a2945ef3942f9807aabd1fc02036c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▄▂▂▁▂▄▂▃▂▂▃▁▃▆▁▃▁▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>13</td></tr><tr><td>best_val_loss</td><td>8.25153</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>loss</td><td>27.00829</td></tr><tr><td>val_loss</td><td>22.7594</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">robust-paper-7</strong>: <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20MLP%20rw/runs/1b40zrpr\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20MLP%20rw/runs/1b40zrpr</a><br/>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20230524_171838-1b40zrpr\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_56220\\487089501.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mrun_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mtrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproject\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Deep learning for option pricing - MLP rw'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[0mc_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_56220\\500472254.py\u001b[0m in \u001b[0;36mtrainer\u001b[1;34m(train_x, train_y, val_x, val_y, config, project, checkpoint_path)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;31m# Train model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         model.fit(\n\u001b[0m\u001b[0;32m     44\u001b[0m             \u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Erlend\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\wandb\\integration\\keras\\keras.py\u001b[0m in \u001b[0;36mnew_v2\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mcbk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcbks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m                 \u001b[0mset_wandb_attrs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcbk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mold_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[0mtraining_arrays\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morig_fit_loop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mold_arrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Erlend\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\wandb\\integration\\keras\\keras.py\u001b[0m in \u001b[0;36mnew_v2\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mcbk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcbks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m                 \u001b[0mset_wandb_attrs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcbk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mold_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[0mtraining_arrays\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morig_fit_loop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mold_arrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Erlend\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\wandb\\integration\\keras\\keras.py\u001b[0m in \u001b[0;36mnew_v2\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mcbk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcbks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m                 \u001b[0mset_wandb_attrs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcbk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mold_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[0mtraining_arrays\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morig_fit_loop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mold_arrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Erlend\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Erlend\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Erlend\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Erlend\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Erlend\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Erlend\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Erlend\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[1;32mc:\\Users\\Erlend\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Erlend\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "num_windows = 99\n",
        "\n",
        "config = {\n",
        "    'MLP_units': 200,\n",
        "    'MLP_layers': 3,\n",
        "    'Bn_momentum': 0.4,\n",
        "    'Lr': 0.0564,\n",
        "    'Lr_decay': 0.809,\n",
        "    'Leaky_relu_alpha': 0.1,\n",
        "    'Minibatch_size': 4096,\n",
        "    'Min_delta': 1,\n",
        "    'Patience': 20,\n",
        "    'Num_features': 5, \n",
        "    'Architecture': 'LSTM-MLP v1.0',\n",
        "}\n",
        "\n",
        "df_test_combined = pd.DataFrame()\n",
        "\n",
        "if not checkpoint_time:\n",
        "    checkpoint_time = datetime.now().strftime(\"%m-%d_%H-%M\")\n",
        "print(f'Checkpoint time: {checkpoint_time}')\n",
        "\n",
        "for window in range(starting_window, num_windows):\n",
        "    np.random.seed(random_seed)\n",
        "    tf.random.set_seed(random_seed)\n",
        "\n",
        "    train_x, train_y, val_x, val_y, test_x, test_y, train_start, val_start, test_start, df_test = create_rw_dataset(df=df, window_number=window)\n",
        "\n",
        "    print('-------------------------------------------')\n",
        "    print(f'Window {window + 1} of {num_windows}')\n",
        "    print(\"Training start: \", train_start, \"Validation start: \", val_start, \"Test start: \", test_start)\n",
        "        \n",
        "    if google_colab:\n",
        "        checkpoint_path = f'/content/drive/MyDrive/01. Masters Thesis - Shared/05. Checkpoints/{checkpoint_time}/{train_start}/'\n",
        "    else:\n",
        "        checkpoint_path = f'./checkpoints/{checkpoint_time}/{train_start}/'\n",
        "\n",
        "    config['Dataset'] = f'{train_start} - {val_start} - {test_start}'\n",
        "\n",
        "    if run_training:\n",
        "        trainer(train_x = train_x, train_y = train_y, val_x = val_x, val_y = val_y, config = config, project = 'Deep learning for option pricing - MLP rw', checkpoint_path = checkpoint_path)\n",
        "    c_model = load_model(checkpoint_path + \".h5\")\n",
        "    \n",
        "    # Make predictions\n",
        "    predictions = c_model(test_x)\n",
        "\n",
        "    print(f'--- Predictions for test_start {test_start} ---')\n",
        "    calculate_error(predictions, test_y)\n",
        "    print('-------------------------------------------')\n",
        "    \n",
        "    df_test[\"Prediction\"] = predictions\n",
        "    df_test_combined = pd.concat([df_test_combined, df_test[[\"Quote_date\", \"Price\", \"Prediction\"] + bs_vars]])\n",
        "    \n",
        "    del train_x\n",
        "    del train_y\n",
        "    del val_x\n",
        "    del val_y\n",
        "    del test_x\n",
        "    del test_y\n",
        "    del train_start\n",
        "    del val_start\n",
        "    del test_start\n",
        "    del df_test\n",
        "\n",
        "print(f\"--- All model predictions ---\")\n",
        "calculate_error(df_test_combined[\"Prediction\"], df_test_combined[\"Price\"])\n",
        "print(\"-------------------------------------------\")\n",
        "\n",
        "if google_colab == False:\n",
        "    predictions_path = './predictions/'\n",
        "    if checkpoint_path and not os.path.exists(predictions_path):\n",
        "        os.makedirs(predictions_path)\n",
        "    df_test_combined.to_csv(f'{predictions_path}{checkpoint_time}.csv')\n",
        "else:\n",
        "  path = f'/content/drive/MyDrive/01. Masters Thesis - Shared/06. Predictions/{checkpoint_time}.csv'\n",
        "  df_test_combined.to_csv(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a function to calculate RMSE\n",
        "def calculate_rmse(group):\n",
        "    rmse_model_1 = np.sqrt(mean_squared_error(group[\"Price\"], group[\"Prediction\"]))\n",
        "    return pd.Series({\"RMSE_Model_1\": rmse_model_1})\n",
        "\n",
        "# Group data by Quote_date and calculate RMSE for each group\n",
        "rmse_df = df_test_combined.groupby(\"Quote_date\").apply(calculate_rmse).reset_index()\n",
        "\n",
        "# Plot the RMSE values for each model\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "sns.lineplot(data=rmse_df, x=\"Quote_date\", y=\"RMSE_Model_1\", label=\"Model\", ax=ax)\n",
        "\n",
        "# Set the interval for x-axis labels\n",
        "ax.xaxis.set_major_locator(ticker.MultipleLocator(base=10))\n",
        "\n",
        "plt.xlabel(\"Quote_date\")\n",
        "plt.ylabel(\"RMSE\")\n",
        "plt.title(\"RMSE for Models\")\n",
        "plt.legend()\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
