{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights and Biases\n",
    "!pip install -q wandb\n",
    "# Tensorflow\n",
    "!pip install -q tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-22 10:13:51.038490: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, LSTM, Concatenate, Dense, BatchNormalization, LeakyReLU\n",
    "from keras.activations import tanh\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from tensorflow import square, reduce_mean\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.math import multiply\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvinje\u001b[0m (\u001b[33mavogadro\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/hjalmarjacobvinje/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If running in colab, insert your wandb key here\n",
    "\n",
    "#import config\n",
    "#Erlend\n",
    "#wandb.login(key=config.erlend_key)\n",
    "# Hjalmar\n",
    "wandb.login(key=config.hjalmar_key)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, split and normalize data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Quote_date</th>\n",
       "      <th>Price</th>\n",
       "      <th>Underlying_last</th>\n",
       "      <th>Strike</th>\n",
       "      <th>TTM</th>\n",
       "      <th>R</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>361.355</td>\n",
       "      <td>1462.33</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>336.600</td>\n",
       "      <td>1462.33</td>\n",
       "      <td>1125.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>311.350</td>\n",
       "      <td>1462.33</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>286.345</td>\n",
       "      <td>1462.33</td>\n",
       "      <td>1175.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>281.345</td>\n",
       "      <td>1462.33</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12459173</th>\n",
       "      <td>12459173</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>6.950</td>\n",
       "      <td>3839.81</td>\n",
       "      <td>8400.0</td>\n",
       "      <td>1085</td>\n",
       "      <td>4.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12459174</th>\n",
       "      <td>12459174</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>5.950</td>\n",
       "      <td>3839.81</td>\n",
       "      <td>8600.0</td>\n",
       "      <td>1085</td>\n",
       "      <td>4.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12459175</th>\n",
       "      <td>12459175</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>5.450</td>\n",
       "      <td>3839.81</td>\n",
       "      <td>8800.0</td>\n",
       "      <td>1085</td>\n",
       "      <td>4.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12459176</th>\n",
       "      <td>12459176</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>4.500</td>\n",
       "      <td>3839.81</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>1085</td>\n",
       "      <td>4.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12459177</th>\n",
       "      <td>12459177</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>4.025</td>\n",
       "      <td>3839.81</td>\n",
       "      <td>9200.0</td>\n",
       "      <td>1085</td>\n",
       "      <td>4.22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12459178 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Unnamed: 0  Quote_date    Price  Underlying_last  Strike   TTM     R\n",
       "0                  0  2013-01-02  361.355          1462.33  1100.0     2  0.07\n",
       "1                  1  2013-01-02  336.600          1462.33  1125.0     2  0.07\n",
       "2                  2  2013-01-02  311.350          1462.33  1150.0     2  0.07\n",
       "3                  3  2013-01-02  286.345          1462.33  1175.0     2  0.07\n",
       "4                  4  2013-01-02  281.345          1462.33  1180.0     2  0.07\n",
       "...              ...         ...      ...              ...     ...   ...   ...\n",
       "12459173    12459173  2022-12-30    6.950          3839.81  8400.0  1085  4.22\n",
       "12459174    12459174  2022-12-30    5.950          3839.81  8600.0  1085  4.22\n",
       "12459175    12459175  2022-12-30    5.450          3839.81  8800.0  1085  4.22\n",
       "12459176    12459176  2022-12-30    4.500          3839.81  9000.0  1085  4.22\n",
       "12459177    12459177  2022-12-30    4.025          3839.81  9200.0  1085  4.22\n",
       "\n",
       "[12459178 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "google_colab = True\n",
    "\n",
    "if google_colab:\n",
    "    import tensorflow as tf\n",
    "    # Pring info\n",
    "    gpu_info = !nvidia-smi\n",
    "    gpu_info = '\\n'.join(gpu_info)\n",
    "    if gpu_info.find('failed') >= 0:\n",
    "        print('Not connected to a GPU')\n",
    "    else:\n",
    "        print(gpu_info)\n",
    "    \n",
    "    from psutil import virtual_memory\n",
    "    ram_gb = virtual_memory().total / 1e9\n",
    "    print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "    if ram_gb < 20:\n",
    "        print('Not using a high-RAM runtime')\n",
    "    else:\n",
    "        print('You are using a high-RAM runtime!')\n",
    "\n",
    "    # Code to read csv file into Colaboratory:\n",
    "    !pip install -U -q PyDrive\n",
    "    from pydrive.auth import GoogleAuth\n",
    "    from pydrive.drive import GoogleDrive\n",
    "    from google.colab import auth\n",
    "    from oauth2client.client import GoogleCredentials\n",
    "    # Authenticate and create the PyDrive client.\n",
    "    auth.authenticate_user()\n",
    "    gauth = GoogleAuth()\n",
    "    gauth.credentials = GoogleCredentials.get_application_default()\n",
    "    drive = GoogleDrive(gauth)\n",
    "    id = \"1Ic73MqpS5ACG1p2oJ_R5cTuEhlQRM_Q6\"\n",
    "    downloaded = drive.CreateFile({'id':id}) \n",
    "    downloaded.GetContentFile('2013-2022_wo_lags.csv')  \n",
    "    df_read = pd.read_csv('2013-2022_wo_lags.csv')\n",
    "else:\n",
    "    file = \"../data/processed_data/2013-2022_wo_lags.csv\"\n",
    "    df_read = pd.read_csv(file)\n",
    "\n",
    "display(df_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
      "/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_35927/676062849.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n"
     ]
    }
   ],
   "source": [
    "df = df_read\n",
    "del df_read\n",
    "\n",
    "# Group the data by Quote Date and calculate the mean for Underlying Price\n",
    "df_agg = df.groupby('Quote_date').mean().reset_index()\n",
    "\n",
    "# Values to returns\n",
    "df_agg[\"Underlying_return\"] = df_agg[\"Underlying_last\"].pct_change()\n",
    "\n",
    "lags = 150\n",
    "\n",
    "# Add the Underlying Price Lag column\n",
    "for i in range(1, lags + 1):\n",
    "    df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
    "\n",
    "df = pd.merge(df, df_agg[['Quote_date', \"Underlying_return\"] + ['Underlying_' + str(i) for i in range(1, lags + 1)]], on='Quote_date', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (4907245, 90, 1), (4907245, 4)\n",
      "Val shape: (847486, 90, 1), (847486, 4)\n"
     ]
    }
   ],
   "source": [
    "# Format settings\n",
    "max_timesteps = 150\n",
    "moneyness = False # Moneyness = True WIP\n",
    "bs_vars = ['Moneyness', 'TTM', 'R'] if moneyness else ['Underlying_last', 'Strike', 'TTM', 'R']\n",
    "underlying_lags = ['Underlying_last'] + [f'Underlying_{i}' for i in range (1, max_timesteps)]\n",
    "\n",
    "# Create train, validation and test set split points\n",
    "train_start = datetime(2017,1,1) \n",
    "val_start = train_start + relativedelta(months=84) \n",
    "test_start = val_start + relativedelta(months=12)\n",
    "test_end = test_start + relativedelta(months=12)\n",
    "train_start = str(train_start.date())\n",
    "val_start = str(val_start.date())\n",
    "test_start = str(test_start.date())\n",
    "test_end = str(test_end.date())\n",
    "\n",
    "# Split train and validation data\n",
    "df_train = df[(df['Quote_date'] >= train_start) & ( df['Quote_date'] < val_start)]\n",
    "df_val = df[(df['Quote_date'] >= val_start) & ( df['Quote_date'] < test_start)]\n",
    "\n",
    "del df\n",
    "# Extract target values\n",
    "train_y = (df_train['Price']/df_train['Strike']).to_numpy() if moneyness else df_train['Price'].to_numpy()\n",
    "val_y = (df_val['Price']/df_val['Strike']).to_numpy() if moneyness else df_val['Price'].to_numpy()\n",
    "\n",
    "# If usining moneyness, extract strike\n",
    "if moneyness:\n",
    "    train_strike = df_train['Strike'].to_numpy()\n",
    "    val_strike = df_val['Strike'].to_numpy()\n",
    "\n",
    "# Convert dataframes to numpy arrays\n",
    "train_x = [df_train[underlying_lags].to_numpy(), df_train[bs_vars].to_numpy()]\n",
    "val_x = [df_val[underlying_lags].to_numpy(), df_val[bs_vars].to_numpy()]\n",
    "\n",
    "del df_train\n",
    "del df_val\n",
    "\n",
    "# Scale features based on training set\n",
    "underlying_scaler = MinMaxScaler()\n",
    "train_x[0] = underlying_scaler.fit_transform(train_x[0])\n",
    "val_x[0] = underlying_scaler.transform(val_x[0])\n",
    "\n",
    "bs_scaler = MinMaxScaler()\n",
    "train_x[1] = bs_scaler.fit_transform(train_x[1])\n",
    "val_x[1] = bs_scaler.transform(val_x[1])\n",
    "\n",
    "# Shuffle training set\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(len(train_x[0]))\n",
    "train_x = [train_x[0][shuffle], train_x[1][shuffle]]\n",
    "train_y = train_y[shuffle]\n",
    "if moneyness:\n",
    "    train_strike = train_strike[shuffle]\n",
    "\n",
    "# Reshape data to fit LSTM\n",
    "train_x = [train_x[0].reshape(len(train_x[0]), max_timesteps,1), train_x[1]]\n",
    "val_x = [val_x[0].reshape(len(val_x[0]), max_timesteps, 1), val_x[1]]\n",
    "\n",
    "print(f'Train shape: {train_x[0].shape}, {train_x[1].shape}')\n",
    "print(f'Val shape: {val_x[0].shape}, {val_x[1].shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(config):\n",
    "    '''Builds an LSTM-MLP model of minimum 2 layers sequentially from a given config dictionary'''\n",
    "\n",
    "    # Input layers\n",
    "    underlying_history = Input((config.LSTM_timesteps,1))\n",
    "    bs_vars = Input((config.Num_features,))\n",
    "\n",
    "    # LSTM layers\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(\n",
    "        units = config.LSTM_units,\n",
    "        activation = tanh,\n",
    "        input_shape = (config.LSTM_timesteps, 1),\n",
    "        return_sequences = True\n",
    "    ))\n",
    "\n",
    "    for _ in range(config.LSTM_layers - 2):\n",
    "        model.add(LSTM(\n",
    "            units = config.LSTM_units,\n",
    "            activation = tanh,\n",
    "            return_sequences = True\n",
    "        ))\n",
    "    \n",
    "    model.add(LSTM(\n",
    "        units = config.LSTM_units,\n",
    "        activation = tanh,\n",
    "        return_sequences = False\n",
    "    ))\n",
    "\n",
    "    # MLP layers\n",
    "    layers = Concatenate()([model(underlying_history), bs_vars])\n",
    "    \n",
    "    for _ in range(config.MLP_layers - 1):\n",
    "        layers = Dense(config.MLP_units)(layers)\n",
    "        layers = BatchNormalization(momentum=config.Bn_momentum)(layers)\n",
    "        layers = LeakyReLU()(layers)\n",
    "\n",
    "    output = Dense(1, activation='relu')(layers)\n",
    "\n",
    "    # Exponential decaying learning rate\n",
    "    lr_schedule = ExponentialDecay(\n",
    "        initial_learning_rate = config.Lr,\n",
    "        decay_steps = int(len(train_x[0])/config.Minibatch_size),\n",
    "        decay_rate=config.Lr_decay\n",
    "    )\n",
    "\n",
    "    # Compile model\n",
    "    model = Model(inputs=[underlying_history, bs_vars], outputs=output)\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=lr_schedule))\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter search setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Malformed sweep config detected! This may cause your sweep to behave in unexpected ways.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m To avoid this, please fix the sweep config schema violations below:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m   Violation 1. Lr_decay uses log_uniform, where min/max specify base-e exponents. Use log_uniform_values to specify limit values.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m   Violation 2. Lr uses log_uniform, where min/max specify base-e exponents. Use log_uniform_values to specify limit values.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: sua8kmo3\n",
      "Sweep URL: https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing/sweeps/sua8kmo3\n"
     ]
    }
   ],
   "source": [
    "# Configuring the sweep hyperparameter search space\n",
    "sweep_configuration = {\n",
    "    'method': 'random',\n",
    "    'name': 'LSTM-MLP v3.3: unfilted data, 2017-2022, single LSTM',\n",
    "    'metric': {\n",
    "        'goal': 'minimize', \n",
    "        'name': 'val_loss'\n",
    "\t\t},\n",
    "    'parameters': {\n",
    "        'LSTM_units': {\n",
    "            'values': [4, 8, 16, 32]},\n",
    "        'MLP_units': {\n",
    "            'values': [50, 100, 200, 400, 600]},\n",
    "        'LSTM_timesteps': {\n",
    "            'values': [10, 20, 40, 60, 90, 150]},\n",
    "        'LSTM_layers': {\n",
    "            'distribution': 'int_uniform',\n",
    "            'max': 8, 'min': 2},\n",
    "        'MLP_layers': {\n",
    "            'distribution': 'int_uniform',\n",
    "            'max': 8, 'min': 2},\n",
    "        'Bn_momentum': {\n",
    "            'values': [0.1, 0.4, 0.7, 0.99]},\n",
    "        'Lr': {\n",
    "            'distribution': 'log_uniform',\n",
    "            'max': log(0.1), 'min': log(0.0001)},\n",
    "        'Lr_decay': {\n",
    "            'distribution': 'log_uniform',\n",
    "            'max': log(1), 'min': log(0.8)},        \n",
    "        'Minibatch_size': {\n",
    "            'value': 4096},\n",
    "        'Min_delta': {\n",
    "            'value': 0.01 if moneyness else 1},\n",
    "        'Patience': {\n",
    "            'value': 20},\n",
    "        'Num_features': {\n",
    "            'value': 3 if moneyness else 4},\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize sweep and creating sweepID\n",
    "\n",
    "# If new sweep, uncomment the line below and comment the line after it\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project='Deep learning for option pricing') \n",
    "#sweep_id = '98bxt6oq'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WIP\n",
    "class MSE_LossCallback(Callback):\n",
    "    def __init__(self, train_x, train_y, train_strike, val_x, val_y, val_strike):\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "        self.train_strike = train_strike\n",
    "        self.val_x = val_x\n",
    "        self.val_y = val_y\n",
    "        self.val_strike = val_strike\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        train_pred = self.model(train_x)\n",
    "        val_pred = self.model(val_x)\n",
    "\n",
    "        train_mse = reduce_mean(square(multiply(train_pred[:,0] - self.train_y, self.train_strike)))\n",
    "        val_mse = reduce_mean(square(multiply(val_pred[:,0] - self.val_y, self.val_strike)))\n",
    "\n",
    "        print(f' Training scaled MSE: {train_mse}, Validation scaled MSE: {val_mse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the training and validation MSE loss on the actual option price when using price/strike as the target\n",
    "def MSE_loss(model, train_x, train_y, train_strike, val_x, val_y, val_strike):\n",
    "    train_pred = model(train_x)\n",
    "    val_pred = model(val_x)\n",
    "\n",
    "    train_mse = reduce_mean(square((train_pred[:,0] - train_y)*train_strike))\n",
    "    val_mse = reduce_mean(square((val_pred[:,0] - val_y)*val_strike))\n",
    "\n",
    "    print(f' Training scaled MSE: {train_mse}, Validation scaled MSE: {val_mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from tensorflow.keras import backend as k\n",
    "\n",
    "class ClearMemory(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        gc.collect()\n",
    "        k.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(train_x = train_x, train_y = train_y, val_x = val_x, val_y = val_y, config = None, project = None):\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config, project = project):\n",
    "\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "\n",
    "        # Build model and create callbacks\n",
    "        model = create_model(config)\n",
    "\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            min_delta = config.Min_delta,\n",
    "            patience = config.Patience,\n",
    "        )\n",
    "        \n",
    "        wandb_callback = WandbCallback(\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            save_model=False\n",
    "        )\n",
    "\n",
    "        if moneyness:\n",
    "            mse_callback = MSE_LossCallback(train_x, train_y, train_strike, val_x, val_y, val_strike)\n",
    "\n",
    "        # Adapt sequence length to config\n",
    "        train_x_adjusted = [train_x[0][:, :config.LSTM_timesteps, :], train_x[1]]\n",
    "        val_x_adjusted = [val_x[0][:, :config.LSTM_timesteps, :], val_x[1]]\n",
    "        print(f'Train shape: {train_x_adjusted[0].shape}, {train_x_adjusted[0].shape}')\n",
    "        print(f'Val shape: {val_x_adjusted[0].shape}, {val_x_adjusted[0].shape}')\n",
    "\n",
    "        # Train model\n",
    "        model.fit(\n",
    "            train_x_adjusted,\n",
    "            train_y,\n",
    "            batch_size = config.Minibatch_size,\n",
    "            validation_data = (val_x_adjusted, val_y),\n",
    "            epochs = 1000,\n",
    "            callbacks = [early_stopping, wandb_callback, mse_callback, ClearMemory()] if moneyness else [early_stopping, wandb_callback],\n",
    "        )\n",
    "\n",
    "        if moneyness:\n",
    "            MSE_loss(model, train_x, train_y, train_strike, val_x, val_y, val_strike)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run full sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5659veis with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBn_momentum: 0.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_layers: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_timesteps: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_units: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLr: 0.0003907476538068363\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLr_decay: 0.998004453429634\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tMLP_layers: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tMLP_units: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tMin_delta: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tMinibatch_size: 4096\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tNum_features: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tPatience: 20\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/hjalmarjacobvinje/Documents/deep-learning-for-option-pricing/LSTM-MLP_notebooks/wandb/run-20230222_103026-5659veis</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing/runs/5659veis\" target=\"_blank\">rose-sweep-1</a></strong> to <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page: <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing/sweeps/sua8kmo3\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing/sweeps/sua8kmo3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing/sweeps/sua8kmo3\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing/sweeps/sua8kmo3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing/runs/5659veis\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing/runs/5659veis</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-22 10:30:28.350560: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 20, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 16)           11712       ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 20)           0           ['sequential[0][0]',             \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 50)           1050        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 50)          200         ['dense[0][0]']                  \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 50)           0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 50)           2550        ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 50)          200         ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 50)           0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 50)           2550        ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 50)          200         ['dense_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 50)           0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 50)           2550        ['leaky_re_lu_2[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 50)          200         ['dense_3[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)      (None, 50)           0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 1)            51          ['leaky_re_lu_3[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 21,263\n",
      "Trainable params: 20,863\n",
      "Non-trainable params: 400\n",
      "__________________________________________________________________________________________________\n",
      "Train shape: (4907245, 20, 1), (4907245, 20, 1)\n",
      "Val shape: (847486, 20, 1), (847486, 20, 1)\n",
      "Epoch 1/1000\n",
      " 382/1199 [========>.....................] - ETA: 6:44 - loss: 370929.5000"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id=sweep_id, function=trainer, project='Deep learning for option pricing', count = 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'LSTM_units': 8,\n",
    "    'MLP_units': 100,\n",
    "    'LSTM_timesteps': 20,\n",
    "    'LSTM_layers': 3,\n",
    "    'MLP_layers': 4,\n",
    "    'Bn_momentum': 0.99,\n",
    "    'Lr': 0.01,\n",
    "    'Lr_decay': 1,\n",
    "    'Minibatch_size': 4096,\n",
    "    'Min_delta': 0.01 if moneyness else 1,\n",
    "    'Patience': 20,\n",
    "    'Num_features': 3 if moneyness else 4, \n",
    "    'Architecture': 'LSTM-MLP v.0.1',\n",
    "    'Dataset': f'Moneyness. Train_start: {train_start}, val_start: {val_start}, test_start: {test_start}',\n",
    "}\n",
    "trainer(config = config, project = 'Deep learning for option pricing - test area')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
