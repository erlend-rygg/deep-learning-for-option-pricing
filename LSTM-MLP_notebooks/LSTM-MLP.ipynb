{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "s2MNO3kPTnkd"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JlTtOWVUD6YH"
      },
      "source": [
        "### Model decisions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oOQkqhqFTnkg"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "\n",
            "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Weights and Biases\n",
        "!pip install -q wandb\n",
        "# Tensorflow\n",
        "!pip install -q tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4PZBtROGD6YJ"
      },
      "outputs": [],
      "source": [
        "previous_loading = False\n",
        "google_colab = False\n",
        "moneyness = False\n",
        "lags = 140\n",
        "hyperparameter_search = False\n",
        "training_size = 991232\n",
        "random_seed = 0\n",
        "finance_computers = False\n",
        "fc_path = 'M:/Master/'\n",
        "checkpoint_time = '11.05 1 mnd test sett full model run' # Insert checkpoint time if continuing training\n",
        "starting_window = 0 # Insert starting window if continuing training\n",
        "run_training = False\n",
        "run_shap = True\n",
        "run_prediction = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ovc8FFP6D6YJ"
      },
      "outputs": [],
      "source": [
        "if google_colab == True:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8xVVTnPD6YJ"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Bfwvf6vGTnki"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, LSTM, Concatenate, Dense, BatchNormalization, LeakyReLU\n",
        "from keras.activations import tanh\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from tensorflow import square, reduce_mean\n",
        "from tensorflow.keras.losses import MSE\n",
        "from tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.math import multiply\n",
        "from tensorflow.keras.metrics import MeanSquaredError, RootMeanSquaredError\n",
        "from math import log\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import shap\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1fduF6iTTnkj"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33merlendrygg\u001b[0m (\u001b[33mavogadro\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Erlend/.netrc\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# If running in colab, insert your wandb key here\n",
        "\n",
        "#import config\n",
        "#Erlend\n",
        "wandb.login(key='3cae81eb56be3190be5bb48c571e69933071df69')\n",
        "# Hjalmar\n",
        "#wandb.login(key=\"b47bcf387a0571c5520c58a13be35cda8ada0a99\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zJGDiGMLTnkl"
      },
      "source": [
        "# Load, split and normalize data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UI1j3mQWTnkl"
      },
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OKcl8WWJTnkl"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Quote_date</th>\n",
              "      <th>Price</th>\n",
              "      <th>Underlying_last</th>\n",
              "      <th>Strike</th>\n",
              "      <th>TTM</th>\n",
              "      <th>R</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2010-01-04</td>\n",
              "      <td>207.490</td>\n",
              "      <td>1132.99</td>\n",
              "      <td>925.0</td>\n",
              "      <td>0.008219</td>\n",
              "      <td>0.00050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2010-01-04</td>\n",
              "      <td>182.500</td>\n",
              "      <td>1132.99</td>\n",
              "      <td>950.0</td>\n",
              "      <td>0.008219</td>\n",
              "      <td>0.00050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2010-01-04</td>\n",
              "      <td>157.500</td>\n",
              "      <td>1132.99</td>\n",
              "      <td>975.0</td>\n",
              "      <td>0.008219</td>\n",
              "      <td>0.00050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>2010-01-04</td>\n",
              "      <td>132.600</td>\n",
              "      <td>1132.99</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>0.008219</td>\n",
              "      <td>0.00050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2010-01-04</td>\n",
              "      <td>107.705</td>\n",
              "      <td>1132.99</td>\n",
              "      <td>1025.0</td>\n",
              "      <td>0.008219</td>\n",
              "      <td>0.00050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12057638</th>\n",
              "      <td>13739049</td>\n",
              "      <td>2023-03-31</td>\n",
              "      <td>217.750</td>\n",
              "      <td>4109.88</td>\n",
              "      <td>4700.0</td>\n",
              "      <td>1.726027</td>\n",
              "      <td>0.04198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12057639</th>\n",
              "      <td>13739050</td>\n",
              "      <td>2023-03-31</td>\n",
              "      <td>180.000</td>\n",
              "      <td>4109.88</td>\n",
              "      <td>4800.0</td>\n",
              "      <td>1.726027</td>\n",
              "      <td>0.04198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12057640</th>\n",
              "      <td>13739051</td>\n",
              "      <td>2023-03-31</td>\n",
              "      <td>146.550</td>\n",
              "      <td>4109.88</td>\n",
              "      <td>4900.0</td>\n",
              "      <td>1.726027</td>\n",
              "      <td>0.04198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12057641</th>\n",
              "      <td>13739052</td>\n",
              "      <td>2023-03-31</td>\n",
              "      <td>118.200</td>\n",
              "      <td>4109.88</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>1.726027</td>\n",
              "      <td>0.04198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12057642</th>\n",
              "      <td>13739053</td>\n",
              "      <td>2023-03-31</td>\n",
              "      <td>94.400</td>\n",
              "      <td>4109.88</td>\n",
              "      <td>5100.0</td>\n",
              "      <td>1.726027</td>\n",
              "      <td>0.04198</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12057643 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          Unnamed: 0  Quote_date    Price  Underlying_last  Strike       TTM  \\\n",
              "0                  0  2010-01-04  207.490          1132.99   925.0  0.008219   \n",
              "1                  1  2010-01-04  182.500          1132.99   950.0  0.008219   \n",
              "2                  2  2010-01-04  157.500          1132.99   975.0  0.008219   \n",
              "3                  3  2010-01-04  132.600          1132.99  1000.0  0.008219   \n",
              "4                  4  2010-01-04  107.705          1132.99  1025.0  0.008219   \n",
              "...              ...         ...      ...              ...     ...       ...   \n",
              "12057638    13739049  2023-03-31  217.750          4109.88  4700.0  1.726027   \n",
              "12057639    13739050  2023-03-31  180.000          4109.88  4800.0  1.726027   \n",
              "12057640    13739051  2023-03-31  146.550          4109.88  4900.0  1.726027   \n",
              "12057641    13739052  2023-03-31  118.200          4109.88  5000.0  1.726027   \n",
              "12057642    13739053  2023-03-31   94.400          4109.88  5100.0  1.726027   \n",
              "\n",
              "                R  \n",
              "0         0.00050  \n",
              "1         0.00050  \n",
              "2         0.00050  \n",
              "3         0.00050  \n",
              "4         0.00050  \n",
              "...           ...  \n",
              "12057638  0.04198  \n",
              "12057639  0.04198  \n",
              "12057640  0.04198  \n",
              "12057641  0.04198  \n",
              "12057642  0.04198  \n",
              "\n",
              "[12057643 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "if google_colab:\n",
        "    import tensorflow as tf\n",
        "    # Print info\n",
        "    gpu_info = !nvidia-smi\n",
        "    gpu_info = '\\n'.join(gpu_info)\n",
        "    if gpu_info.find('failed') >= 0:\n",
        "        print('Not connected to a GPU')\n",
        "    else:\n",
        "        print(gpu_info)\n",
        "    \n",
        "    from psutil import virtual_memory\n",
        "    ram_gb = virtual_memory().total / 1e9\n",
        "    print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "    if ram_gb < 20:\n",
        "        print('Not using a high-RAM runtime')\n",
        "    else:\n",
        "        print('You are using a high-RAM runtime!')\n",
        "\n",
        "    # Code to read csv file into Colaboratory:\n",
        "    !pip install -U -q PyDrive\n",
        "    from pydrive.auth import GoogleAuth\n",
        "    from pydrive.drive import GoogleDrive\n",
        "    from google.colab import auth\n",
        "    from oauth2client.client import GoogleCredentials\n",
        "    # Authenticate and create the PyDrive client.\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)\n",
        "    id = \"1Doyuyo_VDOmJf0CLo5kl9XzMTfhGtxiR\"\n",
        "    downloaded = drive.CreateFile({'id':id}) \n",
        "    downloaded.GetContentFile('2010-2023_NSS_filtered_vF.csv')  \n",
        "    df_read = pd.read_csv('2010-2023_NSS_filtered_vF.csv')\n",
        "elif finance_computers:\n",
        "    file = fc_path + \"/data/processed_data/2010-2023_NSS_filtered_vF.csv\"\n",
        "    df_read = pd.read_csv(file)\n",
        "else:\n",
        "    file = \"../data/processed_data/2010-2023_NSS_filtered_vF.csv\"\n",
        "    df_read = pd.read_csv(file)\n",
        "\n",
        "display(df_read)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wdbRyWt1D6YM"
      },
      "source": [
        "### Create lags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CpcmhVq6Tnkm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         Unnamed: 0  Quote_date     Price  Underlying_last  Strike       TTM  \\\n",
            "0           6063430  2019-01-02  1209.300          2509.98  1300.0  0.005479   \n",
            "1           6063431  2019-01-02  1157.605          2509.98  1350.0  0.005479   \n",
            "2           6063432  2019-01-02  1108.345          2509.98  1400.0  0.005479   \n",
            "3           6063433  2019-01-02  1058.400          2509.98  1450.0  0.005479   \n",
            "4           6063434  2019-01-02  1007.600          2509.98  1500.0  0.005479   \n",
            "...             ...         ...       ...              ...     ...       ...   \n",
            "6688863    13739049  2023-03-31   217.750          4109.88  4700.0  1.726027   \n",
            "6688864    13739050  2023-03-31   180.000          4109.88  4800.0  1.726027   \n",
            "6688865    13739051  2023-03-31   146.550          4109.88  4900.0  1.726027   \n",
            "6688866    13739052  2023-03-31   118.200          4109.88  5000.0  1.726027   \n",
            "6688867    13739053  2023-03-31    94.400          4109.88  5100.0  1.726027   \n",
            "\n",
            "               R  Underlying_return  Underlying_1  Underlying_2  ...  \\\n",
            "0        0.02400                NaN           NaN           NaN  ...   \n",
            "1        0.02400                NaN           NaN           NaN  ...   \n",
            "2        0.02400                NaN           NaN           NaN  ...   \n",
            "3        0.02400                NaN           NaN           NaN  ...   \n",
            "4        0.02400                NaN           NaN           NaN  ...   \n",
            "...          ...                ...           ...           ...  ...   \n",
            "6688863  0.04198           0.014547      0.005628      0.014302  ...   \n",
            "6688864  0.04198           0.014547      0.005628      0.014302  ...   \n",
            "6688865  0.04198           0.014547      0.005628      0.014302  ...   \n",
            "6688866  0.04198           0.014547      0.005628      0.014302  ...   \n",
            "6688867  0.04198           0.014547      0.005628      0.014302  ...   \n",
            "\n",
            "         Underlying_131  Underlying_132  Underlying_133  Underlying_134  \\\n",
            "0                   NaN             NaN             NaN             NaN   \n",
            "1                   NaN             NaN             NaN             NaN   \n",
            "2                   NaN             NaN             NaN             NaN   \n",
            "3                   NaN             NaN             NaN             NaN   \n",
            "4                   NaN             NaN             NaN             NaN   \n",
            "...                 ...             ...             ...             ...   \n",
            "6688863         -0.0087       -0.017354       -0.011082        0.006893   \n",
            "6688864         -0.0087       -0.017354       -0.011082        0.006893   \n",
            "6688865         -0.0087       -0.017354       -0.011082        0.006893   \n",
            "6688866         -0.0087       -0.017354       -0.011082        0.006893   \n",
            "6688867         -0.0087       -0.017354       -0.011082        0.006893   \n",
            "\n",
            "         Underlying_135  Underlying_136  Underlying_137  Underlying_138  \\\n",
            "0                   NaN             NaN             NaN             NaN   \n",
            "1                   NaN             NaN             NaN             NaN   \n",
            "2                   NaN             NaN             NaN             NaN   \n",
            "3                   NaN             NaN             NaN             NaN   \n",
            "4                   NaN             NaN             NaN             NaN   \n",
            "...                 ...             ...             ...             ...   \n",
            "6688863       -0.007175       -0.011553        0.003606       -0.043355   \n",
            "6688864       -0.007175       -0.011553        0.003606       -0.043355   \n",
            "6688865       -0.007175       -0.011553        0.003606       -0.043355   \n",
            "6688866       -0.007175       -0.011553        0.003606       -0.043355   \n",
            "6688867       -0.007175       -0.011553        0.003606       -0.043355   \n",
            "\n",
            "         Underlying_139  Underlying_140  \n",
            "0                   NaN             NaN  \n",
            "1                   NaN             NaN  \n",
            "2                   NaN             NaN  \n",
            "3                   NaN             NaN  \n",
            "4                   NaN             NaN  \n",
            "...                 ...             ...  \n",
            "6688863        0.010645        0.015417  \n",
            "6688864        0.010645        0.015417  \n",
            "6688865        0.010645        0.015417  \n",
            "6688866        0.010645        0.015417  \n",
            "6688867        0.010645        0.015417  \n",
            "\n",
            "[6688868 rows x 148 columns]\n"
          ]
        }
      ],
      "source": [
        "df = df_read\n",
        "del df_read\n",
        "\n",
        "if hyperparameter_search:\n",
        "    df = df[df['Quote_date'] <= '2015-02-01']\n",
        "\n",
        "if run_shap:\n",
        "    df = df[df['Quote_date'] >= '2019-01-01']\n",
        "\n",
        "# Group the data by Quote Date and calculate the mean for Underlying Price\n",
        "df_agg = df.groupby('Quote_date').mean().reset_index()\n",
        "\n",
        "# Values to returns\n",
        "df_agg[\"Underlying_return\"] = df_agg[\"Underlying_last\"].pct_change()\n",
        "\n",
        "# Add the Underlying Price Lag column\n",
        "for i in range(1, lags + 1):\n",
        "    df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
        "\n",
        "df = pd.merge(df, df_agg[['Quote_date', 'Underlying_return'] + ['Underlying_' + str(i) for i in range(1, lags + 1)]], on='Quote_date', how='left')\n",
        "del df_agg\n",
        "\n",
        "# Filter df between 2011-09-01\n",
        "df = df[(df[\"Quote_date\"] >= \"2011-09-01\")] if hyperparameter_search else df[(df[\"Quote_date\"] >= \"2011-12-01\")]\n",
        "\n",
        "print(df)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "j1fuXIvmTnkn"
      },
      "source": [
        "### Format input data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YKAFyCmTTnko"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------Dataframe dates--------------\n",
            "Train: nan - nan\n",
            "Val: nan - nan\n",
            "Test: nan - nan\n",
            "-------------------------------------------\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by MinMaxScaler.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12644\\943711903.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_strike\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_strike\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_strike\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_rw_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m     \u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_rw_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhyperparameter_search\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12644\\943711903.py\u001b[0m in \u001b[0;36mcreate_rw_dataset\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;31m# Scale features based on training set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0munderlying_scaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m     \u001b[0mtrain_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munderlying_scaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[0mval_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munderlying_scaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mtest_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munderlying_scaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Erlend\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    850\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    851\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 852\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    853\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Erlend\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    414\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 416\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Erlend\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m         \u001b[0mfirst_pass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"n_samples_seen_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 453\u001b[1;33m         X = self._validate_data(\n\u001b[0m\u001b[0;32m    454\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m             \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfirst_pass\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Erlend\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    564\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 566\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    567\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Erlend\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    803\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    804\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 805\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    806\u001b[0m                 \u001b[1;34m\"Found array with %d sample(s) (shape=%s) while a\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    807\u001b[0m                 \u001b[1;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by MinMaxScaler."
          ]
        }
      ],
      "source": [
        "# Format settings\n",
        "max_timesteps = lags\n",
        "bs_vars = ['Moneyness', 'TTM', 'R'] if moneyness else ['Underlying_last', 'Strike', 'TTM', 'R']\n",
        "underlying_lags = [f'Underlying_{i}' for i in range (max_timesteps - 1, 0, -1)] + ['Underlying_return']\n",
        "\n",
        "def create_rw_dataset(window_number = 0, df = None, shap = False):\n",
        "    '''Creates dataset for a single rolling window period offsett by the window number'''\n",
        "\n",
        "    # Create train, validation and test set split points\n",
        "    test_months = 1\n",
        "    train_start = datetime(2011,9,1) + relativedelta(months=window_number * test_months) if hyperparameter_search else datetime(2011,12,1) + relativedelta(months=window_number * test_months)\n",
        "    val_start = train_start + relativedelta(months=3*12)\n",
        "    test_start = val_start + relativedelta(months = 1 + test_months) if hyperparameter_search else val_start + relativedelta(months = 1)\n",
        "    test_end = test_start + relativedelta(months=test_months)\n",
        "    train_start = str(train_start.date())\n",
        "    val_start = str(val_start.date())\n",
        "    test_start = str(test_start.date())\n",
        "    test_end = str(test_end.date())\n",
        "        \n",
        "    # Split train and validation data\n",
        "    df_train = df[(df['Quote_date'] >= train_start) & (df['Quote_date'] < val_start)]\n",
        "    df_val = df[(df['Quote_date'] >= val_start) & (df['Quote_date'] < test_start)]\n",
        "    df_test = df[(df['Quote_date'] >= test_start) & (df['Quote_date'] < test_end)]\n",
        "\n",
        "    del df\n",
        "\n",
        "    # Extract target values\n",
        "    train_y = (df_train['Price'] / df_train['Strike']).to_numpy() if moneyness else df_train['Price'].to_numpy()\n",
        "    val_y = (df_val['Price'] / df_val['Strike']).to_numpy() if moneyness else df_val['Price'].to_numpy()\n",
        "    test_y = (df_test['Price'] / df_test['Strike']).to_numpy() if moneyness else df_test['Price'].to_numpy()\n",
        "\n",
        "    # If usining moneyness, extract strike\n",
        "    if moneyness:\n",
        "        train_strike = df_train['Strike'].to_numpy()\n",
        "        val_strike = df_val['Strike'].to_numpy()\n",
        "        test_strike = df_test['Strike'].to_numpy()\n",
        "\n",
        "\n",
        "    # Print earliest and latest date in every dataframe used\n",
        "    print(\"--------------Dataframe dates--------------\")\n",
        "    print(f\"Train: {df_train['Quote_date'].min()} - {df_train['Quote_date'].max()}\")\n",
        "    print(f\"Val: {df_val['Quote_date'].min()} - {df_val['Quote_date'].max()}\")\n",
        "    print(f\"Test: {df_test['Quote_date'].min()} - {df_test['Quote_date'].max()}\")\n",
        "    print(\"-------------------------------------------\")\n",
        "\n",
        "    # Convert dataframes to numpy arrays\n",
        "    train_x = [df_train[underlying_lags].to_numpy(), df_train[bs_vars].to_numpy()]\n",
        "    val_x = [df_val[underlying_lags].to_numpy(), df_val[bs_vars].to_numpy()]\n",
        "    test_x = [df_test[underlying_lags].to_numpy(), df_test[bs_vars].to_numpy()]\n",
        "\n",
        "    del df_train\n",
        "    del df_val\n",
        "\n",
        "    # Scale features based on training set\n",
        "    underlying_scaler = MinMaxScaler()\n",
        "    train_x[0] = underlying_scaler.fit_transform(train_x[0].flatten().reshape(-1, 1)).reshape(train_x[0].shape)\n",
        "    val_x[0] = underlying_scaler.transform(val_x[0].flatten().reshape(-1,1)).reshape(val_x[0].shape)\n",
        "    test_x[0] = underlying_scaler.transform(test_x[0].flatten().reshape(-1,1)).reshape(test_x[0].shape)\n",
        "\n",
        "    bs_scaler = MinMaxScaler()\n",
        "    train_x[1] = bs_scaler.fit_transform(train_x[1])\n",
        "    val_x[1] = bs_scaler.transform(val_x[1])\n",
        "    test_x[1] = bs_scaler.transform(test_x[1])\n",
        "\n",
        "\n",
        "    # Shuffle training set\n",
        "    np.random.seed(random_seed)\n",
        "    shuffle = np.random.permutation(len(train_x[0]))\n",
        "    train_x = [train_x[0][shuffle], train_x[1][shuffle]]\n",
        "    train_y = train_y[shuffle]\n",
        "\n",
        "    # Extract training set\n",
        "    train_x = [train_x[0][:training_size], train_x[1][:training_size]]\n",
        "    train_y = train_y[:training_size]\n",
        "\n",
        "    if moneyness:\n",
        "        train_strike = train_strike[shuffle][:training_size]\n",
        "    \n",
        "    if shap:\n",
        "        return train_x, test_x, train_start, val_start, test_start\n",
        "\n",
        "    # Reshape data to fit LSTM\n",
        "    train_x = [train_x[0].reshape(len(train_x[0]), max_timesteps, 1), train_x[1]]\n",
        "    val_x = [val_x[0].reshape(len(val_x[0]), max_timesteps, 1), val_x[1]]\n",
        "    test_x = [test_x[0].reshape(len(test_x[0]), max_timesteps, 1), test_x[1]]\n",
        "\n",
        "    print(f'Train shape: {train_x[0].shape}, {train_x[1].shape}')\n",
        "    print(f'Val shape: {val_x[0].shape}, {val_x[1].shape}')\n",
        "    print(f'Test shape: {test_x[0].shape}, {test_x[1].shape}')\n",
        "\n",
        "    if moneyness:\n",
        "        return train_x, train_y, val_x, val_y, test_x, test_y, train_start, val_start, test_start, df_test, train_strike, val_strike, test_strike,\n",
        "    return train_x, train_y, val_x, val_y, test_x, test_y, train_start, val_start, test_start, df_test\n",
        "\n",
        "# Create the dataset for the first rolling window period\n",
        "if not run_shap:\n",
        "    if moneyness:\n",
        "        train_x, train_y, val_x, val_y, test_x, test_y, train_start, val_start, test_start, df_test, train_strike, val_strike, test_strike = create_rw_dataset(df=df)\n",
        "    else:\n",
        "        train_x, train_y, val_x, val_y, test_x, test_y, train_start, val_start, test_start, df_test = create_rw_dataset(df=df)\n",
        "        if hyperparameter_search:\n",
        "            del df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "inutSrd8Tnko"
      },
      "source": [
        "# Model construction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gewRG-rHTnkp"
      },
      "outputs": [],
      "source": [
        "def create_model(config):\n",
        "    '''Builds an LSTM-MLP model of minimum 2 layers sequentially from a given config dictionary'''\n",
        "\n",
        "    # Input layers\n",
        "    underlying_history = Input((config.LSTM_timesteps,1))\n",
        "    bs_vars = Input((config.Num_features,))\n",
        "\n",
        "    # LSTM layers\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(LSTM(\n",
        "        units = config.LSTM_units,\n",
        "        activation = tanh,\n",
        "        input_shape = (config.LSTM_timesteps, 1),\n",
        "        return_sequences = True\n",
        "    ))\n",
        "\n",
        "    for _ in range(config.LSTM_layers - 2):\n",
        "        model.add(LSTM(\n",
        "            units = config.LSTM_units,\n",
        "            activation = tanh,\n",
        "            return_sequences = True\n",
        "        ))\n",
        "    \n",
        "    model.add(LSTM(\n",
        "        units = config.Interface_units,\n",
        "        activation = tanh,\n",
        "        return_sequences = False\n",
        "    ))\n",
        "\n",
        "    # MLP layers\n",
        "    layers = Concatenate()([model(underlying_history), model(underlying_history), model(underlying_history), model(underlying_history), model(underlying_history), bs_vars])\n",
        "    \n",
        "    for _ in range(config.MLP_layers - 1):\n",
        "        layers = Dense(config.MLP_units)(layers)\n",
        "        layers = BatchNormalization(momentum=config.Bn_momentum)(layers)\n",
        "        layers = LeakyReLU(alpha=config.Leaky_relu_alpha)(layers)\n",
        "\n",
        "    output = Dense(1, activation='relu')(layers)\n",
        "\n",
        "    # Exponential decaying learning rate\n",
        "    lr_schedule = ExponentialDecay(\n",
        "        initial_learning_rate = config.Lr,\n",
        "        decay_steps = int(len(train_x[0])/config.Minibatch_size),\n",
        "        decay_rate=config.Lr_decay\n",
        "    )\n",
        "\n",
        "    # Compile model\n",
        "    model = Model(inputs=[underlying_history, bs_vars], outputs=output)\n",
        "    model.compile(loss='mse', optimizer=Adam(learning_rate=lr_schedule))\n",
        "\n",
        "    model.summary()\n",
        "    return model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AogdJkrRTnkq"
      },
      "source": [
        "# Help functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5ogZE0y_Tnkq"
      },
      "outputs": [],
      "source": [
        "# Calculate the training and validation MSE loss on the actual option price when using price/strike as the target\n",
        "def MSE_loss(model, train_x, train_y, train_strike, val_x, val_y, val_strike):\n",
        "    train_pred = model(train_x)\n",
        "    val_pred = model(val_x)\n",
        "\n",
        "    train_mse = reduce_mean(square((train_pred[:,0] - train_y)*train_strike))\n",
        "    val_mse = reduce_mean(square((val_pred[:,0] - val_y)*val_strike))\n",
        "\n",
        "    print(f' Training scaled MSE: {train_mse}, Validation scaled MSE: {val_mse}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xf-ICRDKTnkr"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "from tensorflow.keras import backend as k\n",
        "\n",
        "class ClearMemory(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        gc.collect()\n",
        "        k.clear_session()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "v4CRxFJrTnkr"
      },
      "source": [
        "## Creating trainer function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EZsaCBmrTnkr"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_x' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12644\\901324145.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproject\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_checkpoint_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;31m# Initialize a new wandb run\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mwandb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproject\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;31m# If called by wandb.agent, as below,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'train_x' is not defined"
          ]
        }
      ],
      "source": [
        "def trainer(train_x = train_x, train_y = train_y, val_x = val_x, val_y = val_y, config = None, project = None, previous_checkpoint_path = None, checkpoint_path = None):\n",
        "    # Initialize a new wandb run\n",
        "    with wandb.init(config=config, project = project):\n",
        "\n",
        "        # If called by wandb.agent, as below,\n",
        "        # this config will be set by Sweep Controller\n",
        "        config = wandb.config\n",
        "\n",
        "        # Build model and create callbacks\n",
        "        if previous_checkpoint_path and os.path.exists(previous_checkpoint_path + \".h5\"):\n",
        "            model = load_model(previous_checkpoint_path + \".h5\")\n",
        "        else:\n",
        "            model = create_model(config)\n",
        "\n",
        "        early_stopping = EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            mode='min',\n",
        "            min_delta = config.Min_delta,\n",
        "            patience = config.Patience,\n",
        "        )\n",
        "        \n",
        "        wandb_callback = WandbCallback(\n",
        "            monitor='val_loss',\n",
        "            mode='min',\n",
        "            save_model=False\n",
        "        )\n",
        "        \n",
        "        # Check if the checkpoint folder exists\n",
        "        if checkpoint_path and not os.path.exists(checkpoint_path):\n",
        "            # Create the checkpoint folder if it does not exist\n",
        "            os.makedirs(checkpoint_path)\n",
        "        \n",
        "        if checkpoint_path:\n",
        "          checkpoint = ModelCheckpoint(\n",
        "              filepath=checkpoint_path + \".h5\",\n",
        "              monitor='val_loss',\n",
        "              mode='min',\n",
        "              save_best_only=True,\n",
        "              save_weights_only=False\n",
        "          )\n",
        "\n",
        "        # Adapt sequence length to config\n",
        "        train_x_adjusted = [train_x[0][:, :config.LSTM_timesteps, :], train_x[1]]\n",
        "        val_x_adjusted = [val_x[0][:, :config.LSTM_timesteps, :], val_x[1]]\n",
        "        print(f'Train shape: {train_x_adjusted[0].shape}, {train_x_adjusted[0].shape}')\n",
        "        print(f'Val shape: {val_x_adjusted[0].shape}, {val_x_adjusted[0].shape}')\n",
        "\n",
        "        # Train model\n",
        "        model.fit(\n",
        "            train_x_adjusted,\n",
        "            train_y,\n",
        "            batch_size = config.Minibatch_size,\n",
        "            validation_data = (val_x_adjusted, val_y),\n",
        "            epochs = 1000,\n",
        "            callbacks = [early_stopping, wandb_callback, checkpoint, ClearMemory()] if checkpoint_path else [early_stopping, wandb_callback, ClearMemory()],\n",
        "        )\n",
        "\n",
        "        if moneyness:\n",
        "            MSE_loss(model, train_x, train_y, train_strike, val_x, val_y, val_strike)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Rqno7WFBTnkp"
      },
      "source": [
        "# Hyperparameter search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "__KmrL9HTnkp"
      },
      "outputs": [],
      "source": [
        "# Configuring the sweep hyperparameter search space\n",
        "sweep_configuration = {\n",
        "    'method': 'random',\n",
        "    'name': 'LSTM-MLP v7.0',\n",
        "    'metric': {\n",
        "        'goal': 'minimize', \n",
        "        'name': 'val_loss'\n",
        "\t\t},\n",
        "    'parameters': {\n",
        "        'LSTM_units': {\n",
        "            'values': [4, 8, 16, 32]},\n",
        "        'Interface_units': {\n",
        "            'values': [4, 8, 16, 32]},\n",
        "        'MLP_units': {\n",
        "            'values': [100, 200, 400, 600]},        \n",
        "        'LSTM_timesteps': {\n",
        "            'values': [20, 40, 80, 120]},\n",
        "        'LSTM_layers': {\n",
        "            'distribution': 'int_uniform',\n",
        "            'max': 7, 'min': 2},\n",
        "        'MLP_layers': {\n",
        "            'distribution': 'int_uniform',\n",
        "            'max': 7, 'min': 2},\n",
        "        'Bn_momentum': {\n",
        "            'values': [0.1, 0.4, 0.7, 0.99]},\n",
        "        'Lr': {\n",
        "            'distribution': 'log_uniform',\n",
        "            'max': log(0.1), 'min': log(0.0002)},\n",
        "        'Lr_decay': {\n",
        "            'distribution': 'uniform',\n",
        "            'max': 1, 'min': 0.8},\n",
        "        'Leaky_relu_alpha': {\n",
        "            'values': [0.01, 0.05, 0.1, 0.3]},\n",
        "        'Minibatch_size': {\n",
        "            'value': 4096},\n",
        "        'Min_delta': {\n",
        "            'value': 0.01 if moneyness else 1},\n",
        "        'Patience': {\n",
        "            'value': 20},\n",
        "        'Num_features': {\n",
        "            'value': 3 if moneyness else 4},\n",
        "        'Training_size': {\n",
        "            'value': training_size},\n",
        "    }\n",
        "}\n",
        "\n",
        "# Initialize sweep and creating sweepID\n",
        "\n",
        "# If new sweep, uncomment the line below and comment the line after it\n",
        "#sweep_id = wandb.sweep(sweep=sweep_configuration, project='Deep learning for option pricing') \n",
        "sweep_id = 'kdb4nbpi'\n",
        "\n",
        "if hyperparameter_search:\n",
        "    wandb.agent(sweep_id=sweep_id, function=trainer, project='Deep learning for option pricing', count = 1000)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zaS4BiSNTnks"
      },
      "source": [
        "# Rolling window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ifDAjXS3Tnks"
      },
      "outputs": [],
      "source": [
        "def calculate_error(predictions, original):\n",
        "    m = MeanSquaredError()\n",
        "    m.update_state(predictions, original)\n",
        "    print(\"MSE:\", m.result().numpy())\n",
        "    m = RootMeanSquaredError()\n",
        "    m.update_state(predictions, original)\n",
        "    print(\"RMSE:\", m.result().numpy())\n",
        "\n",
        "class config_object:\n",
        "    def __init__(self, config):\n",
        "        self.LSTM_units = config['LSTM_units']\n",
        "        self.Interface_units = config['Interface_units']\n",
        "        self.MLP_units = config['MLP_units']\n",
        "        self.LSTM_timesteps = config['LSTM_timesteps']\n",
        "        self.LSTM_layers = config['LSTM_layers']\n",
        "        self.MLP_layers = config['MLP_layers']\n",
        "        self.Bn_momentum = config['Bn_momentum']\n",
        "        self.Lr = config['Lr']\n",
        "        self.Lr_decay = config['Lr_decay']\n",
        "        self.Minibatch_size = config['Minibatch_size']\n",
        "        self.Min_delta = config['Min_delta']\n",
        "        self.Patience = config['Patience']\n",
        "        self.Num_features = config['Num_features']\n",
        "        self.Architecture = config['Architecture']\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "lRV8wlklTnks"
      },
      "outputs": [
        {
          "ename": "UnboundLocalError",
          "evalue": "local variable 'checkpoint_time' referenced before assignment",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21484\\690544027.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mrun_prediction\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m     \u001b[0mrun_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21484\\690544027.py\u001b[0m in \u001b[0;36mrun_prediction\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mdf_test_combined\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheckpoint_time\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mcheckpoint_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%m-%d_%H-%M\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Checkpoint time: {checkpoint_time}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'checkpoint_time' referenced before assignment"
          ]
        }
      ],
      "source": [
        "num_windows = 99\n",
        "\n",
        "config = {\n",
        "    'LSTM_units': 8,\n",
        "    'Interface_units': 4,\n",
        "    'MLP_units': 200,\n",
        "    'LSTM_timesteps': 140,\n",
        "    'LSTM_layers': 3,\n",
        "    'MLP_layers': 4,\n",
        "    'Bn_momentum': 0.4,\n",
        "    'Lr': 0.0564,\n",
        "    'Lr_decay': 0.809,\n",
        "    'Leaky_relu_alpha': 0.1,\n",
        "    'Minibatch_size': 4096,\n",
        "    'Min_delta': 0.01 if moneyness else 1,\n",
        "    'Patience': 20,\n",
        "    'Num_features': 3 if moneyness else 4, \n",
        "    'Architecture': 'LSTM-MLP v.E.2',\n",
        "}\n",
        "\n",
        "df_test_combined = pd.DataFrame()\n",
        "\n",
        "if not checkpoint_time:\n",
        "    checkpoint_time = datetime.now().strftime(\"%m-%d_%H-%M\")\n",
        "print(f'Checkpoint time: {checkpoint_time}')\n",
        "\n",
        "previous_checkpoint_path = None\n",
        "\n",
        "for window in range(starting_window, num_windows):\n",
        "    np.random.seed(random_seed)\n",
        "    tf.random.set_seed(random_seed)\n",
        "\n",
        "    if moneyness:\n",
        "        train_x, train_y, val_x, val_y, test_x, test_y, train_start, val_start, test_start, df_test, train_strike, val_strike, test_strike = create_rw_dataset(df=df, window_number=window)\n",
        "    else:\n",
        "        train_x, train_y, val_x, val_y, test_x, test_y, train_start, val_start, test_start, df_test = create_rw_dataset(df=df, window_number=window)\n",
        "\n",
        "    print('-------------------------------------------')\n",
        "    print(f'Window {window + 1} of {num_windows}')\n",
        "    print(\"Training start: \", train_start, \"Validation start: \", val_start, \"Test start: \", test_start)\n",
        "    print(\"Loading previous checkpoint: \", previous_loading)\n",
        "        \n",
        "    if google_colab:\n",
        "        checkpoint_path = f'/content/drive/MyDrive/01. Masters Thesis - Shared/05. Checkpoints/{checkpoint_time}/{train_start}/'\n",
        "    elif finance_computers:\n",
        "        checkpoint_path = f'{fc_path}/checkpoints/{checkpoint_time}/{train_start}/'\n",
        "    else:\n",
        "        checkpoint_path = f'./checkpoints/{checkpoint_time}/{train_start}/'\n",
        "\n",
        "    config['Dataset'] = f'{train_start} - {val_start} - {test_start}'\n",
        "\n",
        "    if run_training:\n",
        "        trainer(train_x = train_x, train_y = train_y, val_x = val_x, val_y = val_y, config = config, project = 'Deep learning for option pricing - rolling windows',  previous_checkpoint_path = previous_checkpoint_path, checkpoint_path = checkpoint_path)\n",
        "    c_model = load_model(checkpoint_path + \".h5\")\n",
        "    \n",
        "    # Split test_x into five parts\n",
        "    part_size = len(test_x[0]) // 5\n",
        "    test_x_parts = []\n",
        "    for i in range(5):\n",
        "        subset_1 = test_x[0][i * part_size:(i + 1) * part_size]\n",
        "        subset_2 = test_x[1][i * part_size:(i + 1) * part_size]\n",
        "        test_x_parts.append((subset_1, subset_2))\n",
        "\n",
        "    # Adjust the last part to include any remaining data\n",
        "    test_x_parts[-1] = (test_x[0][(len(test_x_parts) - 1) * part_size:], test_x[1][(len(test_x_parts) - 1) * part_size:])\n",
        "\n",
        "    # Make predictions for each part\n",
        "    predictions_parts = [np.array(c_model(part)) for part in test_x_parts]\n",
        "\n",
        "    # Combine the predictions\n",
        "    predictions = np.concatenate(predictions_parts, axis=0)\n",
        "\n",
        "    print(f'--- Predictions for test_start {test_start} ---')\n",
        "    calculate_error(predictions, test_y)\n",
        "    print('-------------------------------------------')\n",
        "    \n",
        "    df_test[\"Prediction\"] = predictions\n",
        "    df_test_combined = pd.concat([df_test_combined, df_test[[\"Quote_date\", \"Price\", \"Prediction\"] + bs_vars]])\n",
        "\n",
        "    if previous_loading:\n",
        "       previous_checkpoint_path = checkpoint_path\n",
        "    \n",
        "    del train_x\n",
        "    del train_y\n",
        "    del val_x\n",
        "    del val_y\n",
        "    del test_x\n",
        "    del test_y\n",
        "    del train_start\n",
        "    del val_start\n",
        "    del test_start\n",
        "    del df_test\n",
        "\n",
        "print(f\"--- All model predictions ---\")\n",
        "calculate_error(df_test_combined[\"Prediction\"], df_test_combined[\"Price\"])\n",
        "print(\"-------------------------------------------\")\n",
        "\n",
        "if google_colab == False:\n",
        "    predictions_path = './predictions/'\n",
        "    if checkpoint_path and not os.path.exists(predictions_path):\n",
        "        os.makedirs(predictions_path)\n",
        "    df_test_combined.to_csv(f'{predictions_path}{datetime.now().strftime(\"%m-%d_%H-%M\")}.csv')\n",
        "elif finance_computers:\n",
        "    predictions_path = f'{fc_path}/predictions/'\n",
        "    if checkpoint_path and not os.path.exists(predictions_path):\n",
        "        os.makedirs(predictions_path)\n",
        "    df_test_combined.to_csv(f'{predictions_path}{datetime.now().strftime(\"%m-%d_%H-%M\")}.csv')\n",
        "else:\n",
        "  path = f'/content/drive/MyDrive/01. Masters Thesis - Shared/06. Predictions/{checkpoint_time}.csv'\n",
        "  df_test_combined.to_csv(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a function to calculate RMSE\n",
        "def calculate_rmse(group):\n",
        "    rmse_model_1 = np.sqrt(mean_squared_error(group[\"Price\"], group[\"Prediction\"]))\n",
        "    return pd.Series({\"RMSE_Model_1\": rmse_model_1})\n",
        "\n",
        "# Group data by Quote_date and calculate RMSE for each group\n",
        "rmse_df = df_test_combined.groupby(\"Quote_date\").apply(calculate_rmse).reset_index()\n",
        "\n",
        "# Plot the RMSE values for each model\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "sns.lineplot(data=rmse_df, x=\"Quote_date\", y=\"RMSE_Model_1\", label=\"Model\", ax=ax)\n",
        "\n",
        "# Set the interval for x-axis labels\n",
        "ax.xaxis.set_major_locator(ticker.MultipleLocator(base=10))\n",
        "\n",
        "plt.xlabel(\"Quote_date\")\n",
        "plt.ylabel(\"RMSE\")\n",
        "plt.title(\"RMSE for Models\")\n",
        "plt.legend()\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Shap values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "c_model = None\n",
        "\n",
        "def shapley_predict(x_2d):\n",
        "    # Reshape data to fit LSTM\n",
        "    x_3d = [x_2d[:, :lags].reshape(len(x_2d[:, :lags]), max_timesteps, 1), x_2d[:, lags:]]\n",
        "    return np.array(c_model(x_3d))\n",
        "\n",
        "def create_shapley_values(train_x_2d, test_x_2d):\n",
        "    explainer = shap.KernelExplainer(shapley_predict, train_x_2d)\n",
        "    shap_values = explainer.shap_values(test_x_2d)\n",
        "    return explainer, shap_values\n",
        "\n",
        "def shap_summary_all(shap_values, test_x_2d):\n",
        "    shap.initjs()\n",
        "    shap.summary_plot(shap_values[0], test_x_2d, feature_names = underlying_lags + bs_vars, max_display = 150)\n",
        "\n",
        "def shap_summary_returns(shap_values, test_x_2d):\n",
        "    shap.initjs()\n",
        "    shap.summary_plot(shap_values[0][:, :lags], test_x_2d[:, :lags], feature_names = underlying_lags, max_display = 150)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "weight_decay is not a valid argument, kwargs should be empty  for `optimizer_experimental.Optimizer`.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12644\\2148109408.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mcheckpoint_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf'../checkpoints/{checkpoint_time}/{train_start}/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mc_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mtrain_x_2d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Erlend\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Erlend\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\optimizer_experimental\\optimizer.py\u001b[0m in \u001b[0;36m_process_kwargs\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m             \", please check the docstring for valid arguments.\", k)\n\u001b[0;32m     85\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m         raise TypeError(f\"{k} is not a valid argument, kwargs should be empty \"\n\u001b[0m\u001b[0;32m     87\u001b[0m                         \" for `optimizer_experimental.Optimizer`.\")\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: weight_decay is not a valid argument, kwargs should be empty  for `optimizer_experimental.Optimizer`."
          ]
        }
      ],
      "source": [
        "kmeans = 75\n",
        "sample_size = 5000\n",
        "windows = 99\n",
        "start_window = 99\n",
        "window_interval = 1\n",
        "\n",
        "df_shap_combined = pd.DataFrame()\n",
        "\n",
        "model = None\n",
        "\n",
        "timestamp = datetime.now()\n",
        "timestamp = timestamp.strftime(\"%m-%d_%H-%M\")\n",
        "\n",
        "for window in range(start_window-1, windows, window_interval):\n",
        "    # Load data\n",
        "    train_x, test_x, train_x_org, train_start, val_start, test_start = create_rw_dataset(df=df, window_number=window, shap = True)\n",
        "\n",
        "    if google_colab:\n",
        "        checkpoint_path = f'/content/drive/MyDrive/01. Masters Thesis - Shared/05. Checkpoints/{checkpoint_time}/{train_start}/'\n",
        "    elif finance_computers:\n",
        "        checkpoint_path = f'{fc_path}/checkpoints/{checkpoint_time}/{train_start}/'\n",
        "    else:\n",
        "        checkpoint_path = f'../checkpoints/{checkpoint_time}/{train_start}/'\n",
        "\n",
        "    c_model = load_model(checkpoint_path + \".h5\")\n",
        "\n",
        "    train_x_2d_input = np.hstack((train_x[0], train_x[1]))\n",
        "    test_x_2d_input = np.hstack((test_x[0], test_x[1]))\n",
        "    train_x_2d_org_input = np.hstack((train_x_org[0], train_x_org[1]))\n",
        "\n",
        "    train_x_2d = shap.kmeans(train_x_2d_input, kmeans)\n",
        "    test_x_2d = shap.sample(train_x_2d_input, sample_size)\n",
        "    test_x_2d = train_x_2d_input[:sample_size]\n",
        "    test_x_2d_org = train_x_2d_org_input[:sample_size]\n",
        "\n",
        "    explainer, shap_values = create_shapley_values(train_x_2d, test_x_2d)\n",
        "\n",
        "    shap_summary_all(shap_values, test_x_2d)\n",
        "    shap_summary_returns(shap_values, test_x_2d)\n",
        "\n",
        "    df_shap = pd.DataFrame(shap_values[0], columns = underlying_lags + bs_vars)\n",
        "    df_shap[underlying_lags + bs_vars] = test_x_2d\n",
        "\n",
        "    if google_colab:\n",
        "        save_path = f'/content/drive/MyDrive/01. Masters Thesis - Shared/06. Predictions/Shap/{timestamp}_Shap_{train_start}.csv'\n",
        "    else:\n",
        "        save_path = f'../results/Shap/{timestamp}_Shap_{train_start}.csv'\n",
        "    df_shap.to_csv(save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
