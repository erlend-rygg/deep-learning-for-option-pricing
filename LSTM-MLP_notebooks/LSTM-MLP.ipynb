{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights and Biases\n",
    "#!pip install -q wandb\n",
    "# Tensorflow\n",
    "#!pip install -q tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, LSTM, Concatenate, Dense, BatchNormalization, LeakyReLU\n",
    "from keras.activations import tanh\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33merlendrygg\u001b[0m (\u001b[33mavogadro\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Erlend/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import setup\n",
    "#Erlend\n",
    "wandb.login(key=setup.erlend_key)\n",
    "# Hjalmar\n",
    "#wandb.login(key=setup.hjalmar_key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, split and normalize data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Quote_date</th>\n",
       "      <th>Expire_date</th>\n",
       "      <th>Price</th>\n",
       "      <th>Underlying_last</th>\n",
       "      <th>Strike</th>\n",
       "      <th>TTM</th>\n",
       "      <th>Underlying_1</th>\n",
       "      <th>Underlying_2</th>\n",
       "      <th>Underlying_3</th>\n",
       "      <th>...</th>\n",
       "      <th>Underlying_12</th>\n",
       "      <th>Underlying_13</th>\n",
       "      <th>Underlying_14</th>\n",
       "      <th>Underlying_15</th>\n",
       "      <th>Underlying_16</th>\n",
       "      <th>Underlying_17</th>\n",
       "      <th>Underlying_18</th>\n",
       "      <th>Underlying_19</th>\n",
       "      <th>Underlying_20</th>\n",
       "      <th>R</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>173719</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>3312.50</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>173720</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>3114.00</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>173721</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>2913.55</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>173722</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>2712.60</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>173723</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>2512.60</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>173724</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>2312.60</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>173725</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>2112.60</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>173726</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>1912.60</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>2600.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>173727</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>1712.60</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>2800.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>173728</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>1612.60</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>2900.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>173729</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>1513.25</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>173730</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>1412.60</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>3100.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>173731</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>1313.35</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>173732</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>1213.35</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>3300.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>173733</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>1113.40</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>3400.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>173734</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>1013.40</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>173735</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>963.40</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>3550.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>173736</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>913.40</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>173737</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>863.45</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>3650.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>173738</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>838.45</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>3675.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>173739</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>813.70</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>3700.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>173740</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>788.70</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>3725.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>173741</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>763.70</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>173742</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>738.75</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>3775.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>173743</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>713.75</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>173744</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>688.75</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>3825.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>173745</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>663.80</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>3850.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>173746</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>638.65</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>3875.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>173747</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>613.70</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>3900.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>173748</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>588.65</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>3925.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  Quote_date Expire_date    Price  Underlying_last  Strike  TTM  \\\n",
       "0       173719  2022-01-31  2022-02-02  3312.50          4516.89  1200.0    2   \n",
       "1       173720  2022-01-31  2022-02-02  3114.00          4516.89  1400.0    2   \n",
       "2       173721  2022-01-31  2022-02-02  2913.55          4516.89  1600.0    2   \n",
       "3       173722  2022-01-31  2022-02-02  2712.60          4516.89  1800.0    2   \n",
       "4       173723  2022-01-31  2022-02-02  2512.60          4516.89  2000.0    2   \n",
       "5       173724  2022-01-31  2022-02-02  2312.60          4516.89  2200.0    2   \n",
       "6       173725  2022-01-31  2022-02-02  2112.60          4516.89  2400.0    2   \n",
       "7       173726  2022-01-31  2022-02-02  1912.60          4516.89  2600.0    2   \n",
       "8       173727  2022-01-31  2022-02-02  1712.60          4516.89  2800.0    2   \n",
       "9       173728  2022-01-31  2022-02-02  1612.60          4516.89  2900.0    2   \n",
       "10      173729  2022-01-31  2022-02-02  1513.25          4516.89  3000.0    2   \n",
       "11      173730  2022-01-31  2022-02-02  1412.60          4516.89  3100.0    2   \n",
       "12      173731  2022-01-31  2022-02-02  1313.35          4516.89  3200.0    2   \n",
       "13      173732  2022-01-31  2022-02-02  1213.35          4516.89  3300.0    2   \n",
       "14      173733  2022-01-31  2022-02-02  1113.40          4516.89  3400.0    2   \n",
       "15      173734  2022-01-31  2022-02-02  1013.40          4516.89  3500.0    2   \n",
       "16      173735  2022-01-31  2022-02-02   963.40          4516.89  3550.0    2   \n",
       "17      173736  2022-01-31  2022-02-02   913.40          4516.89  3600.0    2   \n",
       "18      173737  2022-01-31  2022-02-02   863.45          4516.89  3650.0    2   \n",
       "19      173738  2022-01-31  2022-02-02   838.45          4516.89  3675.0    2   \n",
       "20      173739  2022-01-31  2022-02-02   813.70          4516.89  3700.0    2   \n",
       "21      173740  2022-01-31  2022-02-02   788.70          4516.89  3725.0    2   \n",
       "22      173741  2022-01-31  2022-02-02   763.70          4516.89  3750.0    2   \n",
       "23      173742  2022-01-31  2022-02-02   738.75          4516.89  3775.0    2   \n",
       "24      173743  2022-01-31  2022-02-02   713.75          4516.89  3800.0    2   \n",
       "25      173744  2022-01-31  2022-02-02   688.75          4516.89  3825.0    2   \n",
       "26      173745  2022-01-31  2022-02-02   663.80          4516.89  3850.0    2   \n",
       "27      173746  2022-01-31  2022-02-02   638.65          4516.89  3875.0    2   \n",
       "28      173747  2022-01-31  2022-02-02   613.70          4516.89  3900.0    2   \n",
       "29      173748  2022-01-31  2022-02-02   588.65          4516.89  3925.0    2   \n",
       "\n",
       "    Underlying_1  Underlying_2  Underlying_3  ...  Underlying_12  \\\n",
       "0         4431.8       4325.89       4347.26  ...        4660.64   \n",
       "1         4431.8       4325.89       4347.26  ...        4660.64   \n",
       "2         4431.8       4325.89       4347.26  ...        4660.64   \n",
       "3         4431.8       4325.89       4347.26  ...        4660.64   \n",
       "4         4431.8       4325.89       4347.26  ...        4660.64   \n",
       "5         4431.8       4325.89       4347.26  ...        4660.64   \n",
       "6         4431.8       4325.89       4347.26  ...        4660.64   \n",
       "7         4431.8       4325.89       4347.26  ...        4660.64   \n",
       "8         4431.8       4325.89       4347.26  ...        4660.64   \n",
       "9         4431.8       4325.89       4347.26  ...        4660.64   \n",
       "10        4431.8       4325.89       4347.26  ...        4660.64   \n",
       "11        4431.8       4325.89       4347.26  ...        4660.64   \n",
       "12        4431.8       4325.89       4347.26  ...        4660.64   \n",
       "13        4431.8       4325.89       4347.26  ...        4660.64   \n",
       "14        4431.8       4325.89       4347.26  ...        4660.64   \n",
       "15        4431.8       4325.89       4347.26  ...        4660.64   \n",
       "16        4431.8       4325.89       4347.26  ...        4660.64   \n",
       "17        4431.8       4325.89       4347.26  ...        4660.64   \n",
       "18        4431.8       4325.89       4347.26  ...        4660.64   \n",
       "19        4431.8       4325.89       4347.26  ...        4660.64   \n",
       "20        4431.8       4325.89       4347.26  ...        4660.64   \n",
       "21        4431.8       4325.89       4347.26  ...        4660.64   \n",
       "22        4431.8       4325.89       4347.26  ...        4660.64   \n",
       "23        4431.8       4325.89       4347.26  ...        4660.64   \n",
       "24        4431.8       4325.89       4347.26  ...        4660.64   \n",
       "25        4431.8       4325.89       4347.26  ...        4660.64   \n",
       "26        4431.8       4325.89       4347.26  ...        4660.64   \n",
       "27        4431.8       4325.89       4347.26  ...        4660.64   \n",
       "28        4431.8       4325.89       4347.26  ...        4660.64   \n",
       "29        4431.8       4325.89       4347.26  ...        4660.64   \n",
       "\n",
       "    Underlying_13  Underlying_14  Underlying_15  Underlying_16  Underlying_17  \\\n",
       "0         4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "1         4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "2         4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "3         4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "4         4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "5         4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "6         4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "7         4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "8         4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "9         4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "10        4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "11        4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "12        4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "13        4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "14        4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "15        4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "16        4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "17        4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "18        4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "19        4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "20        4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "21        4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "22        4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "23        4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "24        4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "25        4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "26        4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "27        4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "28        4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "29        4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "\n",
       "    Underlying_18  Underlying_19  Underlying_20     R  \n",
       "0         4700.64        4793.19        4795.57  0.03  \n",
       "1         4700.64        4793.19        4795.57  0.03  \n",
       "2         4700.64        4793.19        4795.57  0.03  \n",
       "3         4700.64        4793.19        4795.57  0.03  \n",
       "4         4700.64        4793.19        4795.57  0.03  \n",
       "5         4700.64        4793.19        4795.57  0.03  \n",
       "6         4700.64        4793.19        4795.57  0.03  \n",
       "7         4700.64        4793.19        4795.57  0.03  \n",
       "8         4700.64        4793.19        4795.57  0.03  \n",
       "9         4700.64        4793.19        4795.57  0.03  \n",
       "10        4700.64        4793.19        4795.57  0.03  \n",
       "11        4700.64        4793.19        4795.57  0.03  \n",
       "12        4700.64        4793.19        4795.57  0.03  \n",
       "13        4700.64        4793.19        4795.57  0.03  \n",
       "14        4700.64        4793.19        4795.57  0.03  \n",
       "15        4700.64        4793.19        4795.57  0.03  \n",
       "16        4700.64        4793.19        4795.57  0.03  \n",
       "17        4700.64        4793.19        4795.57  0.03  \n",
       "18        4700.64        4793.19        4795.57  0.03  \n",
       "19        4700.64        4793.19        4795.57  0.03  \n",
       "20        4700.64        4793.19        4795.57  0.03  \n",
       "21        4700.64        4793.19        4795.57  0.03  \n",
       "22        4700.64        4793.19        4795.57  0.03  \n",
       "23        4700.64        4793.19        4795.57  0.03  \n",
       "24        4700.64        4793.19        4795.57  0.03  \n",
       "25        4700.64        4793.19        4795.57  0.03  \n",
       "26        4700.64        4793.19        4795.57  0.03  \n",
       "27        4700.64        4793.19        4795.57  0.03  \n",
       "28        4700.64        4793.19        4795.57  0.03  \n",
       "29        4700.64        4793.19        4795.57  0.03  \n",
       "\n",
       "[30 rows x 28 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file = \"../data/processed_data/2022.csv\"\n",
    "df_read = pd.read_csv(file)\n",
    "display(df_read.head(30))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1452255, 20, 1), (1452255, 4)\n",
      "Val shape: (311065, 20, 1), (311065, 4)\n"
     ]
    }
   ],
   "source": [
    "df = df_read\n",
    "\n",
    "# Formating settings\n",
    "val_start = \"2022-11-01\"\n",
    "num_features = 4\n",
    "time_steps = 20\n",
    "bs_vars = ['Underlying_last', 'Strike', 'TTM', 'R']\n",
    "underlying_lags = ['Underlying_last'] + [f'Underlying_{i}' for i in range (1, time_steps)]\n",
    "\n",
    "# Split train and validation data\n",
    "df_train = df[df['Quote_date'] < val_start]\n",
    "df_val = df[df['Quote_date'] >= val_start]\n",
    "\n",
    "# Extract target values\n",
    "train_y = df_train['Price'].to_numpy()\n",
    "val_y = df_val['Price'].to_numpy()\n",
    "\n",
    "# Convert dataframes to numpy arrays\n",
    "train_x = [df_train[underlying_lags].to_numpy(), df_train[bs_vars].to_numpy()]\n",
    "val_x = [df_val[underlying_lags].to_numpy(), df_val[bs_vars].to_numpy()]\n",
    "\n",
    "# Scale features based on training set\n",
    "underlying_scaler = MinMaxScaler()\n",
    "train_x[0] = underlying_scaler.fit_transform(train_x[0])\n",
    "val_x[0] = underlying_scaler.transform(val_x[0])\n",
    "\n",
    "bs_scaler = MinMaxScaler()\n",
    "train_x[1] = bs_scaler.fit_transform(train_x[1])\n",
    "val_x[1] = bs_scaler.transform(val_x[1])\n",
    "\n",
    "# Shuffle training set\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(len(train_x[0]))\n",
    "train_x = [train_x[0][shuffle], train_x[1][shuffle]]\n",
    "train_y = train_y[shuffle]\n",
    "\n",
    "# Reshape data to fit LSTM\n",
    "train_x = [train_x[0].reshape(len(train_x[0]),time_steps,1), train_x[1]]\n",
    "val_x = [val_x[0].reshape(len(val_x[0]), time_steps, 1), val_x[1]]\n",
    "\n",
    "print(f'Train shape: {train_x[0].shape}, {train_x[1].shape}')\n",
    "print(f'Val shape: {val_x[0].shape}, {val_x[1].shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(config):\n",
    "    '''Builds an LSTM-MLP model of minimum 2 layers sequentially from a given config dictionary'''\n",
    "\n",
    "    # Input layers\n",
    "    underlying_history = Input((config.LSTM_timesteps,1))\n",
    "    bs_vars = Input((config.Num_features,))\n",
    "\n",
    "    # LSTM layers\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(\n",
    "        units = config.LSTM_units,\n",
    "        activation = tanh,\n",
    "        input_shape = (config.LSTM_timesteps, 1),\n",
    "        return_sequences = True\n",
    "    ))\n",
    "\n",
    "    for _ in range(config.LSTM_layers - 2):\n",
    "        model.add(LSTM(\n",
    "            units = config.LSTM_units,\n",
    "            activation = tanh,\n",
    "            return_sequences = True\n",
    "        ))\n",
    "    \n",
    "    model.add(LSTM(\n",
    "        units = config.LSTM_units,\n",
    "        activation = tanh,\n",
    "        return_sequences = False\n",
    "    ))\n",
    "\n",
    "    # MLP layers\n",
    "    layers = Concatenate()([model(underlying_history), bs_vars])\n",
    "    \n",
    "    for _ in range(config.MLP_layers - 1):\n",
    "        layers = Dense(config.MLP_units)(layers)\n",
    "        layers = BatchNormalization(momentum=config.Bn_momentum)(layers)\n",
    "        layers = LeakyReLU()(layers)\n",
    "\n",
    "    output = Dense(1, activation='relu')(layers)\n",
    "\n",
    "    # Exponential decaying learning rate\n",
    "    lr_schedule = ExponentialDecay(\n",
    "        initial_learning_rate = config.Lr,\n",
    "        decay_steps = int(len(train_x[0])/config.Minibatch_size),\n",
    "        decay_rate=config.Lr_decay\n",
    "    )\n",
    "\n",
    "    # Compile model\n",
    "    model = Model(inputs=[underlying_history, bs_vars], outputs=output)\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=lr_schedule))\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter search setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: bu76nc4s\n",
      "Sweep URL: https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/sweeps/bu76nc4s\n"
     ]
    }
   ],
   "source": [
    "# Configuring the sweep hyperparameter search space\n",
    "sweep_configuration = {\n",
    "    'method': 'random',\n",
    "    'name': 'LSTM-MLP v.1.0 testing',\n",
    "    'metric': {\n",
    "        'goal': 'minimize', \n",
    "        'name': 'val_loss'\n",
    "\t\t},\n",
    "    'parameters': {\n",
    "        'LSTM_units': {\n",
    "            'values': [8, 16, 32, 64, 96, 128]},\n",
    "        'MLP_units': {\n",
    "            'values': [32, 64, 96, 128]},\n",
    "        'LSTM_timesteps': {\n",
    "            'values': [10, 20]},\n",
    "        'LSTM_layers': {\n",
    "            'distribution': 'int_uniform',\n",
    "            'max': 6, 'min': 2},\n",
    "        'MLP_layers': {\n",
    "            'distribution': 'int_uniform',\n",
    "            'max': 6, 'min': 2},\n",
    "        'Bn_momentum': {\n",
    "            'distribution': 'uniform',\n",
    "            'max': 1, 'min': 0},\n",
    "        'Lr': {\n",
    "            'distribution': 'uniform',\n",
    "            'max': 0.01, 'min': 0.0001},\n",
    "        'Lr_decay': {\n",
    "            'distribution': 'uniform',\n",
    "            'max': 1, 'min': 0.9},        \n",
    "        'Minibatch_size': {\n",
    "            'values': [1024, 2048, 4096]},\n",
    "        'Min_delta': {\n",
    "            'value': 1},\n",
    "        'Patience': {\n",
    "            'value': 20},\n",
    "        'Num_features': {\n",
    "            'value': 4},\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize sweep and creating sweepID\n",
    "\n",
    "# If new sweep, uncomment the line below and comment the line after it\n",
    "#sweep_id = wandb.sweep(sweep=sweep_configuration, project='Deep learning for option pricing - test area') \n",
    "#sweep_id = '98bxt6oq'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(train_x = train_x, train_y = train_y, val_x = val_x, val_y = val_y, config = None, project = None):\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config, project = project):\n",
    "\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "\n",
    "        # Build model and create callbacks\n",
    "        model = create_model(config)\n",
    "\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            min_delta = config.Min_delta,\n",
    "            patience = config.Patience,\n",
    "        )\n",
    "        \n",
    "        wandb_callback = WandbCallback(\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            save_model=False\n",
    "        )\n",
    "\n",
    "        # Adapt sequence length to config\n",
    "        train_x[0] = train_x[0][:, :config.LSTM_timesteps, :]\n",
    "        val_x[0] = val_x[0][:, :config.LSTM_timesteps, :]\n",
    "        print(f'Train shape: {train_x[0].shape}, {train_x[1].shape}')\n",
    "        print(f'Val shape: {val_x[0].shape}, {val_x[1].shape}')\n",
    "\n",
    "        # Train model\n",
    "        model.fit(\n",
    "            train_x,\n",
    "            train_y,\n",
    "            batch_size = config.Minibatch_size,\n",
    "            validation_data = (val_x, val_y),\n",
    "            epochs = 1000,\n",
    "            callbacks = [early_stopping, wandb_callback] \n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run full sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: h2mmkvf0 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBn_momentum: 0.8268163490936014\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_layers: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_timesteps: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_units: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLr: 0.002322679981694606\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLr_decay: 0.9381204396211013\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tMLP_layers: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tMLP_units: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tMin_delta: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tMinibatch_size: 2048\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tNum_features: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tPatience: 20\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/hjalmarjacobvinje/Documents/deep-learning-for-option-pricing/LSTM-MLP_notebooks/wandb/run-20230204_224347-h2mmkvf0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/h2mmkvf0\" target=\"_blank\">misty-sweep-1</a></strong> to <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page: <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/sweeps/bu76nc4s\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/sweeps/bu76nc4s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/sweeps/bu76nc4s\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/sweeps/bu76nc4s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/h2mmkvf0\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/h2mmkvf0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-04 22:43:48.609942: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 10, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 8)            3040        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 12)           0           ['sequential[0][0]',             \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 32)           416         ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32)          128         ['dense[0][0]']                  \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 32)           0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 32)           1056        ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32)          128         ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 32)           0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 32)           1056        ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32)          128         ['dense_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 32)           0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 32)           1056        ['leaky_re_lu_2[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 32)          128         ['dense_3[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)      (None, 32)           0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 32)           1056        ['leaky_re_lu_3[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 32)          128         ['dense_4[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)      (None, 32)           0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 1)            33          ['leaky_re_lu_4[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8,353\n",
      "Trainable params: 8,033\n",
      "Non-trainable params: 320\n",
      "__________________________________________________________________________________________________\n",
      "Train shape: (1452255, 10, 1), (1452255, 4)\n",
      "Val shape: (311065, 10, 1), (311065, 4)\n",
      "Epoch 1/1000\n",
      " 57/710 [=>............................] - ETA: 1:25 - loss: 726653.8125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id=sweep_id, function=trainer, project='Deep learning for option pricing - test area', count = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.13.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Erlend\\Google Drive\\NTNU\\5. Klasse\\Master\\Git-folder\\deep-learning-for-option-pricing\\LSTM-MLP_notebooks\\wandb\\run-20230207_123606-30brlfcr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/30brlfcr\" target=\"_blank\">woven-lion-80</a></strong> to <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 20, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 8)            1408        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 12)           0           ['sequential[0][0]',             \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 100)          1300        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 100)         400         ['dense[0][0]']                  \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 100)          0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 100)          10100       ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 100)         400         ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 100)          0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 100)          10100       ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 100)         400         ['dense_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 100)          0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            101         ['leaky_re_lu_2[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 24,209\n",
      "Trainable params: 23,609\n",
      "Non-trainable params: 600\n",
      "__________________________________________________________________________________________________\n",
      "Train shape: (1452255, 20, 1), (1452255, 4)\n",
      "Val shape: (311065, 20, 1), (311065, 4)\n",
      "Epoch 1/1000\n",
      "355/355 [==============================] - 82s 213ms/step - loss: 227251.4375 - val_loss: 194344.3438\n",
      "Epoch 2/1000\n",
      "355/355 [==============================] - 65s 183ms/step - loss: 474.4754 - val_loss: 8041.1001\n",
      "Epoch 3/1000\n",
      "355/355 [==============================] - 63s 177ms/step - loss: 472.3555 - val_loss: 28381.0332\n",
      "Epoch 4/1000\n",
      "355/355 [==============================] - 64s 182ms/step - loss: 493.7442 - val_loss: 118136.7031\n",
      "Epoch 5/1000\n",
      "355/355 [==============================] - 57s 161ms/step - loss: 496.0572 - val_loss: 35079.3477\n",
      "Epoch 6/1000\n",
      "355/355 [==============================] - 56s 158ms/step - loss: 428.2709 - val_loss: 8320.0557\n",
      "Epoch 7/1000\n",
      "355/355 [==============================] - 57s 160ms/step - loss: 393.6819 - val_loss: 24273.2500\n",
      "Epoch 8/1000\n",
      "355/355 [==============================] - 64s 179ms/step - loss: 447.3439 - val_loss: 26465.7852\n",
      "Epoch 9/1000\n",
      "355/355 [==============================] - 80s 226ms/step - loss: 477.1388 - val_loss: 892.9767\n",
      "Epoch 10/1000\n",
      "355/355 [==============================] - 74s 208ms/step - loss: 443.5166 - val_loss: 20383.4570\n",
      "Epoch 11/1000\n",
      "355/355 [==============================] - 75s 211ms/step - loss: 476.6283 - val_loss: 3994.0474\n",
      "Epoch 12/1000\n",
      "355/355 [==============================] - 80s 225ms/step - loss: 447.2078 - val_loss: 2879.4412\n",
      "Epoch 13/1000\n",
      "355/355 [==============================] - 79s 223ms/step - loss: 488.3635 - val_loss: 4333.6914\n",
      "Epoch 14/1000\n",
      "355/355 [==============================] - 77s 217ms/step - loss: 430.5404 - val_loss: 29619.6465\n",
      "Epoch 15/1000\n",
      "355/355 [==============================] - 78s 219ms/step - loss: 530.3374 - val_loss: 10875.7090\n",
      "Epoch 16/1000\n",
      "355/355 [==============================] - 79s 222ms/step - loss: 486.3429 - val_loss: 15483.7510\n",
      "Epoch 17/1000\n",
      "355/355 [==============================] - 75s 211ms/step - loss: 503.0679 - val_loss: 322795.0625\n",
      "Epoch 18/1000\n",
      "355/355 [==============================] - 76s 215ms/step - loss: 498.5128 - val_loss: 4782.7451\n",
      "Epoch 19/1000\n",
      "355/355 [==============================] - 85s 241ms/step - loss: 468.4017 - val_loss: 711.2976\n",
      "Epoch 20/1000\n",
      "355/355 [==============================] - 85s 239ms/step - loss: 502.0860 - val_loss: 4848.7100\n",
      "Epoch 21/1000\n",
      "355/355 [==============================] - 83s 233ms/step - loss: 508.9420 - val_loss: 1462.7319\n",
      "Epoch 22/1000\n",
      "355/355 [==============================] - 138s 388ms/step - loss: 451.0058 - val_loss: 29380.2402\n",
      "Epoch 23/1000\n",
      "355/355 [==============================] - 86s 242ms/step - loss: 517.1207 - val_loss: 10699.0078\n",
      "Epoch 24/1000\n",
      "355/355 [==============================] - 86s 243ms/step - loss: 461.2269 - val_loss: 1517.8308\n",
      "Epoch 25/1000\n",
      "355/355 [==============================] - 87s 244ms/step - loss: 504.4256 - val_loss: 10513.8828\n",
      "Epoch 26/1000\n",
      "355/355 [==============================] - 87s 244ms/step - loss: 471.1577 - val_loss: 5945.6787\n",
      "Epoch 27/1000\n",
      "355/355 [==============================] - 82s 232ms/step - loss: 480.0703 - val_loss: 9894.0664\n",
      "Epoch 28/1000\n",
      "355/355 [==============================] - 88s 247ms/step - loss: 477.5972 - val_loss: 2048.1921\n",
      "Epoch 29/1000\n",
      "355/355 [==============================] - 87s 244ms/step - loss: 424.0838 - val_loss: 735.6666\n",
      "Epoch 30/1000\n",
      "355/355 [==============================] - 86s 243ms/step - loss: 462.2721 - val_loss: 1180.0521\n",
      "Epoch 31/1000\n",
      "355/355 [==============================] - 86s 243ms/step - loss: 451.7561 - val_loss: 498.6244\n",
      "Epoch 32/1000\n",
      "355/355 [==============================] - 82s 230ms/step - loss: 501.2716 - val_loss: 7325.4185\n",
      "Epoch 33/1000\n",
      "355/355 [==============================] - 80s 227ms/step - loss: 433.8280 - val_loss: 1553.6085\n",
      "Epoch 34/1000\n",
      "355/355 [==============================] - 87s 245ms/step - loss: 478.0516 - val_loss: 4188.3994\n",
      "Epoch 35/1000\n",
      "355/355 [==============================] - 88s 249ms/step - loss: 439.7157 - val_loss: 1014.1069\n",
      "Epoch 36/1000\n",
      "355/355 [==============================] - 88s 247ms/step - loss: 421.3161 - val_loss: 8202.8496\n",
      "Epoch 37/1000\n",
      "355/355 [==============================] - 89s 250ms/step - loss: 514.5905 - val_loss: 14672.0059\n",
      "Epoch 38/1000\n",
      "355/355 [==============================] - 88s 248ms/step - loss: 462.6916 - val_loss: 1117.5245\n",
      "Epoch 39/1000\n",
      "355/355 [==============================] - 89s 250ms/step - loss: 507.5510 - val_loss: 5179.5312\n",
      "Epoch 40/1000\n",
      "355/355 [==============================] - 88s 248ms/step - loss: 473.2314 - val_loss: 10093.3779\n",
      "Epoch 41/1000\n",
      "355/355 [==============================] - 89s 250ms/step - loss: 444.4245 - val_loss: 1840.4677\n",
      "Epoch 42/1000\n",
      "355/355 [==============================] - 88s 247ms/step - loss: 448.1315 - val_loss: 4814.8911\n",
      "Epoch 43/1000\n",
      "355/355 [==============================] - 89s 250ms/step - loss: 433.8468 - val_loss: 2949.1570\n",
      "Epoch 44/1000\n",
      "355/355 [==============================] - 90s 252ms/step - loss: 489.1543 - val_loss: 15036.3730\n",
      "Epoch 45/1000\n",
      "355/355 [==============================] - 89s 252ms/step - loss: 521.8607 - val_loss: 400.2130\n",
      "Epoch 46/1000\n",
      "355/355 [==============================] - 89s 252ms/step - loss: 521.3358 - val_loss: 1224.6058\n",
      "Epoch 47/1000\n",
      "355/355 [==============================] - 89s 251ms/step - loss: 475.3169 - val_loss: 7957.5820\n",
      "Epoch 48/1000\n",
      "355/355 [==============================] - 88s 249ms/step - loss: 426.7780 - val_loss: 852.2328\n",
      "Epoch 49/1000\n",
      "355/355 [==============================] - 87s 246ms/step - loss: 453.4919 - val_loss: 1049.4094\n",
      "Epoch 50/1000\n",
      "355/355 [==============================] - 87s 245ms/step - loss: 404.7739 - val_loss: 970.6992\n",
      "Epoch 51/1000\n",
      "355/355 [==============================] - 87s 246ms/step - loss: 481.2761 - val_loss: 8349.8418\n",
      "Epoch 52/1000\n",
      "355/355 [==============================] - 88s 248ms/step - loss: 479.6592 - val_loss: 311.6687\n",
      "Epoch 53/1000\n",
      "355/355 [==============================] - 87s 244ms/step - loss: 409.8933 - val_loss: 2815.3296\n",
      "Epoch 54/1000\n",
      "355/355 [==============================] - 88s 248ms/step - loss: 474.1137 - val_loss: 1729.8138\n",
      "Epoch 55/1000\n",
      "355/355 [==============================] - 88s 247ms/step - loss: 415.7791 - val_loss: 613.2369\n",
      "Epoch 56/1000\n",
      "355/355 [==============================] - 87s 246ms/step - loss: 441.0002 - val_loss: 4207.3071\n",
      "Epoch 57/1000\n",
      "355/355 [==============================] - 88s 247ms/step - loss: 507.4158 - val_loss: 1194.2615\n",
      "Epoch 58/1000\n",
      "355/355 [==============================] - 88s 247ms/step - loss: 418.9736 - val_loss: 2422.4272\n",
      "Epoch 59/1000\n",
      "355/355 [==============================] - 88s 249ms/step - loss: 406.8038 - val_loss: 1141.5482\n",
      "Epoch 60/1000\n",
      "355/355 [==============================] - 83s 234ms/step - loss: 463.2537 - val_loss: 1415.8501\n",
      "Epoch 61/1000\n",
      "355/355 [==============================] - 1419s 4s/step - loss: 509.9047 - val_loss: 2562.0278\n",
      "Epoch 62/1000\n",
      "355/355 [==============================] - 77s 217ms/step - loss: 504.8528 - val_loss: 15742.5244\n",
      "Epoch 63/1000\n",
      "355/355 [==============================] - 39s 111ms/step - loss: 452.6597 - val_loss: 1052.1387\n",
      "Epoch 64/1000\n",
      "355/355 [==============================] - 45s 126ms/step - loss: 442.7572 - val_loss: 2963.8674\n",
      "Epoch 65/1000\n",
      "355/355 [==============================] - 45s 126ms/step - loss: 428.1247 - val_loss: 535.8888\n",
      "Epoch 66/1000\n",
      "355/355 [==============================] - 48s 135ms/step - loss: 460.9793 - val_loss: 557.7028\n",
      "Epoch 67/1000\n",
      "355/355 [==============================] - 50s 140ms/step - loss: 410.4386 - val_loss: 4313.2124\n",
      "Epoch 68/1000\n",
      "355/355 [==============================] - 55s 156ms/step - loss: 413.8075 - val_loss: 1429.6615\n",
      "Epoch 69/1000\n",
      "355/355 [==============================] - 57s 161ms/step - loss: 440.8260 - val_loss: 482.5964\n",
      "Epoch 70/1000\n",
      "355/355 [==============================] - 57s 161ms/step - loss: 490.0808 - val_loss: 306.8202\n",
      "Epoch 71/1000\n",
      "355/355 [==============================] - 57s 160ms/step - loss: 463.9520 - val_loss: 1481.3186\n",
      "Epoch 72/1000\n",
      "355/355 [==============================] - 54s 152ms/step - loss: 450.5670 - val_loss: 1344.7737\n",
      "Epoch 73/1000\n",
      "355/355 [==============================] - 54s 151ms/step - loss: 429.7532 - val_loss: 1318.8152\n",
      "Epoch 74/1000\n",
      "355/355 [==============================] - 52s 148ms/step - loss: 504.5970 - val_loss: 1145.9813\n",
      "Epoch 75/1000\n",
      "355/355 [==============================] - 56s 157ms/step - loss: 399.9900 - val_loss: 1310.1387\n",
      "Epoch 76/1000\n",
      "355/355 [==============================] - 56s 158ms/step - loss: 434.5365 - val_loss: 12243.3672\n",
      "Epoch 77/1000\n",
      "355/355 [==============================] - 57s 159ms/step - loss: 490.2596 - val_loss: 651.0121\n",
      "Epoch 78/1000\n",
      "355/355 [==============================] - 53s 150ms/step - loss: 468.4704 - val_loss: 5090.4053\n",
      "Epoch 79/1000\n",
      "355/355 [==============================] - 55s 156ms/step - loss: 433.4182 - val_loss: 669.8524\n",
      "Epoch 80/1000\n",
      "355/355 [==============================] - 59s 167ms/step - loss: 490.4784 - val_loss: 2610.3911\n",
      "Epoch 81/1000\n",
      "355/355 [==============================] - 63s 178ms/step - loss: 458.1244 - val_loss: 424.7125\n",
      "Epoch 82/1000\n",
      "355/355 [==============================] - 60s 170ms/step - loss: 429.4602 - val_loss: 293.6133\n",
      "Epoch 83/1000\n",
      "355/355 [==============================] - 68s 191ms/step - loss: 430.9532 - val_loss: 1192.0078\n",
      "Epoch 84/1000\n",
      "355/355 [==============================] - 91s 258ms/step - loss: 436.7829 - val_loss: 8281.8896\n",
      "Epoch 85/1000\n",
      "355/355 [==============================] - 90s 254ms/step - loss: 451.0257 - val_loss: 2566.0015\n",
      "Epoch 86/1000\n",
      "355/355 [==============================] - 89s 251ms/step - loss: 467.9313 - val_loss: 1053.2496\n",
      "Epoch 87/1000\n",
      "355/355 [==============================] - 87s 245ms/step - loss: 532.1520 - val_loss: 434.2608\n",
      "Epoch 88/1000\n",
      "355/355 [==============================] - 88s 247ms/step - loss: 496.7449 - val_loss: 1141.6506\n",
      "Epoch 89/1000\n",
      "355/355 [==============================] - 94s 264ms/step - loss: 468.7637 - val_loss: 1826.3696\n",
      "Epoch 90/1000\n",
      "355/355 [==============================] - 87s 244ms/step - loss: 472.5748 - val_loss: 3400.1597\n",
      "Epoch 91/1000\n",
      "355/355 [==============================] - 86s 243ms/step - loss: 463.0588 - val_loss: 4943.6714\n",
      "Epoch 92/1000\n",
      "355/355 [==============================] - 87s 244ms/step - loss: 437.7230 - val_loss: 1504.6399\n",
      "Epoch 93/1000\n",
      "355/355 [==============================] - 87s 244ms/step - loss: 405.3413 - val_loss: 638.5938\n",
      "Epoch 94/1000\n",
      "355/355 [==============================] - 88s 247ms/step - loss: 432.0475 - val_loss: 12842.6348\n",
      "Epoch 95/1000\n",
      "355/355 [==============================] - 87s 246ms/step - loss: 406.6276 - val_loss: 1575.5344\n",
      "Epoch 96/1000\n",
      "355/355 [==============================] - 87s 245ms/step - loss: 403.8755 - val_loss: 9268.0146\n",
      "Epoch 97/1000\n",
      "355/355 [==============================] - 87s 246ms/step - loss: 397.7638 - val_loss: 3296.1489\n",
      "Epoch 98/1000\n",
      "355/355 [==============================] - 87s 246ms/step - loss: 413.9491 - val_loss: 624.9629\n",
      "Epoch 99/1000\n",
      "355/355 [==============================] - 88s 247ms/step - loss: 454.4109 - val_loss: 5426.9785\n",
      "Epoch 100/1000\n",
      "355/355 [==============================] - 88s 247ms/step - loss: 451.9129 - val_loss: 3327.4910\n",
      "Epoch 101/1000\n",
      "355/355 [==============================] - 89s 250ms/step - loss: 436.2657 - val_loss: 1758.1342\n",
      "Epoch 102/1000\n",
      "355/355 [==============================] - 89s 252ms/step - loss: 386.8758 - val_loss: 3117.2615\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f95c1ab094bd48c3b46a4fdcc56c4a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.265 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.012505…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▁▂▁▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>81</td></tr><tr><td>best_val_loss</td><td>293.61331</td></tr><tr><td>epoch</td><td>101</td></tr><tr><td>loss</td><td>386.87576</td></tr><tr><td>val_loss</td><td>3117.26147</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">woven-lion-80</strong>: <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/30brlfcr\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/30brlfcr</a><br/>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230207_123606-30brlfcr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = {\n",
    "    'LSTM_units': 8,\n",
    "    'MLP_units': 100,\n",
    "    'LSTM_timesteps': 20,\n",
    "    'LSTM_layers': 3,\n",
    "    'MLP_layers': 4,\n",
    "    'Bn_momentum': 0.99,\n",
    "    'Lr': 0.01,\n",
    "    'Lr_decay': 1,\n",
    "    'Minibatch_size': 4096,\n",
    "    'Min_delta': 1,\n",
    "    'Patience': 20,\n",
    "    'Num_features': 4,\n",
    "    'Architecture': 'LSTM-MLP v.0.1',\n",
    "    'Dataset': '2022: Val split 01.11 with shuffle',\n",
    "}\n",
    "trainer(config = config, project = 'Deep learning for option pricing - test area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d22c5043c0ee61c8547bd88adb0f85d3d6a0a630c1da4282026f99271dac814"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
