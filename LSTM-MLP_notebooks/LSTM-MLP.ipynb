{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights and Biases\n",
    "!pip install -q wandb\n",
    "# Tensorflow\n",
    "!pip install -q tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, LSTM, Concatenate, Dense, BatchNormalization, LeakyReLU\n",
    "from keras.activations import tanh\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33merlendrygg\u001b[0m (\u001b[33mavogadro\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Erlend/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.login(key='3cae81eb56be3190be5bb48c571e69933071df69')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Quote_date</th>\n",
       "      <th>Expire_date</th>\n",
       "      <th>Price</th>\n",
       "      <th>Underlying_last</th>\n",
       "      <th>Strike</th>\n",
       "      <th>TTM</th>\n",
       "      <th>Underlying_1</th>\n",
       "      <th>Underlying_2</th>\n",
       "      <th>Underlying_3</th>\n",
       "      <th>...</th>\n",
       "      <th>Underlying_12</th>\n",
       "      <th>Underlying_13</th>\n",
       "      <th>Underlying_14</th>\n",
       "      <th>Underlying_15</th>\n",
       "      <th>Underlying_16</th>\n",
       "      <th>Underlying_17</th>\n",
       "      <th>Underlying_18</th>\n",
       "      <th>Underlying_19</th>\n",
       "      <th>Underlying_20</th>\n",
       "      <th>R</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>173719</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>3312.50</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>173720</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>3114.00</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>173721</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>2913.55</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>173722</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>2712.60</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>173723</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>2512.60</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>173724</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>2312.60</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>173725</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>2112.60</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>173726</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>1912.60</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>2600.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>173727</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>1712.60</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>2800.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>173728</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>1612.60</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>2900.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>173729</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>1513.25</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>173730</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>1412.60</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>3100.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>173731</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>1313.35</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>173732</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>1213.35</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>3300.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>173733</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>1113.40</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>3400.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>173734</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>1013.40</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>173735</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>963.40</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>3550.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>173736</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>913.40</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>173737</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>863.45</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>3650.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>173738</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>838.45</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>3675.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>173739</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>813.70</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>3700.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>173740</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>788.70</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>3725.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>173741</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>763.70</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>173742</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>738.75</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>3775.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>173743</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>713.75</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>173744</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>688.75</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>3825.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>173745</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>663.80</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>3850.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>173746</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>638.65</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>3875.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>173747</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>613.70</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>3900.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>173748</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-02-02</td>\n",
       "      <td>588.65</td>\n",
       "      <td>4516.89</td>\n",
       "      <td>3925.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4431.8</td>\n",
       "      <td>4325.89</td>\n",
       "      <td>4347.26</td>\n",
       "      <td>...</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>4795.57</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  Quote_date Expire_date    Price  Underlying_last  Strike  TTM  \\\n",
       "0       173719  2022-01-31  2022-02-02  3312.50          4516.89  1200.0    2   \n",
       "1       173720  2022-01-31  2022-02-02  3114.00          4516.89  1400.0    2   \n",
       "2       173721  2022-01-31  2022-02-02  2913.55          4516.89  1600.0    2   \n",
       "3       173722  2022-01-31  2022-02-02  2712.60          4516.89  1800.0    2   \n",
       "4       173723  2022-01-31  2022-02-02  2512.60          4516.89  2000.0    2   \n",
       "5       173724  2022-01-31  2022-02-02  2312.60          4516.89  2200.0    2   \n",
       "6       173725  2022-01-31  2022-02-02  2112.60          4516.89  2400.0    2   \n",
       "7       173726  2022-01-31  2022-02-02  1912.60          4516.89  2600.0    2   \n",
       "8       173727  2022-01-31  2022-02-02  1712.60          4516.89  2800.0    2   \n",
       "9       173728  2022-01-31  2022-02-02  1612.60          4516.89  2900.0    2   \n",
       "10      173729  2022-01-31  2022-02-02  1513.25          4516.89  3000.0    2   \n",
       "11      173730  2022-01-31  2022-02-02  1412.60          4516.89  3100.0    2   \n",
       "12      173731  2022-01-31  2022-02-02  1313.35          4516.89  3200.0    2   \n",
       "13      173732  2022-01-31  2022-02-02  1213.35          4516.89  3300.0    2   \n",
       "14      173733  2022-01-31  2022-02-02  1113.40          4516.89  3400.0    2   \n",
       "15      173734  2022-01-31  2022-02-02  1013.40          4516.89  3500.0    2   \n",
       "16      173735  2022-01-31  2022-02-02   963.40          4516.89  3550.0    2   \n",
       "17      173736  2022-01-31  2022-02-02   913.40          4516.89  3600.0    2   \n",
       "18      173737  2022-01-31  2022-02-02   863.45          4516.89  3650.0    2   \n",
       "19      173738  2022-01-31  2022-02-02   838.45          4516.89  3675.0    2   \n",
       "20      173739  2022-01-31  2022-02-02   813.70          4516.89  3700.0    2   \n",
       "21      173740  2022-01-31  2022-02-02   788.70          4516.89  3725.0    2   \n",
       "22      173741  2022-01-31  2022-02-02   763.70          4516.89  3750.0    2   \n",
       "23      173742  2022-01-31  2022-02-02   738.75          4516.89  3775.0    2   \n",
       "24      173743  2022-01-31  2022-02-02   713.75          4516.89  3800.0    2   \n",
       "25      173744  2022-01-31  2022-02-02   688.75          4516.89  3825.0    2   \n",
       "26      173745  2022-01-31  2022-02-02   663.80          4516.89  3850.0    2   \n",
       "27      173746  2022-01-31  2022-02-02   638.65          4516.89  3875.0    2   \n",
       "28      173747  2022-01-31  2022-02-02   613.70          4516.89  3900.0    2   \n",
       "29      173748  2022-01-31  2022-02-02   588.65          4516.89  3925.0    2   \n",
       "\n",
       "    Underlying_1  Underlying_2  Underlying_3  ...  Underlying_12  \\\n",
       "0         4431.8       4325.89       4347.26  ...        4660.64   \n",
       "1         4431.8       4325.89       4347.26  ...        4660.64   \n",
       "2         4431.8       4325.89       4347.26  ...        4660.64   \n",
       "3         4431.8       4325.89       4347.26  ...        4660.64   \n",
       "4         4431.8       4325.89       4347.26  ...        4660.64   \n",
       "5         4431.8       4325.89       4347.26  ...        4660.64   \n",
       "6         4431.8       4325.89       4347.26  ...        4660.64   \n",
       "7         4431.8       4325.89       4347.26  ...        4660.64   \n",
       "8         4431.8       4325.89       4347.26  ...        4660.64   \n",
       "9         4431.8       4325.89       4347.26  ...        4660.64   \n",
       "10        4431.8       4325.89       4347.26  ...        4660.64   \n",
       "11        4431.8       4325.89       4347.26  ...        4660.64   \n",
       "12        4431.8       4325.89       4347.26  ...        4660.64   \n",
       "13        4431.8       4325.89       4347.26  ...        4660.64   \n",
       "14        4431.8       4325.89       4347.26  ...        4660.64   \n",
       "15        4431.8       4325.89       4347.26  ...        4660.64   \n",
       "16        4431.8       4325.89       4347.26  ...        4660.64   \n",
       "17        4431.8       4325.89       4347.26  ...        4660.64   \n",
       "18        4431.8       4325.89       4347.26  ...        4660.64   \n",
       "19        4431.8       4325.89       4347.26  ...        4660.64   \n",
       "20        4431.8       4325.89       4347.26  ...        4660.64   \n",
       "21        4431.8       4325.89       4347.26  ...        4660.64   \n",
       "22        4431.8       4325.89       4347.26  ...        4660.64   \n",
       "23        4431.8       4325.89       4347.26  ...        4660.64   \n",
       "24        4431.8       4325.89       4347.26  ...        4660.64   \n",
       "25        4431.8       4325.89       4347.26  ...        4660.64   \n",
       "26        4431.8       4325.89       4347.26  ...        4660.64   \n",
       "27        4431.8       4325.89       4347.26  ...        4660.64   \n",
       "28        4431.8       4325.89       4347.26  ...        4660.64   \n",
       "29        4431.8       4325.89       4347.26  ...        4660.64   \n",
       "\n",
       "    Underlying_13  Underlying_14  Underlying_15  Underlying_16  Underlying_17  \\\n",
       "0         4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "1         4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "2         4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "3         4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "4         4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "5         4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "6         4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "7         4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "8         4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "9         4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "10        4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "11        4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "12        4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "13        4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "14        4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "15        4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "16        4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "17        4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "18        4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "19        4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "20        4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "21        4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "22        4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "23        4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "24        4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "25        4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "26        4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "27        4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "28        4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "29        4726.55        4713.53        4669.85        4676.41        4696.25   \n",
       "\n",
       "    Underlying_18  Underlying_19  Underlying_20     R  \n",
       "0         4700.64        4793.19        4795.57  0.03  \n",
       "1         4700.64        4793.19        4795.57  0.03  \n",
       "2         4700.64        4793.19        4795.57  0.03  \n",
       "3         4700.64        4793.19        4795.57  0.03  \n",
       "4         4700.64        4793.19        4795.57  0.03  \n",
       "5         4700.64        4793.19        4795.57  0.03  \n",
       "6         4700.64        4793.19        4795.57  0.03  \n",
       "7         4700.64        4793.19        4795.57  0.03  \n",
       "8         4700.64        4793.19        4795.57  0.03  \n",
       "9         4700.64        4793.19        4795.57  0.03  \n",
       "10        4700.64        4793.19        4795.57  0.03  \n",
       "11        4700.64        4793.19        4795.57  0.03  \n",
       "12        4700.64        4793.19        4795.57  0.03  \n",
       "13        4700.64        4793.19        4795.57  0.03  \n",
       "14        4700.64        4793.19        4795.57  0.03  \n",
       "15        4700.64        4793.19        4795.57  0.03  \n",
       "16        4700.64        4793.19        4795.57  0.03  \n",
       "17        4700.64        4793.19        4795.57  0.03  \n",
       "18        4700.64        4793.19        4795.57  0.03  \n",
       "19        4700.64        4793.19        4795.57  0.03  \n",
       "20        4700.64        4793.19        4795.57  0.03  \n",
       "21        4700.64        4793.19        4795.57  0.03  \n",
       "22        4700.64        4793.19        4795.57  0.03  \n",
       "23        4700.64        4793.19        4795.57  0.03  \n",
       "24        4700.64        4793.19        4795.57  0.03  \n",
       "25        4700.64        4793.19        4795.57  0.03  \n",
       "26        4700.64        4793.19        4795.57  0.03  \n",
       "27        4700.64        4793.19        4795.57  0.03  \n",
       "28        4700.64        4793.19        4795.57  0.03  \n",
       "29        4700.64        4793.19        4795.57  0.03  \n",
       "\n",
       "[30 rows x 28 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file = \"../data/processed_data/2022.csv\"\n",
    "df_read = pd.read_csv(file)\n",
    "display(df_read.head(30))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1452255, 20, 1), (1452255, 4)\n",
      "Val shape: (311065, 20, 1), (311065, 4)\n"
     ]
    }
   ],
   "source": [
    "df = df_read\n",
    "\n",
    "# Formating settings\n",
    "val_start = \"2022-11-01\"\n",
    "num_features = 4\n",
    "time_steps = 20\n",
    "bs_vars = ['Underlying_last', 'Strike', 'TTM', 'R']\n",
    "underlying_lags = ['Underlying_last'] + [f'Underlying_{i}' for i in range (1, time_steps)]\n",
    "\n",
    "# Split train and validation data\n",
    "df_train = df[df['Quote_date'] < val_start]\n",
    "df_val = df[df['Quote_date'] >= val_start]\n",
    "\n",
    "# Extract target values\n",
    "train_y = df_train['Price'].to_numpy()\n",
    "val_y = df_val['Price'].to_numpy()\n",
    "\n",
    "# Convert dataframes to numpy arrays\n",
    "train_x = [df_train[underlying_lags].to_numpy(), df_train[bs_vars].to_numpy()]\n",
    "val_x = [df_val[underlying_lags].to_numpy(), df_val[bs_vars].to_numpy()]\n",
    "\n",
    "# Scale features based on training set\n",
    "underlying_scaler = MinMaxScaler()\n",
    "train_x[0] = underlying_scaler.fit_transform(train_x[0])\n",
    "val_x[0] = underlying_scaler.transform(val_x[0])\n",
    "\n",
    "bs_scaler = MinMaxScaler()\n",
    "train_x[1] = bs_scaler.fit_transform(train_x[1])\n",
    "val_x[1] = bs_scaler.transform(val_x[1])\n",
    "\n",
    "# Reshape data to fit LSTM\n",
    "train_x = [train_x[0].reshape(len(train_x[0]),time_steps,1), train_x[1]]\n",
    "val_x = [val_x[0].reshape(len(val_x[0]), time_steps, 1), val_x[1]]\n",
    "\n",
    "# Shuffle training set\n",
    "# WIP - Loss stays at 4-500 000 for the first epochs when used\n",
    "#shuffle = np.random.permutation(len(train_x[0]))\n",
    "#train_x = [train_x[0][shuffle], train_x[1][shuffle]]\n",
    "\n",
    "print(f'Train shape: {train_x[0].shape}, {train_x[1].shape}')\n",
    "print(f'Val shape: {val_x[0].shape}, {val_x[1].shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(config):\n",
    "    '''Builds an LSTM-MLP model of minimum 2 layers sequentially from a given config dictionary'''\n",
    "\n",
    "    # Input layers\n",
    "    underlying_history = Input((config.LSTM_timesteps,1))\n",
    "    bs_vars = Input((config.Num_features,))\n",
    "\n",
    "    # LSTM layers\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(\n",
    "        units = config.LSTM_units,\n",
    "        activation = tanh,\n",
    "        input_shape = (config.LSTM_timesteps, 1),\n",
    "        return_sequences = True\n",
    "    ))\n",
    "\n",
    "    for _ in range(config.LSTM_layers - 2):\n",
    "        model.add(LSTM(\n",
    "            units = config.LSTM_units,\n",
    "            activation = tanh,\n",
    "            return_sequences = True\n",
    "        ))\n",
    "    \n",
    "    model.add(LSTM(\n",
    "        units = config.LSTM_units,\n",
    "        activation = tanh,\n",
    "        return_sequences = False\n",
    "    ))\n",
    "\n",
    "    # MLP layers\n",
    "    layers = Concatenate()([model(underlying_history), bs_vars])\n",
    "    \n",
    "    for _ in range(config.MLP_layers - 1):\n",
    "        layers = Dense(config.MLP_units)(layers)\n",
    "        layers = BatchNormalization(momentum=config.Bn_momentum)(layers)\n",
    "        layers = LeakyReLU()(layers)\n",
    "\n",
    "    output = Dense(1, activation='relu')(layers)\n",
    "\n",
    "    # Exponential decaying learning rate\n",
    "    lr_schedule = ExponentialDecay(\n",
    "        initial_learning_rate = config.Lr,\n",
    "        decay_steps = int(len(train_x)/config.Minibatch_size),\n",
    "        decay_rate=config.Lr_decay\n",
    "    )\n",
    "\n",
    "    # Compile model\n",
    "    model = Model(inputs=[underlying_history, bs_vars], outputs=output)\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=lr_schedule))\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter search setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring the sweep hyperparameter search space\n",
    "sweep_configuration = {\n",
    "    'method': 'random',\n",
    "    'name': 'LSTM-MLP v.1.0 testing',\n",
    "    'metric': {\n",
    "        'goal': 'minimize', \n",
    "        'name': 'val_loss'\n",
    "\t\t},\n",
    "    'parameters': {\n",
    "        'LSTM_units': {\n",
    "            'values': [32, 64, 96, 128]},\n",
    "        'MLP_units': {\n",
    "            'values': [32, 64, 96, 128]},\n",
    "        'LSTM_timesteps': {\n",
    "            'values': [10]},\n",
    "        'LSTM_layers': {\n",
    "            'distribution': 'int_uniform',\n",
    "            'max': 6, 'min': 4},\n",
    "        'MLP_layers': {\n",
    "            'distribution': 'int_uniform',\n",
    "            'max': 6, 'min': 4},\n",
    "        'Bn_momentum': {\n",
    "            'distribution': 'uniform',\n",
    "            'max': 0.4, 'min': 0},\n",
    "        'Lr': {\n",
    "            'distribution': 'uniform',\n",
    "            'max': 0.005, 'min': 0.0005},\n",
    "        'Lr_decay': {\n",
    "            'distribution': 'uniform',\n",
    "            'max': 1, 'min': 0.9},        \n",
    "        'Minibatch_size': {\n",
    "            'values': [1024, 2048, 4096]},\n",
    "        'Min_delta': {\n",
    "            'value': 1},\n",
    "        'Patience': {\n",
    "            'value': 20},\n",
    "        'Num_features': {\n",
    "            'value': 4},\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize sweep and creating sweepID\n",
    "\n",
    "# If new sweep, uncomment the line below and comment the line after it\n",
    "#sweep_id = wandb.sweep(sweep=sweep_configuration, project='Deep learning for option pricing - test area') \n",
    "#sweep_id = '98bxt6oq'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(train_x = train_x, train_y = train_y, val_x = val_x, val_y = val_y, config = None, project = None):\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config, project = project):\n",
    "\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "\n",
    "        # Build model and create callbacks\n",
    "        model = create_model(config)\n",
    "\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            min_delta = config.Min_delta,\n",
    "            patience = config.Patience,\n",
    "        )\n",
    "        \n",
    "        wandb_callback = WandbCallback(\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            save_model=False\n",
    "        )\n",
    "\n",
    "        # Adapt sequence length to config\n",
    "        train_x[0] = train_x[0][:, :config.LSTM_timesteps, :]\n",
    "        val_x[0] = val_x[0][:, :config.LSTM_timesteps, :]\n",
    "        print(f'Train shape: {train_x[0].shape}, {train_x[1].shape}')\n",
    "        print(f'Val shape: {val_x[0].shape}, {val_x[1].shape}')\n",
    "\n",
    "        # Train model\n",
    "        model.fit(\n",
    "            train_x,\n",
    "            train_y,\n",
    "            batch_size = config.Minibatch_size,\n",
    "            validation_data = (val_x, val_y),\n",
    "            epochs = 1000,\n",
    "            callbacks = [early_stopping, wandb_callback] \n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run full sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.agent(sweep_id=sweep_id, function=trainer, project='Deep learning for option pricing - test area', count = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.13.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Erlend\\Google Drive\\NTNU\\5. Klasse\\Master\\Git-folder\\deep-learning-for-option-pricing\\LSTM-MLP_notebooks\\wandb\\run-20230204_181058-2yeprom1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/2yeprom1\" target=\"_blank\">alight-fish-49</a></strong> to <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 20, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 8)            1408        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 12)           0           ['sequential[0][0]',             \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 100)          1300        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 100)         400         ['dense[0][0]']                  \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 100)          0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 100)          10100       ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 100)         400         ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 100)          0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 100)          10100       ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 100)         400         ['dense_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 100)          0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            101         ['leaky_re_lu_2[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 24,209\n",
      "Trainable params: 23,609\n",
      "Non-trainable params: 600\n",
      "__________________________________________________________________________________________________\n",
      "Train shape: (1452255, 20, 1), (1452255, 4)\n",
      "Val shape: (311065, 20, 1), (311065, 4)\n",
      "Epoch 1/1000\n",
      "355/355 [==============================] - 127s 331ms/step - loss: 220851.7969 - val_loss: 173176.5469\n",
      "Epoch 2/1000\n",
      "355/355 [==============================] - 112s 315ms/step - loss: 506.4206 - val_loss: 59040.4492\n",
      "Epoch 3/1000\n",
      "355/355 [==============================] - 115s 324ms/step - loss: 503.3889 - val_loss: 9667.5039\n",
      "Epoch 4/1000\n",
      "355/355 [==============================] - 104s 294ms/step - loss: 442.7567 - val_loss: 20124.9043\n",
      "Epoch 5/1000\n",
      "355/355 [==============================] - 81s 229ms/step - loss: 440.4411 - val_loss: 88737.7422\n",
      "Epoch 6/1000\n",
      "355/355 [==============================] - 80s 227ms/step - loss: 465.0238 - val_loss: 1680.7517\n",
      "Epoch 7/1000\n",
      "355/355 [==============================] - 83s 234ms/step - loss: 390.3875 - val_loss: 54816.0586\n",
      "Epoch 8/1000\n",
      "355/355 [==============================] - 81s 227ms/step - loss: 446.6185 - val_loss: 10034.8223\n",
      "Epoch 9/1000\n",
      "355/355 [==============================] - 81s 227ms/step - loss: 469.6968 - val_loss: 6426.3086\n",
      "Epoch 10/1000\n",
      "355/355 [==============================] - 78s 219ms/step - loss: 463.8981 - val_loss: 388.4735\n",
      "Epoch 11/1000\n",
      "355/355 [==============================] - 73s 205ms/step - loss: 475.3435 - val_loss: 74828.8984\n",
      "Epoch 12/1000\n",
      "355/355 [==============================] - 62s 175ms/step - loss: 458.8546 - val_loss: 4674.6831\n",
      "Epoch 13/1000\n",
      "355/355 [==============================] - 62s 175ms/step - loss: 506.4820 - val_loss: 19050.3164\n",
      "Epoch 14/1000\n",
      "355/355 [==============================] - 55s 155ms/step - loss: 420.1768 - val_loss: 7374.5923\n",
      "Epoch 15/1000\n",
      "355/355 [==============================] - 62s 174ms/step - loss: 435.8184 - val_loss: 2808.6428\n",
      "Epoch 16/1000\n",
      "141/355 [==========>...................] - ETA: 32s - loss: 378.5896"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'LSTM_units': 8,\n",
    "    'MLP_units': 100,\n",
    "    'LSTM_timesteps': 20,\n",
    "    'LSTM_layers': 3,\n",
    "    'MLP_layers': 4,\n",
    "    'Bn_momentum': 0.99,\n",
    "    'Lr': 0.01,\n",
    "    'Lr_decay': 1,\n",
    "    'Minibatch_size': 4096,\n",
    "    'Min_delta': 1,\n",
    "    'Patience': 20,\n",
    "    'Num_features': 4,\n",
    "    'Architecture': 'LSTM-MLP v.0.1',\n",
    "    'Dataset': '2022: Val split 01.11',\n",
    "}\n",
    "trainer(config = config, project = 'Deep learning for option pricing - test area')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
