{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights and Biases\n",
    "!pip install -q wandb\n",
    "# Tensorflow\n",
    "!pip install -q tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, LSTM, Concatenate, Dense, BatchNormalization, LeakyReLU\n",
    "from keras.activations import tanh\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from tensorflow import square, reduce_mean\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.math import multiply\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvinje\u001b[0m (\u001b[33mavogadro\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/hjalmarjacobvinje/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If running in colab, insert your wandb key here\n",
    "\n",
    "import config\n",
    "#Erlend\n",
    "#wandb.login(key=config.erlend_key)\n",
    "# Hjalmar\n",
    "wandb.login(key=config.hjalmar_key)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, split and normalize data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Quote_date</th>\n",
       "      <th>Price</th>\n",
       "      <th>Underlying_last</th>\n",
       "      <th>Strike</th>\n",
       "      <th>TTM</th>\n",
       "      <th>R</th>\n",
       "      <th>Moneyness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6701729</td>\n",
       "      <td>6701729</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>1755.645</td>\n",
       "      <td>3258.14</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.53</td>\n",
       "      <td>2.172093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6701730</td>\n",
       "      <td>6701730</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>1655.955</td>\n",
       "      <td>3258.14</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.53</td>\n",
       "      <td>2.036338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6701731</td>\n",
       "      <td>6701731</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>1556.195</td>\n",
       "      <td>3258.14</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1.916553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6701732</td>\n",
       "      <td>6701732</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>1456.210</td>\n",
       "      <td>3258.14</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1.810078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6701733</td>\n",
       "      <td>6701733</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>1406.200</td>\n",
       "      <td>3258.14</td>\n",
       "      <td>1850.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1.761157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5185540</th>\n",
       "      <td>12459146</td>\n",
       "      <td>12459146</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>415.150</td>\n",
       "      <td>3839.81</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>1085</td>\n",
       "      <td>4.22</td>\n",
       "      <td>0.853291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5185541</th>\n",
       "      <td>12459147</td>\n",
       "      <td>12459147</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>375.050</td>\n",
       "      <td>3839.81</td>\n",
       "      <td>4600.0</td>\n",
       "      <td>1085</td>\n",
       "      <td>4.22</td>\n",
       "      <td>0.834741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5185542</th>\n",
       "      <td>12459148</td>\n",
       "      <td>12459148</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>337.350</td>\n",
       "      <td>3839.81</td>\n",
       "      <td>4700.0</td>\n",
       "      <td>1085</td>\n",
       "      <td>4.22</td>\n",
       "      <td>0.816981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5185543</th>\n",
       "      <td>12459149</td>\n",
       "      <td>12459149</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>302.650</td>\n",
       "      <td>3839.81</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>1085</td>\n",
       "      <td>4.22</td>\n",
       "      <td>0.799960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5185544</th>\n",
       "      <td>12459150</td>\n",
       "      <td>12459150</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>270.450</td>\n",
       "      <td>3839.81</td>\n",
       "      <td>4900.0</td>\n",
       "      <td>1085</td>\n",
       "      <td>4.22</td>\n",
       "      <td>0.783635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5185545 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0.1  Unnamed: 0  Quote_date     Price  Underlying_last  \\\n",
       "0             6701729     6701729  2020-01-02  1755.645          3258.14   \n",
       "1             6701730     6701730  2020-01-02  1655.955          3258.14   \n",
       "2             6701731     6701731  2020-01-02  1556.195          3258.14   \n",
       "3             6701732     6701732  2020-01-02  1456.210          3258.14   \n",
       "4             6701733     6701733  2020-01-02  1406.200          3258.14   \n",
       "...               ...         ...         ...       ...              ...   \n",
       "5185540      12459146    12459146  2022-12-30   415.150          3839.81   \n",
       "5185541      12459147    12459147  2022-12-30   375.050          3839.81   \n",
       "5185542      12459148    12459148  2022-12-30   337.350          3839.81   \n",
       "5185543      12459149    12459149  2022-12-30   302.650          3839.81   \n",
       "5185544      12459150    12459150  2022-12-30   270.450          3839.81   \n",
       "\n",
       "         Strike   TTM     R  Moneyness  \n",
       "0        1500.0     1  1.53   2.172093  \n",
       "1        1600.0     1  1.53   2.036338  \n",
       "2        1700.0     1  1.53   1.916553  \n",
       "3        1800.0     1  1.53   1.810078  \n",
       "4        1850.0     1  1.53   1.761157  \n",
       "...         ...   ...   ...        ...  \n",
       "5185540  4500.0  1085  4.22   0.853291  \n",
       "5185541  4600.0  1085  4.22   0.834741  \n",
       "5185542  4700.0  1085  4.22   0.816981  \n",
       "5185543  4800.0  1085  4.22   0.799960  \n",
       "5185544  4900.0  1085  4.22   0.783635  \n",
       "\n",
       "[5185545 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "google_colab = False\n",
    "\n",
    "if google_colab:\n",
    "    import tensorflow as tf\n",
    "    # Pring info\n",
    "    gpu_info = !nvidia-smi\n",
    "    gpu_info = '\\n'.join(gpu_info)\n",
    "    if gpu_info.find('failed') >= 0:\n",
    "        print('Not connected to a GPU')\n",
    "    else:\n",
    "        print(gpu_info)\n",
    "    \n",
    "    from psutil import virtual_memory\n",
    "    ram_gb = virtual_memory().total / 1e9\n",
    "    print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "    if ram_gb < 20:\n",
    "        print('Not using a high-RAM runtime')\n",
    "    else:\n",
    "        print('You are using a high-RAM runtime!')\n",
    "\n",
    "    # Code to read csv file into Colaboratory:\n",
    "    !pip install -U -q PyDrive\n",
    "    from pydrive.auth import GoogleAuth\n",
    "    from pydrive.drive import GoogleDrive\n",
    "    from google.colab import auth\n",
    "    from oauth2client.client import GoogleCredentials\n",
    "    # Authenticate and create the PyDrive client.\n",
    "    auth.authenticate_user()\n",
    "    gauth = GoogleAuth()\n",
    "    gauth.credentials = GoogleCredentials.get_application_default()\n",
    "    drive = GoogleDrive(gauth)\n",
    "    id = \"1Ic73MqpS5ACG1p2oJ_R5cTuEhlQRM_Q6\"\n",
    "    downloaded = drive.CreateFile({'id':id}) \n",
    "    downloaded.GetContentFile('2013-2022_wo_lags.csv')  \n",
    "    df_read = pd.read_csv('2013-2022_wo_lags.csv')\n",
    "else:\n",
    "    file = \"../data/processed_data/2020_2022_moneyness_filtere.csv\"\n",
    "    df_read = pd.read_csv(file)\n",
    "\n",
    "display(df_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_read\n",
    "del df_read\n",
    "\n",
    "# Group the data by Quote Date and calculate the mean for Underlying Price\n",
    "df_agg = df.groupby('Quote_date').mean().reset_index()\n",
    "\n",
    "# Values to returns\n",
    "df_agg[\"Underlying_return\"] = df_agg[\"Underlying_last\"].pct_change()\n",
    "\n",
    "lags = 10\n",
    "\n",
    "# Add the Underlying Price Lag column\n",
    "for i in range(1, lags + 1):\n",
    "    df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
    "\n",
    "df = pd.merge(df, df_agg[['Quote_date', \"Underlying_return\"] + ['Underlying_' + str(i) for i in range(1, lags + 1)]], on='Quote_date', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Quote_date</th>\n",
       "      <th>Price</th>\n",
       "      <th>Underlying_last</th>\n",
       "      <th>Strike</th>\n",
       "      <th>TTM</th>\n",
       "      <th>R</th>\n",
       "      <th>Moneyness</th>\n",
       "      <th>Underlying_return</th>\n",
       "      <th>Underlying_1</th>\n",
       "      <th>Underlying_2</th>\n",
       "      <th>Underlying_3</th>\n",
       "      <th>Underlying_4</th>\n",
       "      <th>Underlying_5</th>\n",
       "      <th>Underlying_6</th>\n",
       "      <th>Underlying_7</th>\n",
       "      <th>Underlying_8</th>\n",
       "      <th>Underlying_9</th>\n",
       "      <th>Underlying_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6701729</td>\n",
       "      <td>6701729</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>1755.645</td>\n",
       "      <td>3258.14</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.53</td>\n",
       "      <td>2.172093</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6701730</td>\n",
       "      <td>6701730</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>1655.955</td>\n",
       "      <td>3258.14</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.53</td>\n",
       "      <td>2.036338</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6701731</td>\n",
       "      <td>6701731</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>1556.195</td>\n",
       "      <td>3258.14</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1.916553</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6701732</td>\n",
       "      <td>6701732</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>1456.210</td>\n",
       "      <td>3258.14</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1.810078</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6701733</td>\n",
       "      <td>6701733</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>1406.200</td>\n",
       "      <td>3258.14</td>\n",
       "      <td>1850.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1.761157</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5185540</th>\n",
       "      <td>12459146</td>\n",
       "      <td>12459146</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>415.150</td>\n",
       "      <td>3839.81</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>1085</td>\n",
       "      <td>4.22</td>\n",
       "      <td>0.853291</td>\n",
       "      <td>-0.0023</td>\n",
       "      <td>0.017343</td>\n",
       "      <td>-0.012034</td>\n",
       "      <td>-0.004081</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.005621</td>\n",
       "      <td>-0.014208</td>\n",
       "      <td>0.014673</td>\n",
       "      <td>0.001048</td>\n",
       "      <td>-0.020257</td>\n",
       "      <td>-0.024582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5185541</th>\n",
       "      <td>12459147</td>\n",
       "      <td>12459147</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>375.050</td>\n",
       "      <td>3839.81</td>\n",
       "      <td>4600.0</td>\n",
       "      <td>1085</td>\n",
       "      <td>4.22</td>\n",
       "      <td>0.834741</td>\n",
       "      <td>-0.0023</td>\n",
       "      <td>0.017343</td>\n",
       "      <td>-0.012034</td>\n",
       "      <td>-0.004081</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.005621</td>\n",
       "      <td>-0.014208</td>\n",
       "      <td>0.014673</td>\n",
       "      <td>0.001048</td>\n",
       "      <td>-0.020257</td>\n",
       "      <td>-0.024582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5185542</th>\n",
       "      <td>12459148</td>\n",
       "      <td>12459148</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>337.350</td>\n",
       "      <td>3839.81</td>\n",
       "      <td>4700.0</td>\n",
       "      <td>1085</td>\n",
       "      <td>4.22</td>\n",
       "      <td>0.816981</td>\n",
       "      <td>-0.0023</td>\n",
       "      <td>0.017343</td>\n",
       "      <td>-0.012034</td>\n",
       "      <td>-0.004081</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.005621</td>\n",
       "      <td>-0.014208</td>\n",
       "      <td>0.014673</td>\n",
       "      <td>0.001048</td>\n",
       "      <td>-0.020257</td>\n",
       "      <td>-0.024582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5185543</th>\n",
       "      <td>12459149</td>\n",
       "      <td>12459149</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>302.650</td>\n",
       "      <td>3839.81</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>1085</td>\n",
       "      <td>4.22</td>\n",
       "      <td>0.799960</td>\n",
       "      <td>-0.0023</td>\n",
       "      <td>0.017343</td>\n",
       "      <td>-0.012034</td>\n",
       "      <td>-0.004081</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.005621</td>\n",
       "      <td>-0.014208</td>\n",
       "      <td>0.014673</td>\n",
       "      <td>0.001048</td>\n",
       "      <td>-0.020257</td>\n",
       "      <td>-0.024582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5185544</th>\n",
       "      <td>12459150</td>\n",
       "      <td>12459150</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>270.450</td>\n",
       "      <td>3839.81</td>\n",
       "      <td>4900.0</td>\n",
       "      <td>1085</td>\n",
       "      <td>4.22</td>\n",
       "      <td>0.783635</td>\n",
       "      <td>-0.0023</td>\n",
       "      <td>0.017343</td>\n",
       "      <td>-0.012034</td>\n",
       "      <td>-0.004081</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.005621</td>\n",
       "      <td>-0.014208</td>\n",
       "      <td>0.014673</td>\n",
       "      <td>0.001048</td>\n",
       "      <td>-0.020257</td>\n",
       "      <td>-0.024582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5185545 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0.1  Unnamed: 0  Quote_date     Price  Underlying_last  \\\n",
       "0             6701729     6701729  2020-01-02  1755.645          3258.14   \n",
       "1             6701730     6701730  2020-01-02  1655.955          3258.14   \n",
       "2             6701731     6701731  2020-01-02  1556.195          3258.14   \n",
       "3             6701732     6701732  2020-01-02  1456.210          3258.14   \n",
       "4             6701733     6701733  2020-01-02  1406.200          3258.14   \n",
       "...               ...         ...         ...       ...              ...   \n",
       "5185540      12459146    12459146  2022-12-30   415.150          3839.81   \n",
       "5185541      12459147    12459147  2022-12-30   375.050          3839.81   \n",
       "5185542      12459148    12459148  2022-12-30   337.350          3839.81   \n",
       "5185543      12459149    12459149  2022-12-30   302.650          3839.81   \n",
       "5185544      12459150    12459150  2022-12-30   270.450          3839.81   \n",
       "\n",
       "         Strike   TTM     R  Moneyness  Underlying_return  Underlying_1  \\\n",
       "0        1500.0     1  1.53   2.172093                NaN           NaN   \n",
       "1        1600.0     1  1.53   2.036338                NaN           NaN   \n",
       "2        1700.0     1  1.53   1.916553                NaN           NaN   \n",
       "3        1800.0     1  1.53   1.810078                NaN           NaN   \n",
       "4        1850.0     1  1.53   1.761157                NaN           NaN   \n",
       "...         ...   ...   ...        ...                ...           ...   \n",
       "5185540  4500.0  1085  4.22   0.853291            -0.0023      0.017343   \n",
       "5185541  4600.0  1085  4.22   0.834741            -0.0023      0.017343   \n",
       "5185542  4700.0  1085  4.22   0.816981            -0.0023      0.017343   \n",
       "5185543  4800.0  1085  4.22   0.799960            -0.0023      0.017343   \n",
       "5185544  4900.0  1085  4.22   0.783635            -0.0023      0.017343   \n",
       "\n",
       "         Underlying_2  Underlying_3  Underlying_4  Underlying_5  Underlying_6  \\\n",
       "0                 NaN           NaN           NaN           NaN           NaN   \n",
       "1                 NaN           NaN           NaN           NaN           NaN   \n",
       "2                 NaN           NaN           NaN           NaN           NaN   \n",
       "3                 NaN           NaN           NaN           NaN           NaN   \n",
       "4                 NaN           NaN           NaN           NaN           NaN   \n",
       "...               ...           ...           ...           ...           ...   \n",
       "5185540     -0.012034     -0.004081      0.000096      0.005621     -0.014208   \n",
       "5185541     -0.012034     -0.004081      0.000096      0.005621     -0.014208   \n",
       "5185542     -0.012034     -0.004081      0.000096      0.005621     -0.014208   \n",
       "5185543     -0.012034     -0.004081      0.000096      0.005621     -0.014208   \n",
       "5185544     -0.012034     -0.004081      0.000096      0.005621     -0.014208   \n",
       "\n",
       "         Underlying_7  Underlying_8  Underlying_9  Underlying_10  \n",
       "0                 NaN           NaN           NaN            NaN  \n",
       "1                 NaN           NaN           NaN            NaN  \n",
       "2                 NaN           NaN           NaN            NaN  \n",
       "3                 NaN           NaN           NaN            NaN  \n",
       "4                 NaN           NaN           NaN            NaN  \n",
       "...               ...           ...           ...            ...  \n",
       "5185540      0.014673      0.001048     -0.020257      -0.024582  \n",
       "5185541      0.014673      0.001048     -0.020257      -0.024582  \n",
       "5185542      0.014673      0.001048     -0.020257      -0.024582  \n",
       "5185543      0.014673      0.001048     -0.020257      -0.024582  \n",
       "5185544      0.014673      0.001048     -0.020257      -0.024582  \n",
       "\n",
       "[5185545 rows x 20 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1479245, 10, 1), (1479245, 4)\n",
      "Val shape: (176483, 10, 1), (176483, 4)\n",
      "Test shape: (184525, 10, 1), (184525, 4)\n"
     ]
    }
   ],
   "source": [
    "# Format settings\n",
    "max_timesteps = lags\n",
    "moneyness = False # Moneyness = True WIP\n",
    "bs_vars = ['Moneyness', 'TTM', 'R'] if moneyness else ['Underlying_last', 'Strike', 'TTM', 'R']\n",
    "underlying_lags = ['Underlying_last'] + [f'Underlying_{i}' for i in range (1, max_timesteps)]\n",
    "\n",
    "# Create train, validation and test set split points\n",
    "train_start = datetime(2021,1,1) \n",
    "val_start = train_start + relativedelta(months=10) \n",
    "test_start = val_start + relativedelta(months=1)\n",
    "test_end = test_start + relativedelta(months=1)\n",
    "train_start = str(train_start.date())\n",
    "val_start = str(val_start.date())\n",
    "test_start = str(test_start.date())\n",
    "test_end = str(test_end.date())\n",
    "\n",
    "# Split train and validation data\n",
    "df_train = df[(df['Quote_date'] >= train_start) & (df['Quote_date'] < val_start)]\n",
    "df_val = df[(df['Quote_date'] >= val_start) & (df['Quote_date'] < test_start)]\n",
    "df_test = df[(df['Quote_date'] >= test_start) & (df['Quote_date'] < test_end)]\n",
    "\n",
    "del df\n",
    "# Extract target values\n",
    "train_y = (df_train['Price'] / df_train['Strike']).to_numpy() if moneyness else df_train['Price'].to_numpy()\n",
    "val_y = (df_val['Price'] / df_val['Strike']).to_numpy() if moneyness else df_val['Price'].to_numpy()\n",
    "test_y = (df_test['Price'] / df_test['Strike']).to_numpy() if moneyness else df_test['Price'].to_numpy()\n",
    "\n",
    "# If usining moneyness, extract strike\n",
    "if moneyness:\n",
    "    train_strike = df_train['Strike'].to_numpy()\n",
    "    val_strike = df_val['Strike'].to_numpy()\n",
    "    test_strike = df_test['Strike'].to_numpy()\n",
    "\n",
    "# Convert dataframes to numpy arrays\n",
    "train_x = [df_train[underlying_lags].to_numpy(), df_train[bs_vars].to_numpy()]\n",
    "val_x = [df_val[underlying_lags].to_numpy(), df_val[bs_vars].to_numpy()]\n",
    "test_x = [df_test[underlying_lags].to_numpy(), df_test[bs_vars].to_numpy()]\n",
    "\n",
    "del df_train\n",
    "del df_val\n",
    "\n",
    "# Scale features based on training set\n",
    "underlying_scaler = MinMaxScaler()\n",
    "train_x[0] = underlying_scaler.fit_transform(train_x[0])\n",
    "val_x[0] = underlying_scaler.transform(val_x[0])\n",
    "test_x[0] = underlying_scaler.transform(test_x[0])\n",
    "\n",
    "bs_scaler = MinMaxScaler()\n",
    "train_x[1] = bs_scaler.fit_transform(train_x[1])\n",
    "val_x[1] = bs_scaler.transform(val_x[1])\n",
    "test_x[1] = bs_scaler.transform(test_x[1])\n",
    "\n",
    "\n",
    "# Shuffle training set\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(len(train_x[0]))\n",
    "train_x = [train_x[0][shuffle], train_x[1][shuffle]]\n",
    "train_y = train_y[shuffle]\n",
    "if moneyness:\n",
    "    train_strike = train_strike[shuffle]\n",
    "\n",
    "# Reshape data to fit LSTM\n",
    "train_x = [train_x[0].reshape(len(train_x[0]), max_timesteps, 1), train_x[1]]\n",
    "val_x = [val_x[0].reshape(len(val_x[0]), max_timesteps, 1), val_x[1]]\n",
    "test_x = [test_x[0].reshape(len(test_x[0]), max_timesteps, 1), test_x[1]]\n",
    "\n",
    "print(f'Train shape: {train_x[0].shape}, {train_x[1].shape}')\n",
    "print(f'Val shape: {val_x[0].shape}, {val_x[1].shape}')\n",
    "print(f'Test shape: {test_x[0].shape}, {test_x[1].shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(config):\n",
    "    '''Builds an LSTM-MLP model of minimum 2 layers sequentially from a given config dictionary'''\n",
    "\n",
    "    # Input layers\n",
    "    underlying_history = Input((config.LSTM_timesteps,1))\n",
    "    bs_vars = Input((config.Num_features,))\n",
    "\n",
    "    # LSTM layers\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(\n",
    "        units = config.LSTM_units,\n",
    "        activation = tanh,\n",
    "        input_shape = (config.LSTM_timesteps, 1),\n",
    "        return_sequences = True\n",
    "    ))\n",
    "\n",
    "    for _ in range(config.LSTM_layers - 2):\n",
    "        model.add(LSTM(\n",
    "            units = config.LSTM_units,\n",
    "            activation = tanh,\n",
    "            return_sequences = True\n",
    "        ))\n",
    "    \n",
    "    model.add(LSTM(\n",
    "        units = config.LSTM_units,\n",
    "        activation = tanh,\n",
    "        return_sequences = False\n",
    "    ))\n",
    "\n",
    "    # MLP layers\n",
    "    layers = Concatenate()([model(underlying_history), bs_vars])\n",
    "    \n",
    "    for _ in range(config.MLP_layers - 1):\n",
    "        layers = Dense(config.MLP_units)(layers)\n",
    "        layers = BatchNormalization(momentum=config.Bn_momentum)(layers)\n",
    "        layers = LeakyReLU()(layers)\n",
    "\n",
    "    output = Dense(1, activation='relu')(layers)\n",
    "\n",
    "    # Exponential decaying learning rate\n",
    "    lr_schedule = ExponentialDecay(\n",
    "        initial_learning_rate = config.Lr,\n",
    "        decay_steps = int(len(train_x[0])/config.Minibatch_size),\n",
    "        decay_rate=config.Lr_decay\n",
    "    )\n",
    "\n",
    "    # Compile model\n",
    "    model = Model(inputs=[underlying_history, bs_vars], outputs=output)\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=lr_schedule))\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter search setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Malformed sweep config detected! This may cause your sweep to behave in unexpected ways.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m To avoid this, please fix the sweep config schema violations below:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m   Violation 1. Lr_decay uses log_uniform, where min/max specify base-e exponents. Use log_uniform_values to specify limit values.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m   Violation 2. Lr uses log_uniform, where min/max specify base-e exponents. Use log_uniform_values to specify limit values.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: nhfyhlh1\n",
      "Sweep URL: https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/sweeps/nhfyhlh1\n"
     ]
    }
   ],
   "source": [
    "# Configuring the sweep hyperparameter search space\n",
    "sweep_configuration = {\n",
    "    'method': 'random',\n",
    "    'name': 'LSTM-MLP v4.0: fix nan issue',\n",
    "    'metric': {\n",
    "        'goal': 'minimize', \n",
    "        'name': 'val_loss'\n",
    "\t\t},\n",
    "    'parameters': {\n",
    "        'LSTM_units': {\n",
    "            'values': [4, 8, 16, 32]},\n",
    "        'MLP_units': {\n",
    "            'values': [50, 100, 200, 400, 600]},\n",
    "        'LSTM_timesteps': {\n",
    "            'values': [10, 20, 40, 60, 90, 150]},\n",
    "        'LSTM_layers': {\n",
    "            'distribution': 'int_uniform',\n",
    "            'max': 8, 'min': 2},\n",
    "        'MLP_layers': {\n",
    "            'distribution': 'int_uniform',\n",
    "            'max': 8, 'min': 2},\n",
    "        'Bn_momentum': {\n",
    "            'values': [0.1, 0.4, 0.7, 0.99]},\n",
    "        'Lr': {\n",
    "            'distribution': 'log_uniform',\n",
    "            'max': log(0.1), 'min': log(0.0001)},\n",
    "        'Lr_decay': {\n",
    "            'distribution': 'log_uniform',\n",
    "            'max': log(1), 'min': log(0.8)},        \n",
    "        'Minibatch_size': {\n",
    "            'value': 4096},\n",
    "        'Min_delta': {\n",
    "            'value': 0.01 if moneyness else 1},\n",
    "        'Patience': {\n",
    "            'value': 20},\n",
    "        'Num_features': {\n",
    "            'value': 3 if moneyness else 4},\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize sweep and creating sweepID\n",
    "\n",
    "# If new sweep, uncomment the line below and comment the line after it\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project='Deep learning for option pricing - test area') \n",
    "#sweep_id = '98bxt6oq'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WIP\n",
    "class MSE_LossCallback(Callback):\n",
    "    def __init__(self, train_x, train_y, train_strike, val_x, val_y, val_strike):\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "        self.train_strike = train_strike\n",
    "        self.val_x = val_x\n",
    "        self.val_y = val_y\n",
    "        self.val_strike = val_strike\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        train_pred = self.model(train_x)\n",
    "        val_pred = self.model(val_x)\n",
    "\n",
    "        train_mse = reduce_mean(square(multiply(train_pred[:,0] - self.train_y, self.train_strike)))\n",
    "        val_mse = reduce_mean(square(multiply(val_pred[:,0] - self.val_y, self.val_strike)))\n",
    "\n",
    "        print(f' Training scaled MSE: {train_mse}, Validation scaled MSE: {val_mse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the training and validation MSE loss on the actual option price when using price/strike as the target\n",
    "def MSE_loss(model, train_x, train_y, train_strike, val_x, val_y, val_strike):\n",
    "    train_pred = model(train_x)\n",
    "    val_pred = model(val_x)\n",
    "\n",
    "    train_mse = reduce_mean(square((train_pred[:,0] - train_y)*train_strike))\n",
    "    val_mse = reduce_mean(square((val_pred[:,0] - val_y)*val_strike))\n",
    "\n",
    "    print(f' Training scaled MSE: {train_mse}, Validation scaled MSE: {val_mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from tensorflow.keras import backend as k\n",
    "\n",
    "class ClearMemory(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        gc.collect()\n",
    "        k.clear_session()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating trainer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(train_x = train_x, train_y = train_y, val_x = val_x, val_y = val_y, config = None, project = None):\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config, project = project):\n",
    "\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "\n",
    "        # Build model and create callbacks\n",
    "        model = create_model(config)\n",
    "\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            min_delta = config.Min_delta,\n",
    "            patience = config.Patience,\n",
    "        )\n",
    "        \n",
    "        wandb_callback = WandbCallback(\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            save_model=False\n",
    "        )\n",
    "\n",
    "        # Checkpoints\n",
    "\n",
    "\n",
    "\n",
    "        # Check if the checkpoint folder exists\n",
    "        if not os.path.exists('./checkpoint/'):\n",
    "            # Create the checkpoint folder if it does not exist\n",
    "            os.makedirs('./checkpoint/')\n",
    "\n",
    "        global time\n",
    "        time = datetime.now()\n",
    "        time = time.strftime(\"%m-%d_%H-%M\")\n",
    "        checkpoint_filepath = f'./checkpoint/{time}'\n",
    "        \n",
    "        checkpoint = ModelCheckpoint(\n",
    "            filepath=checkpoint_filepath,\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True\n",
    "        )\n",
    "\n",
    "        if moneyness:\n",
    "            mse_callback = MSE_LossCallback(train_x, train_y, train_strike, val_x, val_y, val_strike)\n",
    "\n",
    "        # Adapt sequence length to config\n",
    "        train_x_adjusted = [train_x[0][:, :config.LSTM_timesteps, :], train_x[1]]\n",
    "        val_x_adjusted = [val_x[0][:, :config.LSTM_timesteps, :], val_x[1]]\n",
    "        print(f'Train shape: {train_x_adjusted[0].shape}, {train_x_adjusted[0].shape}')\n",
    "        print(f'Val shape: {val_x_adjusted[0].shape}, {val_x_adjusted[0].shape}')\n",
    "\n",
    "        # Train model\n",
    "        model.fit(\n",
    "            train_x_adjusted,\n",
    "            train_y,\n",
    "            batch_size = config.Minibatch_size,\n",
    "            validation_data = (val_x_adjusted, val_y),\n",
    "            epochs = 3,\n",
    "            callbacks = [early_stopping, wandb_callback, mse_callback, checkpoint, ClearMemory()] if moneyness else [early_stopping, wandb_callback, checkpoint, ClearMemory()],\n",
    "        )\n",
    "\n",
    "        if moneyness:\n",
    "            MSE_loss(model, train_x, train_y, train_strike, val_x, val_y, val_strike)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run full sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.agent(sweep_id=sweep_id, function=trainer, project='Deep learning for option pricing - test area', count = 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a11504d5dbe1423e90bef65bd27cd795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016704237283314192, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/hjalmarjacobvinje/Documents/deep-learning-for-option-pricing/LSTM-MLP_notebooks/wandb/run-20230317_152912-80eq2udp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing/runs/80eq2udp\" target=\"_blank\">vibrant-energy-30</a></strong> to <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing/runs/80eq2udp\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing/runs/80eq2udp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 10, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 4)            384         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 8)            0           ['sequential[0][0]',             \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 2)            18          ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 2)           8           ['dense[0][0]']                  \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 2)            0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 2)            6           ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 2)           8           ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 2)            0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 2)            6           ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 2)           8           ['dense_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 2)            0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 2)            6           ['leaky_re_lu_2[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 2)           8           ['dense_3[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)      (None, 2)            0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 2)            6           ['leaky_re_lu_3[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 2)           8           ['dense_4[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)      (None, 2)            0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 2)            6           ['leaky_re_lu_4[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 2)           8           ['dense_5[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)      (None, 2)            0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 1)            3           ['leaky_re_lu_5[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 483\n",
      "Trainable params: 459\n",
      "Non-trainable params: 24\n",
      "__________________________________________________________________________________________________\n",
      "Path exists:  True\n",
      "Train shape: (1479245, 10, 1), (1479245, 10, 1)\n",
      "Val shape: (176483, 10, 1), (176483, 10, 1)\n",
      "Epoch 1/3\n",
      "362/362 [==============================] - 29s 52ms/step - loss: 630194.3750 - val_loss: 721892.0625\n",
      "Epoch 2/3\n",
      "362/362 [==============================] - 21s 51ms/step - loss: 629886.6875 - val_loss: 721996.3125\n",
      "Epoch 3/3\n",
      "362/362 [==============================] - 20s 48ms/step - loss: 629577.6875 - val_loss: 722054.1875\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1bca02cf74047b5997daa5e0963316c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.005 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>loss</td><td>█▅▁</td></tr><tr><td>val_loss</td><td>▁▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>721892.0625</td></tr><tr><td>epoch</td><td>2</td></tr><tr><td>loss</td><td>629577.6875</td></tr><tr><td>val_loss</td><td>722054.1875</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vibrant-energy-30</strong> at: <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing/runs/80eq2udp\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing/runs/80eq2udp</a><br/>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230317_152912-80eq2udp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = {\n",
    "    'LSTM_units': 4,\n",
    "    'MLP_units': 2,\n",
    "    'LSTM_timesteps': 10,\n",
    "    'LSTM_layers': 3,\n",
    "    'MLP_layers': 7,\n",
    "    'Bn_momentum': 0.99,\n",
    "    'Lr': 0.0001,\n",
    "    'Lr_decay': 0.92,\n",
    "    'Minibatch_size': 4096,\n",
    "    'Min_delta': 0.01 if moneyness else 1,\n",
    "    'Patience': 20,\n",
    "    'Num_features': 3 if moneyness else 4, \n",
    "    'Architecture': 'LSTM-MLP v.1.0',\n",
    "    'Dataset': f'Moneyness. Train_start: {train_start}, val_start: {val_start}, test_start: {test_start}',\n",
    "}\n",
    "trainer(config = config, project = 'Deep learning for option pricing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: checkpoint/{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Lowest val_loss\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m c_model \u001b[39m=\u001b[39m load_model(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcheckpoint\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m predictions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(c_model(test_x))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/saved_model/loader_impl.py:115\u001b[0m, in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot parse file \u001b[39m\u001b[39m{\u001b[39;00mpath_to_pbtxt\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(e)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    114\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\n\u001b[1;32m    116\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSavedModel file does not exist at: \u001b[39m\u001b[39m{\u001b[39;00mexport_dir\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00mos\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39msep\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    117\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{{\u001b[39;00m\u001b[39m{\u001b[39;00mconstants\u001b[39m.\u001b[39mSAVED_MODEL_FILENAME_PBTXT\u001b[39m}\u001b[39;00m\u001b[39m|\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    118\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mconstants\u001b[39m.\u001b[39mSAVED_MODEL_FILENAME_PB\u001b[39m}\u001b[39;00m\u001b[39m}}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: checkpoint/{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "# Lowest val_loss\n",
    "c_model = load_model(f\"checkpoint\")\n",
    "predictions = np.array(c_model(test_x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
