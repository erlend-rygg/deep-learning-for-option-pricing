{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights and Biases\n",
    "#!pip install -q wandb\n",
    "# Tensorflow\n",
    "#!pip install -q tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, LSTM, Concatenate, Dense, BatchNormalization, LeakyReLU\n",
    "from keras.activations import tanh\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from tensorflow import square, reduce_mean\n",
    "from tensorflow.keras.losses import MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33merlendrygg\u001b[0m (\u001b[33mavogadro\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Erlend/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import config\n",
    "#Erlend\n",
    "wandb.login(key=config.erlend_key)\n",
    "# Hjalmar\n",
    "#wandb.login(key=config.hjalmar_key)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, split and normalize data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Quote_date</th>\n",
       "      <th>Expire_date</th>\n",
       "      <th>Price</th>\n",
       "      <th>Underlying_last</th>\n",
       "      <th>Strike</th>\n",
       "      <th>TTM</th>\n",
       "      <th>Moneyness</th>\n",
       "      <th>Underlying_return</th>\n",
       "      <th>Underlying_1</th>\n",
       "      <th>...</th>\n",
       "      <th>Underlying_82</th>\n",
       "      <th>Underlying_83</th>\n",
       "      <th>Underlying_84</th>\n",
       "      <th>Underlying_85</th>\n",
       "      <th>Underlying_86</th>\n",
       "      <th>Underlying_87</th>\n",
       "      <th>Underlying_88</th>\n",
       "      <th>Underlying_89</th>\n",
       "      <th>Underlying_90</th>\n",
       "      <th>R</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>704239</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>2021-05-19</td>\n",
       "      <td>3062.000</td>\n",
       "      <td>4163.6</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.785091</td>\n",
       "      <td>-9.88</td>\n",
       "      <td>61.51</td>\n",
       "      <td>...</td>\n",
       "      <td>-27.55</td>\n",
       "      <td>-14.22</td>\n",
       "      <td>9.03</td>\n",
       "      <td>1.14</td>\n",
       "      <td>-24.63</td>\n",
       "      <td>20.88</td>\n",
       "      <td>55.36</td>\n",
       "      <td>21.16</td>\n",
       "      <td>25.67</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>704240</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>2021-05-19</td>\n",
       "      <td>2962.505</td>\n",
       "      <td>4163.6</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.469667</td>\n",
       "      <td>-9.88</td>\n",
       "      <td>61.51</td>\n",
       "      <td>...</td>\n",
       "      <td>-27.55</td>\n",
       "      <td>-14.22</td>\n",
       "      <td>9.03</td>\n",
       "      <td>1.14</td>\n",
       "      <td>-24.63</td>\n",
       "      <td>20.88</td>\n",
       "      <td>55.36</td>\n",
       "      <td>21.16</td>\n",
       "      <td>25.67</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>704241</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>2021-05-19</td>\n",
       "      <td>2862.600</td>\n",
       "      <td>4163.6</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.202769</td>\n",
       "      <td>-9.88</td>\n",
       "      <td>61.51</td>\n",
       "      <td>...</td>\n",
       "      <td>-27.55</td>\n",
       "      <td>-14.22</td>\n",
       "      <td>9.03</td>\n",
       "      <td>1.14</td>\n",
       "      <td>-24.63</td>\n",
       "      <td>20.88</td>\n",
       "      <td>55.36</td>\n",
       "      <td>21.16</td>\n",
       "      <td>25.67</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>704242</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>2021-05-19</td>\n",
       "      <td>2761.600</td>\n",
       "      <td>4163.6</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.974000</td>\n",
       "      <td>-9.88</td>\n",
       "      <td>61.51</td>\n",
       "      <td>...</td>\n",
       "      <td>-27.55</td>\n",
       "      <td>-14.22</td>\n",
       "      <td>9.03</td>\n",
       "      <td>1.14</td>\n",
       "      <td>-24.63</td>\n",
       "      <td>20.88</td>\n",
       "      <td>55.36</td>\n",
       "      <td>21.16</td>\n",
       "      <td>25.67</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>704243</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>2021-05-19</td>\n",
       "      <td>2661.600</td>\n",
       "      <td>4163.6</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.775733</td>\n",
       "      <td>-9.88</td>\n",
       "      <td>61.51</td>\n",
       "      <td>...</td>\n",
       "      <td>-27.55</td>\n",
       "      <td>-14.22</td>\n",
       "      <td>9.03</td>\n",
       "      <td>1.14</td>\n",
       "      <td>-24.63</td>\n",
       "      <td>20.88</td>\n",
       "      <td>55.36</td>\n",
       "      <td>21.16</td>\n",
       "      <td>25.67</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>704244</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>2021-05-19</td>\n",
       "      <td>2561.595</td>\n",
       "      <td>4163.6</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.602250</td>\n",
       "      <td>-9.88</td>\n",
       "      <td>61.51</td>\n",
       "      <td>...</td>\n",
       "      <td>-27.55</td>\n",
       "      <td>-14.22</td>\n",
       "      <td>9.03</td>\n",
       "      <td>1.14</td>\n",
       "      <td>-24.63</td>\n",
       "      <td>20.88</td>\n",
       "      <td>55.36</td>\n",
       "      <td>21.16</td>\n",
       "      <td>25.67</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>704245</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>2021-05-19</td>\n",
       "      <td>2461.600</td>\n",
       "      <td>4163.6</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.449176</td>\n",
       "      <td>-9.88</td>\n",
       "      <td>61.51</td>\n",
       "      <td>...</td>\n",
       "      <td>-27.55</td>\n",
       "      <td>-14.22</td>\n",
       "      <td>9.03</td>\n",
       "      <td>1.14</td>\n",
       "      <td>-24.63</td>\n",
       "      <td>20.88</td>\n",
       "      <td>55.36</td>\n",
       "      <td>21.16</td>\n",
       "      <td>25.67</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>704246</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>2021-05-19</td>\n",
       "      <td>2362.000</td>\n",
       "      <td>4163.6</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.313111</td>\n",
       "      <td>-9.88</td>\n",
       "      <td>61.51</td>\n",
       "      <td>...</td>\n",
       "      <td>-27.55</td>\n",
       "      <td>-14.22</td>\n",
       "      <td>9.03</td>\n",
       "      <td>1.14</td>\n",
       "      <td>-24.63</td>\n",
       "      <td>20.88</td>\n",
       "      <td>55.36</td>\n",
       "      <td>21.16</td>\n",
       "      <td>25.67</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>704247</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>2021-05-19</td>\n",
       "      <td>2261.600</td>\n",
       "      <td>4163.6</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.191368</td>\n",
       "      <td>-9.88</td>\n",
       "      <td>61.51</td>\n",
       "      <td>...</td>\n",
       "      <td>-27.55</td>\n",
       "      <td>-14.22</td>\n",
       "      <td>9.03</td>\n",
       "      <td>1.14</td>\n",
       "      <td>-24.63</td>\n",
       "      <td>20.88</td>\n",
       "      <td>55.36</td>\n",
       "      <td>21.16</td>\n",
       "      <td>25.67</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>704248</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>2021-05-19</td>\n",
       "      <td>2161.595</td>\n",
       "      <td>4163.6</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.081800</td>\n",
       "      <td>-9.88</td>\n",
       "      <td>61.51</td>\n",
       "      <td>...</td>\n",
       "      <td>-27.55</td>\n",
       "      <td>-14.22</td>\n",
       "      <td>9.03</td>\n",
       "      <td>1.14</td>\n",
       "      <td>-24.63</td>\n",
       "      <td>20.88</td>\n",
       "      <td>55.36</td>\n",
       "      <td>21.16</td>\n",
       "      <td>25.67</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>704249</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>2021-05-19</td>\n",
       "      <td>2062.000</td>\n",
       "      <td>4163.6</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.982667</td>\n",
       "      <td>-9.88</td>\n",
       "      <td>61.51</td>\n",
       "      <td>...</td>\n",
       "      <td>-27.55</td>\n",
       "      <td>-14.22</td>\n",
       "      <td>9.03</td>\n",
       "      <td>1.14</td>\n",
       "      <td>-24.63</td>\n",
       "      <td>20.88</td>\n",
       "      <td>55.36</td>\n",
       "      <td>21.16</td>\n",
       "      <td>25.67</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>704250</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>2021-05-19</td>\n",
       "      <td>1962.000</td>\n",
       "      <td>4163.6</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.892545</td>\n",
       "      <td>-9.88</td>\n",
       "      <td>61.51</td>\n",
       "      <td>...</td>\n",
       "      <td>-27.55</td>\n",
       "      <td>-14.22</td>\n",
       "      <td>9.03</td>\n",
       "      <td>1.14</td>\n",
       "      <td>-24.63</td>\n",
       "      <td>20.88</td>\n",
       "      <td>55.36</td>\n",
       "      <td>21.16</td>\n",
       "      <td>25.67</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>704251</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>2021-05-19</td>\n",
       "      <td>1861.605</td>\n",
       "      <td>4163.6</td>\n",
       "      <td>2300.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.810261</td>\n",
       "      <td>-9.88</td>\n",
       "      <td>61.51</td>\n",
       "      <td>...</td>\n",
       "      <td>-27.55</td>\n",
       "      <td>-14.22</td>\n",
       "      <td>9.03</td>\n",
       "      <td>1.14</td>\n",
       "      <td>-24.63</td>\n",
       "      <td>20.88</td>\n",
       "      <td>55.36</td>\n",
       "      <td>21.16</td>\n",
       "      <td>25.67</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>704252</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>2021-05-19</td>\n",
       "      <td>1762.010</td>\n",
       "      <td>4163.6</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.734833</td>\n",
       "      <td>-9.88</td>\n",
       "      <td>61.51</td>\n",
       "      <td>...</td>\n",
       "      <td>-27.55</td>\n",
       "      <td>-14.22</td>\n",
       "      <td>9.03</td>\n",
       "      <td>1.14</td>\n",
       "      <td>-24.63</td>\n",
       "      <td>20.88</td>\n",
       "      <td>55.36</td>\n",
       "      <td>21.16</td>\n",
       "      <td>25.67</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>704253</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>2021-05-19</td>\n",
       "      <td>1662.060</td>\n",
       "      <td>4163.6</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.665440</td>\n",
       "      <td>-9.88</td>\n",
       "      <td>61.51</td>\n",
       "      <td>...</td>\n",
       "      <td>-27.55</td>\n",
       "      <td>-14.22</td>\n",
       "      <td>9.03</td>\n",
       "      <td>1.14</td>\n",
       "      <td>-24.63</td>\n",
       "      <td>20.88</td>\n",
       "      <td>55.36</td>\n",
       "      <td>21.16</td>\n",
       "      <td>25.67</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>704254</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>2021-05-19</td>\n",
       "      <td>1562.645</td>\n",
       "      <td>4163.6</td>\n",
       "      <td>2600.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.601385</td>\n",
       "      <td>-9.88</td>\n",
       "      <td>61.51</td>\n",
       "      <td>...</td>\n",
       "      <td>-27.55</td>\n",
       "      <td>-14.22</td>\n",
       "      <td>9.03</td>\n",
       "      <td>1.14</td>\n",
       "      <td>-24.63</td>\n",
       "      <td>20.88</td>\n",
       "      <td>55.36</td>\n",
       "      <td>21.16</td>\n",
       "      <td>25.67</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>704255</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>2021-05-19</td>\n",
       "      <td>1462.650</td>\n",
       "      <td>4163.6</td>\n",
       "      <td>2700.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.542074</td>\n",
       "      <td>-9.88</td>\n",
       "      <td>61.51</td>\n",
       "      <td>...</td>\n",
       "      <td>-27.55</td>\n",
       "      <td>-14.22</td>\n",
       "      <td>9.03</td>\n",
       "      <td>1.14</td>\n",
       "      <td>-24.63</td>\n",
       "      <td>20.88</td>\n",
       "      <td>55.36</td>\n",
       "      <td>21.16</td>\n",
       "      <td>25.67</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>704256</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>2021-05-19</td>\n",
       "      <td>1412.650</td>\n",
       "      <td>4163.6</td>\n",
       "      <td>2750.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.514036</td>\n",
       "      <td>-9.88</td>\n",
       "      <td>61.51</td>\n",
       "      <td>...</td>\n",
       "      <td>-27.55</td>\n",
       "      <td>-14.22</td>\n",
       "      <td>9.03</td>\n",
       "      <td>1.14</td>\n",
       "      <td>-24.63</td>\n",
       "      <td>20.88</td>\n",
       "      <td>55.36</td>\n",
       "      <td>21.16</td>\n",
       "      <td>25.67</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>704257</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>2021-05-19</td>\n",
       "      <td>1362.095</td>\n",
       "      <td>4163.6</td>\n",
       "      <td>2800.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.487000</td>\n",
       "      <td>-9.88</td>\n",
       "      <td>61.51</td>\n",
       "      <td>...</td>\n",
       "      <td>-27.55</td>\n",
       "      <td>-14.22</td>\n",
       "      <td>9.03</td>\n",
       "      <td>1.14</td>\n",
       "      <td>-24.63</td>\n",
       "      <td>20.88</td>\n",
       "      <td>55.36</td>\n",
       "      <td>21.16</td>\n",
       "      <td>25.67</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>704258</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>2021-05-19</td>\n",
       "      <td>1312.100</td>\n",
       "      <td>4163.6</td>\n",
       "      <td>2850.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.460912</td>\n",
       "      <td>-9.88</td>\n",
       "      <td>61.51</td>\n",
       "      <td>...</td>\n",
       "      <td>-27.55</td>\n",
       "      <td>-14.22</td>\n",
       "      <td>9.03</td>\n",
       "      <td>1.14</td>\n",
       "      <td>-24.63</td>\n",
       "      <td>20.88</td>\n",
       "      <td>55.36</td>\n",
       "      <td>21.16</td>\n",
       "      <td>25.67</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>704259</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>2021-05-19</td>\n",
       "      <td>1262.650</td>\n",
       "      <td>4163.6</td>\n",
       "      <td>2900.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.435724</td>\n",
       "      <td>-9.88</td>\n",
       "      <td>61.51</td>\n",
       "      <td>...</td>\n",
       "      <td>-27.55</td>\n",
       "      <td>-14.22</td>\n",
       "      <td>9.03</td>\n",
       "      <td>1.14</td>\n",
       "      <td>-24.63</td>\n",
       "      <td>20.88</td>\n",
       "      <td>55.36</td>\n",
       "      <td>21.16</td>\n",
       "      <td>25.67</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>704260</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>2021-05-19</td>\n",
       "      <td>1212.150</td>\n",
       "      <td>4163.6</td>\n",
       "      <td>2950.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.411390</td>\n",
       "      <td>-9.88</td>\n",
       "      <td>61.51</td>\n",
       "      <td>...</td>\n",
       "      <td>-27.55</td>\n",
       "      <td>-14.22</td>\n",
       "      <td>9.03</td>\n",
       "      <td>1.14</td>\n",
       "      <td>-24.63</td>\n",
       "      <td>20.88</td>\n",
       "      <td>55.36</td>\n",
       "      <td>21.16</td>\n",
       "      <td>25.67</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>704261</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>2021-05-19</td>\n",
       "      <td>1162.650</td>\n",
       "      <td>4163.6</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.387867</td>\n",
       "      <td>-9.88</td>\n",
       "      <td>61.51</td>\n",
       "      <td>...</td>\n",
       "      <td>-27.55</td>\n",
       "      <td>-14.22</td>\n",
       "      <td>9.03</td>\n",
       "      <td>1.14</td>\n",
       "      <td>-24.63</td>\n",
       "      <td>20.88</td>\n",
       "      <td>55.36</td>\n",
       "      <td>21.16</td>\n",
       "      <td>25.67</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>704262</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>2021-05-19</td>\n",
       "      <td>1112.650</td>\n",
       "      <td>4163.6</td>\n",
       "      <td>3050.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.365115</td>\n",
       "      <td>-9.88</td>\n",
       "      <td>61.51</td>\n",
       "      <td>...</td>\n",
       "      <td>-27.55</td>\n",
       "      <td>-14.22</td>\n",
       "      <td>9.03</td>\n",
       "      <td>1.14</td>\n",
       "      <td>-24.63</td>\n",
       "      <td>20.88</td>\n",
       "      <td>55.36</td>\n",
       "      <td>21.16</td>\n",
       "      <td>25.67</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>704263</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>2021-05-19</td>\n",
       "      <td>1062.650</td>\n",
       "      <td>4163.6</td>\n",
       "      <td>3100.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.343097</td>\n",
       "      <td>-9.88</td>\n",
       "      <td>61.51</td>\n",
       "      <td>...</td>\n",
       "      <td>-27.55</td>\n",
       "      <td>-14.22</td>\n",
       "      <td>9.03</td>\n",
       "      <td>1.14</td>\n",
       "      <td>-24.63</td>\n",
       "      <td>20.88</td>\n",
       "      <td>55.36</td>\n",
       "      <td>21.16</td>\n",
       "      <td>25.67</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>704264</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>2021-05-19</td>\n",
       "      <td>1012.650</td>\n",
       "      <td>4163.6</td>\n",
       "      <td>3150.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.321778</td>\n",
       "      <td>-9.88</td>\n",
       "      <td>61.51</td>\n",
       "      <td>...</td>\n",
       "      <td>-27.55</td>\n",
       "      <td>-14.22</td>\n",
       "      <td>9.03</td>\n",
       "      <td>1.14</td>\n",
       "      <td>-24.63</td>\n",
       "      <td>20.88</td>\n",
       "      <td>55.36</td>\n",
       "      <td>21.16</td>\n",
       "      <td>25.67</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>704265</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>2021-05-19</td>\n",
       "      <td>962.705</td>\n",
       "      <td>4163.6</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.301125</td>\n",
       "      <td>-9.88</td>\n",
       "      <td>61.51</td>\n",
       "      <td>...</td>\n",
       "      <td>-27.55</td>\n",
       "      <td>-14.22</td>\n",
       "      <td>9.03</td>\n",
       "      <td>1.14</td>\n",
       "      <td>-24.63</td>\n",
       "      <td>20.88</td>\n",
       "      <td>55.36</td>\n",
       "      <td>21.16</td>\n",
       "      <td>25.67</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>704266</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>2021-05-19</td>\n",
       "      <td>911.750</td>\n",
       "      <td>4163.6</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.281108</td>\n",
       "      <td>-9.88</td>\n",
       "      <td>61.51</td>\n",
       "      <td>...</td>\n",
       "      <td>-27.55</td>\n",
       "      <td>-14.22</td>\n",
       "      <td>9.03</td>\n",
       "      <td>1.14</td>\n",
       "      <td>-24.63</td>\n",
       "      <td>20.88</td>\n",
       "      <td>55.36</td>\n",
       "      <td>21.16</td>\n",
       "      <td>25.67</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>704267</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>2021-05-19</td>\n",
       "      <td>861.755</td>\n",
       "      <td>4163.6</td>\n",
       "      <td>3300.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.261697</td>\n",
       "      <td>-9.88</td>\n",
       "      <td>61.51</td>\n",
       "      <td>...</td>\n",
       "      <td>-27.55</td>\n",
       "      <td>-14.22</td>\n",
       "      <td>9.03</td>\n",
       "      <td>1.14</td>\n",
       "      <td>-24.63</td>\n",
       "      <td>20.88</td>\n",
       "      <td>55.36</td>\n",
       "      <td>21.16</td>\n",
       "      <td>25.67</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>704268</td>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>2021-05-19</td>\n",
       "      <td>811.750</td>\n",
       "      <td>4163.6</td>\n",
       "      <td>3350.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.242866</td>\n",
       "      <td>-9.88</td>\n",
       "      <td>61.51</td>\n",
       "      <td>...</td>\n",
       "      <td>-27.55</td>\n",
       "      <td>-14.22</td>\n",
       "      <td>9.03</td>\n",
       "      <td>1.14</td>\n",
       "      <td>-24.63</td>\n",
       "      <td>20.88</td>\n",
       "      <td>55.36</td>\n",
       "      <td>21.16</td>\n",
       "      <td>25.67</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  Quote_date Expire_date     Price  Underlying_last  Strike  \\\n",
       "0       704239  2021-05-17  2021-05-19  3062.000           4163.6  1100.0   \n",
       "1       704240  2021-05-17  2021-05-19  2962.505           4163.6  1200.0   \n",
       "2       704241  2021-05-17  2021-05-19  2862.600           4163.6  1300.0   \n",
       "3       704242  2021-05-17  2021-05-19  2761.600           4163.6  1400.0   \n",
       "4       704243  2021-05-17  2021-05-19  2661.600           4163.6  1500.0   \n",
       "5       704244  2021-05-17  2021-05-19  2561.595           4163.6  1600.0   \n",
       "6       704245  2021-05-17  2021-05-19  2461.600           4163.6  1700.0   \n",
       "7       704246  2021-05-17  2021-05-19  2362.000           4163.6  1800.0   \n",
       "8       704247  2021-05-17  2021-05-19  2261.600           4163.6  1900.0   \n",
       "9       704248  2021-05-17  2021-05-19  2161.595           4163.6  2000.0   \n",
       "10      704249  2021-05-17  2021-05-19  2062.000           4163.6  2100.0   \n",
       "11      704250  2021-05-17  2021-05-19  1962.000           4163.6  2200.0   \n",
       "12      704251  2021-05-17  2021-05-19  1861.605           4163.6  2300.0   \n",
       "13      704252  2021-05-17  2021-05-19  1762.010           4163.6  2400.0   \n",
       "14      704253  2021-05-17  2021-05-19  1662.060           4163.6  2500.0   \n",
       "15      704254  2021-05-17  2021-05-19  1562.645           4163.6  2600.0   \n",
       "16      704255  2021-05-17  2021-05-19  1462.650           4163.6  2700.0   \n",
       "17      704256  2021-05-17  2021-05-19  1412.650           4163.6  2750.0   \n",
       "18      704257  2021-05-17  2021-05-19  1362.095           4163.6  2800.0   \n",
       "19      704258  2021-05-17  2021-05-19  1312.100           4163.6  2850.0   \n",
       "20      704259  2021-05-17  2021-05-19  1262.650           4163.6  2900.0   \n",
       "21      704260  2021-05-17  2021-05-19  1212.150           4163.6  2950.0   \n",
       "22      704261  2021-05-17  2021-05-19  1162.650           4163.6  3000.0   \n",
       "23      704262  2021-05-17  2021-05-19  1112.650           4163.6  3050.0   \n",
       "24      704263  2021-05-17  2021-05-19  1062.650           4163.6  3100.0   \n",
       "25      704264  2021-05-17  2021-05-19  1012.650           4163.6  3150.0   \n",
       "26      704265  2021-05-17  2021-05-19   962.705           4163.6  3200.0   \n",
       "27      704266  2021-05-17  2021-05-19   911.750           4163.6  3250.0   \n",
       "28      704267  2021-05-17  2021-05-19   861.755           4163.6  3300.0   \n",
       "29      704268  2021-05-17  2021-05-19   811.750           4163.6  3350.0   \n",
       "\n",
       "    TTM  Moneyness  Underlying_return  Underlying_1  ...  Underlying_82  \\\n",
       "0     2   3.785091              -9.88         61.51  ...         -27.55   \n",
       "1     2   3.469667              -9.88         61.51  ...         -27.55   \n",
       "2     2   3.202769              -9.88         61.51  ...         -27.55   \n",
       "3     2   2.974000              -9.88         61.51  ...         -27.55   \n",
       "4     2   2.775733              -9.88         61.51  ...         -27.55   \n",
       "5     2   2.602250              -9.88         61.51  ...         -27.55   \n",
       "6     2   2.449176              -9.88         61.51  ...         -27.55   \n",
       "7     2   2.313111              -9.88         61.51  ...         -27.55   \n",
       "8     2   2.191368              -9.88         61.51  ...         -27.55   \n",
       "9     2   2.081800              -9.88         61.51  ...         -27.55   \n",
       "10    2   1.982667              -9.88         61.51  ...         -27.55   \n",
       "11    2   1.892545              -9.88         61.51  ...         -27.55   \n",
       "12    2   1.810261              -9.88         61.51  ...         -27.55   \n",
       "13    2   1.734833              -9.88         61.51  ...         -27.55   \n",
       "14    2   1.665440              -9.88         61.51  ...         -27.55   \n",
       "15    2   1.601385              -9.88         61.51  ...         -27.55   \n",
       "16    2   1.542074              -9.88         61.51  ...         -27.55   \n",
       "17    2   1.514036              -9.88         61.51  ...         -27.55   \n",
       "18    2   1.487000              -9.88         61.51  ...         -27.55   \n",
       "19    2   1.460912              -9.88         61.51  ...         -27.55   \n",
       "20    2   1.435724              -9.88         61.51  ...         -27.55   \n",
       "21    2   1.411390              -9.88         61.51  ...         -27.55   \n",
       "22    2   1.387867              -9.88         61.51  ...         -27.55   \n",
       "23    2   1.365115              -9.88         61.51  ...         -27.55   \n",
       "24    2   1.343097              -9.88         61.51  ...         -27.55   \n",
       "25    2   1.321778              -9.88         61.51  ...         -27.55   \n",
       "26    2   1.301125              -9.88         61.51  ...         -27.55   \n",
       "27    2   1.281108              -9.88         61.51  ...         -27.55   \n",
       "28    2   1.261697              -9.88         61.51  ...         -27.55   \n",
       "29    2   1.242866              -9.88         61.51  ...         -27.55   \n",
       "\n",
       "    Underlying_83  Underlying_84  Underlying_85  Underlying_86  Underlying_87  \\\n",
       "0          -14.22           9.03           1.14         -24.63          20.88   \n",
       "1          -14.22           9.03           1.14         -24.63          20.88   \n",
       "2          -14.22           9.03           1.14         -24.63          20.88   \n",
       "3          -14.22           9.03           1.14         -24.63          20.88   \n",
       "4          -14.22           9.03           1.14         -24.63          20.88   \n",
       "5          -14.22           9.03           1.14         -24.63          20.88   \n",
       "6          -14.22           9.03           1.14         -24.63          20.88   \n",
       "7          -14.22           9.03           1.14         -24.63          20.88   \n",
       "8          -14.22           9.03           1.14         -24.63          20.88   \n",
       "9          -14.22           9.03           1.14         -24.63          20.88   \n",
       "10         -14.22           9.03           1.14         -24.63          20.88   \n",
       "11         -14.22           9.03           1.14         -24.63          20.88   \n",
       "12         -14.22           9.03           1.14         -24.63          20.88   \n",
       "13         -14.22           9.03           1.14         -24.63          20.88   \n",
       "14         -14.22           9.03           1.14         -24.63          20.88   \n",
       "15         -14.22           9.03           1.14         -24.63          20.88   \n",
       "16         -14.22           9.03           1.14         -24.63          20.88   \n",
       "17         -14.22           9.03           1.14         -24.63          20.88   \n",
       "18         -14.22           9.03           1.14         -24.63          20.88   \n",
       "19         -14.22           9.03           1.14         -24.63          20.88   \n",
       "20         -14.22           9.03           1.14         -24.63          20.88   \n",
       "21         -14.22           9.03           1.14         -24.63          20.88   \n",
       "22         -14.22           9.03           1.14         -24.63          20.88   \n",
       "23         -14.22           9.03           1.14         -24.63          20.88   \n",
       "24         -14.22           9.03           1.14         -24.63          20.88   \n",
       "25         -14.22           9.03           1.14         -24.63          20.88   \n",
       "26         -14.22           9.03           1.14         -24.63          20.88   \n",
       "27         -14.22           9.03           1.14         -24.63          20.88   \n",
       "28         -14.22           9.03           1.14         -24.63          20.88   \n",
       "29         -14.22           9.03           1.14         -24.63          20.88   \n",
       "\n",
       "    Underlying_88  Underlying_89  Underlying_90    R  \n",
       "0           55.36          21.16          25.67  0.0  \n",
       "1           55.36          21.16          25.67  0.0  \n",
       "2           55.36          21.16          25.67  0.0  \n",
       "3           55.36          21.16          25.67  0.0  \n",
       "4           55.36          21.16          25.67  0.0  \n",
       "5           55.36          21.16          25.67  0.0  \n",
       "6           55.36          21.16          25.67  0.0  \n",
       "7           55.36          21.16          25.67  0.0  \n",
       "8           55.36          21.16          25.67  0.0  \n",
       "9           55.36          21.16          25.67  0.0  \n",
       "10          55.36          21.16          25.67  0.0  \n",
       "11          55.36          21.16          25.67  0.0  \n",
       "12          55.36          21.16          25.67  0.0  \n",
       "13          55.36          21.16          25.67  0.0  \n",
       "14          55.36          21.16          25.67  0.0  \n",
       "15          55.36          21.16          25.67  0.0  \n",
       "16          55.36          21.16          25.67  0.0  \n",
       "17          55.36          21.16          25.67  0.0  \n",
       "18          55.36          21.16          25.67  0.0  \n",
       "19          55.36          21.16          25.67  0.0  \n",
       "20          55.36          21.16          25.67  0.0  \n",
       "21          55.36          21.16          25.67  0.0  \n",
       "22          55.36          21.16          25.67  0.0  \n",
       "23          55.36          21.16          25.67  0.0  \n",
       "24          55.36          21.16          25.67  0.0  \n",
       "25          55.36          21.16          25.67  0.0  \n",
       "26          55.36          21.16          25.67  0.0  \n",
       "27          55.36          21.16          25.67  0.0  \n",
       "28          55.36          21.16          25.67  0.0  \n",
       "29          55.36          21.16          25.67  0.0  \n",
       "\n",
       "[30 rows x 100 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file = \"../data/processed_data/2021_2022.csv\"\n",
    "df_read = pd.read_csv(file)\n",
    "display(df_read)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1119088, 90, 1), (1119088, 4)\n",
      "Val shape: (202710, 90, 1), (202710, 4)\n"
     ]
    }
   ],
   "source": [
    "df = df_read\n",
    "\n",
    "# Format settings\n",
    "max_timesteps = 90\n",
    "moneyness = False # Moneyness = True WIP\n",
    "bs_vars = ['Moneyness', 'TTM', 'R'] if moneyness else ['Underlying_last', 'Strike', 'TTM', 'R']\n",
    "underlying_lags = ['Underlying_last'] + [f'Underlying_{i}' for i in range (1, max_timesteps)]\n",
    "\n",
    "# Create train, validation and test set split points\n",
    "train_start = datetime(2021,1,1)\n",
    "val_start = train_start + relativedelta(months=11)\n",
    "test_start = val_start + relativedelta(months=1)\n",
    "test_end = test_start + relativedelta(months=1)\n",
    "train_start = str(train_start.date())\n",
    "val_start = str(val_start.date())\n",
    "test_start = str(test_start.date())\n",
    "test_end = str(test_end.date())\n",
    "\n",
    "# Split train and validation data\n",
    "df_train = df[(df['Quote_date'] >= train_start) & ( df['Quote_date'] < val_start)]\n",
    "df_val = df[(df['Quote_date'] >= val_start) & ( df['Quote_date'] < test_start)]\n",
    "\n",
    "# Extract target values\n",
    "train_y = df_train['Price'].to_numpy()\n",
    "val_y = df_val['Price'].to_numpy()\n",
    "\n",
    "# Convert dataframes to numpy arrays\n",
    "train_x = [df_train[underlying_lags].to_numpy(), df_train[bs_vars].to_numpy()]\n",
    "val_x = [df_val[underlying_lags].to_numpy(), df_val[bs_vars].to_numpy()]\n",
    "\n",
    "# Scale features based on training set\n",
    "underlying_scaler = MinMaxScaler()\n",
    "train_x[0] = underlying_scaler.fit_transform(train_x[0])\n",
    "val_x[0] = underlying_scaler.transform(val_x[0])\n",
    "\n",
    "bs_scaler = MinMaxScaler()\n",
    "train_x[1] = bs_scaler.fit_transform(train_x[1])\n",
    "val_x[1] = bs_scaler.transform(val_x[1])\n",
    "\n",
    "# Shuffle training set\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(len(train_x[0]))\n",
    "train_x = [train_x[0][shuffle], train_x[1][shuffle]]\n",
    "train_y = train_y[shuffle]\n",
    "\n",
    "# Reshape data to fit LSTM\n",
    "train_x = [train_x[0].reshape(len(train_x[0]), max_timesteps,1), train_x[1]]\n",
    "val_x = [val_x[0].reshape(len(val_x[0]), max_timesteps, 1), val_x[1]]\n",
    "\n",
    "print(f'Train shape: {train_x[0].shape}, {train_x[1].shape}')\n",
    "print(f'Val shape: {val_x[0].shape}, {val_x[1].shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIP\n",
    "def moneyness_mse(y_true, y_pred):\n",
    "    '''Moneyness MSE loss function'''\n",
    "    return MSE(y_true[:,0], y_pred)\n",
    "\n",
    "# WIP\n",
    "def scaled_moneyness_mse(y_true, y_pred):\n",
    "    return MSE(y_true[:,0]*y_true[:,1], y_pred*y_true[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(config):\n",
    "    '''Builds an LSTM-MLP model of minimum 2 layers sequentially from a given config dictionary'''\n",
    "\n",
    "    # Input layers\n",
    "    underlying_history = Input((config.LSTM_timesteps,1))\n",
    "    bs_vars = Input((config.Num_features,))\n",
    "\n",
    "    # LSTM layers\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(\n",
    "        units = config.LSTM_units,\n",
    "        activation = tanh,\n",
    "        input_shape = (config.LSTM_timesteps, 1),\n",
    "        return_sequences = True\n",
    "    ))\n",
    "\n",
    "    for _ in range(config.LSTM_layers - 2):\n",
    "        model.add(LSTM(\n",
    "            units = config.LSTM_units,\n",
    "            activation = tanh,\n",
    "            return_sequences = True\n",
    "        ))\n",
    "    \n",
    "    model.add(LSTM(\n",
    "        units = config.LSTM_units,\n",
    "        activation = tanh,\n",
    "        return_sequences = False\n",
    "    ))\n",
    "\n",
    "    # MLP layers\n",
    "    layers = Concatenate()([model(underlying_history), bs_vars])\n",
    "    \n",
    "    for _ in range(config.MLP_layers - 1):\n",
    "        layers = Dense(config.MLP_units)(layers)\n",
    "        layers = BatchNormalization(momentum=config.Bn_momentum)(layers)\n",
    "        layers = LeakyReLU()(layers)\n",
    "\n",
    "    output = Dense(1, activation='relu')(layers)\n",
    "\n",
    "    # Exponential decaying learning rate\n",
    "    lr_schedule = ExponentialDecay(\n",
    "        initial_learning_rate = config.Lr,\n",
    "        decay_steps = int(len(train_x[0])/config.Minibatch_size),\n",
    "        decay_rate=config.Lr_decay\n",
    "    )\n",
    "\n",
    "    # Compile model\n",
    "    model = Model(inputs=[underlying_history, bs_vars], outputs=output)\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=lr_schedule))\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter search setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: ybzdubg7\n",
      "Sweep URL: https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/sweeps/ybzdubg7\n"
     ]
    }
   ],
   "source": [
    "# Configuring the sweep hyperparameter search space\n",
    "sweep_configuration = {\n",
    "    'method': 'random',\n",
    "    'name': 'LSTM-MLP v.1.0 testing : 21-22 data',\n",
    "    'metric': {\n",
    "        'goal': 'minimize', \n",
    "        'name': 'val_loss'\n",
    "\t\t},\n",
    "    'parameters': {\n",
    "        'LSTM_units': {\n",
    "            'values': [8, 16, 32, 64, 96, 128]},\n",
    "        'MLP_units': {\n",
    "            'values': [32, 64, 96, 128]},\n",
    "        'LSTM_timesteps': {\n",
    "            'values': [20]},\n",
    "        'LSTM_layers': {\n",
    "            'distribution': 'int_uniform',\n",
    "            'max': 6, 'min': 2},\n",
    "        'MLP_layers': {\n",
    "            'distribution': 'int_uniform',\n",
    "            'max': 6, 'min': 2},\n",
    "        'Bn_momentum': {\n",
    "            'distribution': 'uniform',\n",
    "            'max': 1, 'min': 0},\n",
    "        'Lr': {\n",
    "            'distribution': 'uniform',\n",
    "            'max': 0.01, 'min': 0.0001},\n",
    "        'Lr_decay': {\n",
    "            'distribution': 'uniform',\n",
    "            'max': 1, 'min': 0.9},        \n",
    "        'Minibatch_size': {\n",
    "            'values': [1024, 2048, 4096]},\n",
    "        'Min_delta': {\n",
    "            'value': 1},\n",
    "        'Patience': {\n",
    "            'value': 20},\n",
    "        'Num_features': {\n",
    "            'value': 4},\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize sweep and creating sweepID\n",
    "\n",
    "# If new sweep, uncomment the line below and comment the line after it\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project='Deep learning for option pricing - test area') \n",
    "#sweep_id = '98bxt6oq'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(train_x = train_x, train_y = train_y, val_x = val_x, val_y = val_y, config = None, project = None):\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config, project = project):\n",
    "\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "\n",
    "        # Build model and create callbacks\n",
    "        model = create_model(config)\n",
    "\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            min_delta = config.Min_delta,\n",
    "            patience = config.Patience,\n",
    "        )\n",
    "        \n",
    "        wandb_callback = WandbCallback(\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            save_model=False\n",
    "        )\n",
    "\n",
    "        # Adapt sequence length to config\n",
    "        print(config.LSTM_timesteps)\n",
    "        print(train_x[0].shape, train_x[1].shape)\n",
    "        train_x[0] = train_x[0][:, :config.LSTM_timesteps, :]\n",
    "        val_x[0] = val_x[0][:, :config.LSTM_timesteps, :]\n",
    "        print(f'Train shape: {train_x[0].shape}, {train_x[1].shape}')\n",
    "        print(f'Val shape: {val_x[0].shape}, {val_x[1].shape}')\n",
    "\n",
    "        # Train model\n",
    "        model.fit(\n",
    "            train_x,\n",
    "            train_y,\n",
    "            batch_size = config.Minibatch_size,\n",
    "            validation_data = (val_x, val_y),\n",
    "            epochs = 1000,\n",
    "            callbacks = [early_stopping, wandb_callback] \n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run full sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: x1nwz702 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBn_momentum: 0.2115311974369981\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_layers: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_timesteps: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_units: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLr: 0.007252310655150062\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLr_decay: 0.931385828545152\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tMLP_layers: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tMLP_units: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tMin_delta: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tMinibatch_size: 1024\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tNum_features: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tPatience: 20\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id=sweep_id, function=trainer, project='Deep learning for option pricing - test area', count = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.13.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Erlend\\Google Drive\\NTNU\\5. Klasse\\Master\\Git-folder\\deep-learning-for-option-pricing\\LSTM-MLP_notebooks\\wandb\\run-20230209_105833-1ire21u2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/1ire21u2\" target=\"_blank\">blooming-voice-128</a></strong> to <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/sweeps/ybzdubg7\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/sweeps/ybzdubg7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/x1nwz702\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/x1nwz702</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 90, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " sequential_1 (Sequential)      (None, 8)            1408        ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 12)           0           ['sequential_1[0][0]',           \n",
      "                                                                  'input_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 100)          1300        ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 100)         400         ['dense_4[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)      (None, 100)          0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 100)          10100       ['leaky_re_lu_3[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 100)         400         ['dense_5[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)      (None, 100)          0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 100)          10100       ['leaky_re_lu_4[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 100)         400         ['dense_6[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)      (None, 100)          0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 1)            101         ['leaky_re_lu_5[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 128)         512         ['dense_3[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)      (None, 128)          0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 1)            129         ['leaky_re_lu_3[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 176,513\n",
      "Trainable params: 175,489\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "90\n",
      "(1119088, 90, 1) (1119088, 4)\n",
      "Train shape: (1119088, 90, 1), (1119088, 4)\n",
      "Val shape: (202710, 90, 1), (202710, 4)\n",
      "Epoch 1/1000\n",
      "195/274 [====================>.........] - ETA: 2:50 - loss: 690797.3125"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'LSTM_units': 8,\n",
    "    'MLP_units': 100,\n",
    "    'LSTM_timesteps': 90,\n",
    "    'LSTM_layers': 3,\n",
    "    'MLP_layers': 4,\n",
    "    'Bn_momentum': 0.99,\n",
    "    'Lr': 0.01,\n",
    "    'Lr_decay': 1,\n",
    "    'Minibatch_size': 4096,\n",
    "    'Min_delta': 1,\n",
    "    'Patience': 20,\n",
    "    'Num_features': 3 if moneyness else 4, \n",
    "    'Architecture': 'LSTM-MLP v.0.1',\n",
    "    'Dataset': f'Moneyness. Train_start: {train_start}, val_start: {val_start}, {test_start}: {test_start}',\n",
    "}\n",
    "trainer(config = config, project = 'Deep learning for option pricing - test area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
