{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Weights and Biases\n",
    "!pip install -q wandb\n",
    "# Tensorflow\n",
    "!pip install -q tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-01 21:00:20.847008: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, LSTM, Concatenate, Dense, BatchNormalization, LeakyReLU\n",
    "from keras.activations import tanh\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/hjalmarjacobvinje/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.login(key='3cae81eb56be3190be5bb48c571e69933071df69')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Quote_date</th>\n",
       "      <th>Expire_date</th>\n",
       "      <th>Price</th>\n",
       "      <th>Underlying_last</th>\n",
       "      <th>Strike</th>\n",
       "      <th>TTM</th>\n",
       "      <th>Underlying_1</th>\n",
       "      <th>Underlying_2</th>\n",
       "      <th>Underlying_3</th>\n",
       "      <th>Underlying_4</th>\n",
       "      <th>Underlying_5</th>\n",
       "      <th>Underlying_6</th>\n",
       "      <th>Underlying_7</th>\n",
       "      <th>Underlying_8</th>\n",
       "      <th>Underlying_9</th>\n",
       "      <th>Underlying_10</th>\n",
       "      <th>R</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>97145</td>\n",
       "      <td>2022-01-18</td>\n",
       "      <td>2022-01-19</td>\n",
       "      <td>3378.25</td>\n",
       "      <td>4578.88</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4662.85</td>\n",
       "      <td>4662.21</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>97146</td>\n",
       "      <td>2022-01-18</td>\n",
       "      <td>2022-01-19</td>\n",
       "      <td>3178.25</td>\n",
       "      <td>4578.88</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4662.85</td>\n",
       "      <td>4662.21</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>97147</td>\n",
       "      <td>2022-01-18</td>\n",
       "      <td>2022-01-19</td>\n",
       "      <td>2978.25</td>\n",
       "      <td>4578.88</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4662.85</td>\n",
       "      <td>4662.21</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>97148</td>\n",
       "      <td>2022-01-18</td>\n",
       "      <td>2022-01-19</td>\n",
       "      <td>2778.25</td>\n",
       "      <td>4578.88</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4662.85</td>\n",
       "      <td>4662.21</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97149</td>\n",
       "      <td>2022-01-18</td>\n",
       "      <td>2022-01-19</td>\n",
       "      <td>2678.25</td>\n",
       "      <td>4578.88</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4662.85</td>\n",
       "      <td>4662.21</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>97150</td>\n",
       "      <td>2022-01-18</td>\n",
       "      <td>2022-01-19</td>\n",
       "      <td>2578.25</td>\n",
       "      <td>4578.88</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4662.85</td>\n",
       "      <td>4662.21</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>97151</td>\n",
       "      <td>2022-01-18</td>\n",
       "      <td>2022-01-19</td>\n",
       "      <td>2478.25</td>\n",
       "      <td>4578.88</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4662.85</td>\n",
       "      <td>4662.21</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>97152</td>\n",
       "      <td>2022-01-18</td>\n",
       "      <td>2022-01-19</td>\n",
       "      <td>2378.25</td>\n",
       "      <td>4578.88</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4662.85</td>\n",
       "      <td>4662.21</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>97153</td>\n",
       "      <td>2022-01-18</td>\n",
       "      <td>2022-01-19</td>\n",
       "      <td>2278.25</td>\n",
       "      <td>4578.88</td>\n",
       "      <td>2300.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4662.85</td>\n",
       "      <td>4662.21</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>97154</td>\n",
       "      <td>2022-01-18</td>\n",
       "      <td>2022-01-19</td>\n",
       "      <td>2178.25</td>\n",
       "      <td>4578.88</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4662.85</td>\n",
       "      <td>4662.21</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>97155</td>\n",
       "      <td>2022-01-18</td>\n",
       "      <td>2022-01-19</td>\n",
       "      <td>2078.25</td>\n",
       "      <td>4578.88</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4662.85</td>\n",
       "      <td>4662.21</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>97156</td>\n",
       "      <td>2022-01-18</td>\n",
       "      <td>2022-01-19</td>\n",
       "      <td>1978.25</td>\n",
       "      <td>4578.88</td>\n",
       "      <td>2600.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4662.85</td>\n",
       "      <td>4662.21</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>97157</td>\n",
       "      <td>2022-01-18</td>\n",
       "      <td>2022-01-19</td>\n",
       "      <td>1878.25</td>\n",
       "      <td>4578.88</td>\n",
       "      <td>2700.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4662.85</td>\n",
       "      <td>4662.21</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>97158</td>\n",
       "      <td>2022-01-18</td>\n",
       "      <td>2022-01-19</td>\n",
       "      <td>1778.25</td>\n",
       "      <td>4578.88</td>\n",
       "      <td>2800.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4662.85</td>\n",
       "      <td>4662.21</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>97159</td>\n",
       "      <td>2022-01-18</td>\n",
       "      <td>2022-01-19</td>\n",
       "      <td>1678.25</td>\n",
       "      <td>4578.88</td>\n",
       "      <td>2900.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4662.85</td>\n",
       "      <td>4662.21</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>97160</td>\n",
       "      <td>2022-01-18</td>\n",
       "      <td>2022-01-19</td>\n",
       "      <td>1579.25</td>\n",
       "      <td>4578.88</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4662.85</td>\n",
       "      <td>4662.21</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>97161</td>\n",
       "      <td>2022-01-18</td>\n",
       "      <td>2022-01-19</td>\n",
       "      <td>1478.25</td>\n",
       "      <td>4578.88</td>\n",
       "      <td>3100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4662.85</td>\n",
       "      <td>4662.21</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>97162</td>\n",
       "      <td>2022-01-18</td>\n",
       "      <td>2022-01-19</td>\n",
       "      <td>1378.25</td>\n",
       "      <td>4578.88</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4662.85</td>\n",
       "      <td>4662.21</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>97163</td>\n",
       "      <td>2022-01-18</td>\n",
       "      <td>2022-01-19</td>\n",
       "      <td>1279.35</td>\n",
       "      <td>4578.88</td>\n",
       "      <td>3300.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4662.85</td>\n",
       "      <td>4662.21</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>97164</td>\n",
       "      <td>2022-01-18</td>\n",
       "      <td>2022-01-19</td>\n",
       "      <td>1179.35</td>\n",
       "      <td>4578.88</td>\n",
       "      <td>3400.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4662.85</td>\n",
       "      <td>4662.21</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>97165</td>\n",
       "      <td>2022-01-18</td>\n",
       "      <td>2022-01-19</td>\n",
       "      <td>1130.50</td>\n",
       "      <td>4578.88</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4662.85</td>\n",
       "      <td>4662.21</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>97166</td>\n",
       "      <td>2022-01-18</td>\n",
       "      <td>2022-01-19</td>\n",
       "      <td>1080.50</td>\n",
       "      <td>4578.88</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4662.85</td>\n",
       "      <td>4662.21</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>97167</td>\n",
       "      <td>2022-01-18</td>\n",
       "      <td>2022-01-19</td>\n",
       "      <td>1028.60</td>\n",
       "      <td>4578.88</td>\n",
       "      <td>3550.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4662.85</td>\n",
       "      <td>4662.21</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>97168</td>\n",
       "      <td>2022-01-18</td>\n",
       "      <td>2022-01-19</td>\n",
       "      <td>978.60</td>\n",
       "      <td>4578.88</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4662.85</td>\n",
       "      <td>4662.21</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>97169</td>\n",
       "      <td>2022-01-18</td>\n",
       "      <td>2022-01-19</td>\n",
       "      <td>928.65</td>\n",
       "      <td>4578.88</td>\n",
       "      <td>3650.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4662.85</td>\n",
       "      <td>4662.21</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>97170</td>\n",
       "      <td>2022-01-18</td>\n",
       "      <td>2022-01-19</td>\n",
       "      <td>903.65</td>\n",
       "      <td>4578.88</td>\n",
       "      <td>3675.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4662.85</td>\n",
       "      <td>4662.21</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>97171</td>\n",
       "      <td>2022-01-18</td>\n",
       "      <td>2022-01-19</td>\n",
       "      <td>878.65</td>\n",
       "      <td>4578.88</td>\n",
       "      <td>3700.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4662.85</td>\n",
       "      <td>4662.21</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>97172</td>\n",
       "      <td>2022-01-18</td>\n",
       "      <td>2022-01-19</td>\n",
       "      <td>853.65</td>\n",
       "      <td>4578.88</td>\n",
       "      <td>3725.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4662.85</td>\n",
       "      <td>4662.21</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>97173</td>\n",
       "      <td>2022-01-18</td>\n",
       "      <td>2022-01-19</td>\n",
       "      <td>828.65</td>\n",
       "      <td>4578.88</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4662.85</td>\n",
       "      <td>4662.21</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>97174</td>\n",
       "      <td>2022-01-18</td>\n",
       "      <td>2022-01-19</td>\n",
       "      <td>803.65</td>\n",
       "      <td>4578.88</td>\n",
       "      <td>3775.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4662.85</td>\n",
       "      <td>4662.21</td>\n",
       "      <td>4660.64</td>\n",
       "      <td>4726.55</td>\n",
       "      <td>4713.53</td>\n",
       "      <td>4669.85</td>\n",
       "      <td>4676.41</td>\n",
       "      <td>4696.25</td>\n",
       "      <td>4700.64</td>\n",
       "      <td>4793.19</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  Quote_date Expire_date    Price  Underlying_last  Strike  TTM  \\\n",
       "0        97145  2022-01-18  2022-01-19  3378.25          4578.88  1200.0    1   \n",
       "1        97146  2022-01-18  2022-01-19  3178.25          4578.88  1400.0    1   \n",
       "2        97147  2022-01-18  2022-01-19  2978.25          4578.88  1600.0    1   \n",
       "3        97148  2022-01-18  2022-01-19  2778.25          4578.88  1800.0    1   \n",
       "4        97149  2022-01-18  2022-01-19  2678.25          4578.88  1900.0    1   \n",
       "5        97150  2022-01-18  2022-01-19  2578.25          4578.88  2000.0    1   \n",
       "6        97151  2022-01-18  2022-01-19  2478.25          4578.88  2100.0    1   \n",
       "7        97152  2022-01-18  2022-01-19  2378.25          4578.88  2200.0    1   \n",
       "8        97153  2022-01-18  2022-01-19  2278.25          4578.88  2300.0    1   \n",
       "9        97154  2022-01-18  2022-01-19  2178.25          4578.88  2400.0    1   \n",
       "10       97155  2022-01-18  2022-01-19  2078.25          4578.88  2500.0    1   \n",
       "11       97156  2022-01-18  2022-01-19  1978.25          4578.88  2600.0    1   \n",
       "12       97157  2022-01-18  2022-01-19  1878.25          4578.88  2700.0    1   \n",
       "13       97158  2022-01-18  2022-01-19  1778.25          4578.88  2800.0    1   \n",
       "14       97159  2022-01-18  2022-01-19  1678.25          4578.88  2900.0    1   \n",
       "15       97160  2022-01-18  2022-01-19  1579.25          4578.88  3000.0    1   \n",
       "16       97161  2022-01-18  2022-01-19  1478.25          4578.88  3100.0    1   \n",
       "17       97162  2022-01-18  2022-01-19  1378.25          4578.88  3200.0    1   \n",
       "18       97163  2022-01-18  2022-01-19  1279.35          4578.88  3300.0    1   \n",
       "19       97164  2022-01-18  2022-01-19  1179.35          4578.88  3400.0    1   \n",
       "20       97165  2022-01-18  2022-01-19  1130.50          4578.88  3450.0    1   \n",
       "21       97166  2022-01-18  2022-01-19  1080.50          4578.88  3500.0    1   \n",
       "22       97167  2022-01-18  2022-01-19  1028.60          4578.88  3550.0    1   \n",
       "23       97168  2022-01-18  2022-01-19   978.60          4578.88  3600.0    1   \n",
       "24       97169  2022-01-18  2022-01-19   928.65          4578.88  3650.0    1   \n",
       "25       97170  2022-01-18  2022-01-19   903.65          4578.88  3675.0    1   \n",
       "26       97171  2022-01-18  2022-01-19   878.65          4578.88  3700.0    1   \n",
       "27       97172  2022-01-18  2022-01-19   853.65          4578.88  3725.0    1   \n",
       "28       97173  2022-01-18  2022-01-19   828.65          4578.88  3750.0    1   \n",
       "29       97174  2022-01-18  2022-01-19   803.65          4578.88  3775.0    1   \n",
       "\n",
       "    Underlying_1  Underlying_2  Underlying_3  Underlying_4  Underlying_5  \\\n",
       "0        4662.85       4662.21       4660.64       4726.55       4713.53   \n",
       "1        4662.85       4662.21       4660.64       4726.55       4713.53   \n",
       "2        4662.85       4662.21       4660.64       4726.55       4713.53   \n",
       "3        4662.85       4662.21       4660.64       4726.55       4713.53   \n",
       "4        4662.85       4662.21       4660.64       4726.55       4713.53   \n",
       "5        4662.85       4662.21       4660.64       4726.55       4713.53   \n",
       "6        4662.85       4662.21       4660.64       4726.55       4713.53   \n",
       "7        4662.85       4662.21       4660.64       4726.55       4713.53   \n",
       "8        4662.85       4662.21       4660.64       4726.55       4713.53   \n",
       "9        4662.85       4662.21       4660.64       4726.55       4713.53   \n",
       "10       4662.85       4662.21       4660.64       4726.55       4713.53   \n",
       "11       4662.85       4662.21       4660.64       4726.55       4713.53   \n",
       "12       4662.85       4662.21       4660.64       4726.55       4713.53   \n",
       "13       4662.85       4662.21       4660.64       4726.55       4713.53   \n",
       "14       4662.85       4662.21       4660.64       4726.55       4713.53   \n",
       "15       4662.85       4662.21       4660.64       4726.55       4713.53   \n",
       "16       4662.85       4662.21       4660.64       4726.55       4713.53   \n",
       "17       4662.85       4662.21       4660.64       4726.55       4713.53   \n",
       "18       4662.85       4662.21       4660.64       4726.55       4713.53   \n",
       "19       4662.85       4662.21       4660.64       4726.55       4713.53   \n",
       "20       4662.85       4662.21       4660.64       4726.55       4713.53   \n",
       "21       4662.85       4662.21       4660.64       4726.55       4713.53   \n",
       "22       4662.85       4662.21       4660.64       4726.55       4713.53   \n",
       "23       4662.85       4662.21       4660.64       4726.55       4713.53   \n",
       "24       4662.85       4662.21       4660.64       4726.55       4713.53   \n",
       "25       4662.85       4662.21       4660.64       4726.55       4713.53   \n",
       "26       4662.85       4662.21       4660.64       4726.55       4713.53   \n",
       "27       4662.85       4662.21       4660.64       4726.55       4713.53   \n",
       "28       4662.85       4662.21       4660.64       4726.55       4713.53   \n",
       "29       4662.85       4662.21       4660.64       4726.55       4713.53   \n",
       "\n",
       "    Underlying_6  Underlying_7  Underlying_8  Underlying_9  Underlying_10  \\\n",
       "0        4669.85       4676.41       4696.25       4700.64        4793.19   \n",
       "1        4669.85       4676.41       4696.25       4700.64        4793.19   \n",
       "2        4669.85       4676.41       4696.25       4700.64        4793.19   \n",
       "3        4669.85       4676.41       4696.25       4700.64        4793.19   \n",
       "4        4669.85       4676.41       4696.25       4700.64        4793.19   \n",
       "5        4669.85       4676.41       4696.25       4700.64        4793.19   \n",
       "6        4669.85       4676.41       4696.25       4700.64        4793.19   \n",
       "7        4669.85       4676.41       4696.25       4700.64        4793.19   \n",
       "8        4669.85       4676.41       4696.25       4700.64        4793.19   \n",
       "9        4669.85       4676.41       4696.25       4700.64        4793.19   \n",
       "10       4669.85       4676.41       4696.25       4700.64        4793.19   \n",
       "11       4669.85       4676.41       4696.25       4700.64        4793.19   \n",
       "12       4669.85       4676.41       4696.25       4700.64        4793.19   \n",
       "13       4669.85       4676.41       4696.25       4700.64        4793.19   \n",
       "14       4669.85       4676.41       4696.25       4700.64        4793.19   \n",
       "15       4669.85       4676.41       4696.25       4700.64        4793.19   \n",
       "16       4669.85       4676.41       4696.25       4700.64        4793.19   \n",
       "17       4669.85       4676.41       4696.25       4700.64        4793.19   \n",
       "18       4669.85       4676.41       4696.25       4700.64        4793.19   \n",
       "19       4669.85       4676.41       4696.25       4700.64        4793.19   \n",
       "20       4669.85       4676.41       4696.25       4700.64        4793.19   \n",
       "21       4669.85       4676.41       4696.25       4700.64        4793.19   \n",
       "22       4669.85       4676.41       4696.25       4700.64        4793.19   \n",
       "23       4669.85       4676.41       4696.25       4700.64        4793.19   \n",
       "24       4669.85       4676.41       4696.25       4700.64        4793.19   \n",
       "25       4669.85       4676.41       4696.25       4700.64        4793.19   \n",
       "26       4669.85       4676.41       4696.25       4700.64        4793.19   \n",
       "27       4669.85       4676.41       4696.25       4700.64        4793.19   \n",
       "28       4669.85       4676.41       4696.25       4700.64        4793.19   \n",
       "29       4669.85       4676.41       4696.25       4700.64        4793.19   \n",
       "\n",
       "       R  \n",
       "0   0.05  \n",
       "1   0.05  \n",
       "2   0.05  \n",
       "3   0.05  \n",
       "4   0.05  \n",
       "5   0.05  \n",
       "6   0.05  \n",
       "7   0.05  \n",
       "8   0.05  \n",
       "9   0.05  \n",
       "10  0.05  \n",
       "11  0.05  \n",
       "12  0.05  \n",
       "13  0.05  \n",
       "14  0.05  \n",
       "15  0.05  \n",
       "16  0.05  \n",
       "17  0.05  \n",
       "18  0.05  \n",
       "19  0.05  \n",
       "20  0.05  \n",
       "21  0.05  \n",
       "22  0.05  \n",
       "23  0.05  \n",
       "24  0.05  \n",
       "25  0.05  \n",
       "26  0.05  \n",
       "27  0.05  \n",
       "28  0.05  \n",
       "29  0.05  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1739894, 10, 1)\n",
      "(1739894, 4)\n",
      "(100000, 10, 1)\n",
      "(100000, 4)\n"
     ]
    }
   ],
   "source": [
    "file = \"../data/processed_data/2022.csv\"\n",
    "df = pd.read_csv(file)\n",
    "display(df.head(30))\n",
    "df.head(20)\n",
    "\n",
    "val_set = 100000\n",
    "num_features = 4\n",
    "time_steps = 10\n",
    "\n",
    "train_y = df['Quote_date'][:-val_set].to_numpy()\n",
    "val_y = df['Quote_date'][-val_set:].to_numpy()\n",
    "\n",
    "array = df.to_numpy()\n",
    "\n",
    "train_x = [array[:-val_set][:,-time_steps:].reshape(len(array[:-val_set]),time_steps,1), array[:-val_set][:,:num_features]]\n",
    "val_x = [array[-val_set:][:,-time_steps:].reshape(len(array[-val_set:]),time_steps,1), array[-val_set:][:,:num_features]]\n",
    "\n",
    "print(train_x[0].shape)\n",
    "print(train_x[1].shape)\n",
    "print(val_x[0].shape)\n",
    "print(val_x[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(config):\n",
    "    '''Builds an LSTM-MLP model of minimum 2 layers sequentially from a given config dictionary'''\n",
    "    underlying_history = Input((config.LSTM_timesteps,1))\n",
    "    bs_vars = Input((config.Num_features,))\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(\n",
    "        units = config.LSTM_units,\n",
    "        activation = tanh,\n",
    "        input_shape = (config.LSTM_timesteps, 1),\n",
    "        return_sequences = True\n",
    "    ))\n",
    "\n",
    "    for _ in range(config.LSTM_layers - 2):\n",
    "        model.add(LSTM(\n",
    "            units = config.LSTM_units,\n",
    "            activation = tanh,\n",
    "            return_sequences = True\n",
    "        ))\n",
    "    \n",
    "    model.add(LSTM(\n",
    "        units = config.LSTM_units,\n",
    "        activation = tanh,\n",
    "        return_sequences = False\n",
    "    ))\n",
    "\n",
    "    layers = Concatenate()([model(underlying_history), bs_vars])\n",
    "    \n",
    "    for _ in range(config.MLP_layers - 1):\n",
    "        layers = Dense(config.MLP_units)(layers)\n",
    "        layers = BatchNormalization(momentum=config.Bn_momentum)(layers)\n",
    "        layers = LeakyReLU()(layers)\n",
    "\n",
    "    #create the output layer\n",
    "    output = Dense(1, activation='relu')(layers)\n",
    "\n",
    "    #create exponentially decaying learning rate schedule\n",
    "    lr_schedule = ExponentialDecay(\n",
    "        initial_learning_rate = config.Lr,\n",
    "        decay_steps = int(len(train_x)/config.Minibatch_size),\n",
    "        decay_rate=config.Lr_decay\n",
    "    )\n",
    "\n",
    "    #compile the model\n",
    "    model = Model(inputs=[underlying_history, bs_vars], outputs=output)\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=lr_schedule))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter search setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring the sweep hyperparameter search space\n",
    "sweep_configuration = {\n",
    "    'method': 'random',\n",
    "    'name': 'LSTM-MLP v.1.0 test data',\n",
    "    'metric': {\n",
    "        'goal': 'minimize', \n",
    "        'name': 'val_loss'\n",
    "\t\t},\n",
    "    'parameters': {\n",
    "        'LSTM_units': {\n",
    "            'values': [32, 64, 96, 128]},\n",
    "        'MLP_units': {\n",
    "            'values': [32, 64, 96, 128]},\n",
    "        'LSTM_timesteps': {\n",
    "            'values': [10]},\n",
    "        'LSTM_layers': {\n",
    "            'distribution': 'int_uniform',\n",
    "            'max': 6, 'min': 4},\n",
    "        'MLP_layers': {\n",
    "            'distribution': 'int_uniform',\n",
    "            'max': 6, 'min': 4},\n",
    "        'Bn_momentum': {\n",
    "            'distribution': 'uniform',\n",
    "            'max': 0.4, 'min': 0},\n",
    "        'Lr': {\n",
    "            'distribution': 'uniform',\n",
    "            'max': 0.005, 'min': 0.0005},\n",
    "        'Lr_decay': {\n",
    "            'distribution': 'uniform',\n",
    "            'max': 1, 'min': 0.9},        \n",
    "        'Minibatch_size': {\n",
    "            'values': [1024, 2048, 4096]},\n",
    "        'Min_delta': {\n",
    "            'value': 1},\n",
    "        'Patience': {\n",
    "            'value': 20},\n",
    "        'Num_features': {\n",
    "            'value': 4},\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize sweep and creating sweepID\n",
    "\n",
    "# If new sweep, uncomment the line below and comment the line after it\n",
    "#sweep_id = wandb.sweep(sweep=sweep_configuration, project='Deep learning for option pricing - test area') \n",
    "sweep_id = '98bxt6oq'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(train_x = train_x, train_y = train_y, val_x = val_x, val_y = val_y, config = None, project = None):\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config, project = project):\n",
    "\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "\n",
    "        model = create_model(config)\n",
    "\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            min_delta = config.Min_delta,\n",
    "            patience = config.Patience,\n",
    "        )\n",
    "        \n",
    "        wandb_callback = WandbCallback(\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            save_model=False\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            train_x,\n",
    "            train_y,\n",
    "            batch_size = config.Minibatch_size,\n",
    "            validation_data = (val_x, val_y),\n",
    "            epochs = 1000,\n",
    "            callbacks = [early_stopping, wandb_callback] \n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run full sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qi5863el with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBn_momentum: 0.0042450897520058374\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_layers: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_timesteps: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_units: 96\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLr: 0.003814355437640067\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLr_decay: 0.9453983300912104\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tMLP_layers: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tMLP_units: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tMin_delta: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tMinibatch_size: 1024\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tNum_features: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tPatience: 20\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/hjalmarjacobvinje/Documents/deep-learning-for-option-pricing/LSTM-MLP_notebooks/wandb/run-20230202_160503-qi5863el</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/qi5863el\" target=\"_blank\">ethereal-sweep-1</a></strong> to <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page: <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/sweeps/98bxt6oq\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/sweeps/98bxt6oq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/sweeps/98bxt6oq\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/sweeps/98bxt6oq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/qi5863el\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/qi5863el</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ethereal-sweep-1</strong> at: <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/qi5863el\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/qi5863el</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230202_160503-qi5863el/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run qi5863el errored: ValueError('Failed to convert a NumPy array to a Tensor (Unsupported object type float).')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run qi5863el errored: ValueError('Failed to convert a NumPy array to a Tensor (Unsupported object type float).')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xu7u3yy7 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBn_momentum: 0.28450434007205505\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_layers: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_timesteps: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_units: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLr: 0.002378711371909496\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLr_decay: 0.916297516076408\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tMLP_layers: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tMLP_units: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tMin_delta: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tMinibatch_size: 2048\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tNum_features: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tPatience: 20\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/hjalmarjacobvinje/Documents/deep-learning-for-option-pricing/LSTM-MLP_notebooks/wandb/run-20230202_160523-xu7u3yy7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/xu7u3yy7\" target=\"_blank\">unique-sweep-2</a></strong> to <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page: <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/sweeps/98bxt6oq\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/sweeps/98bxt6oq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/sweeps/98bxt6oq\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/sweeps/98bxt6oq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/xu7u3yy7\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/xu7u3yy7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id=sweep_id, function=trainer, project='Deep learning for option pricing - test area', count = 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:xu7u3yy7) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-43 (_run_job):\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_16718/85754476.py\", line 24, in trainer\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/wandb/integration/keras/keras.py\", line 174, in new_v2\n",
      "    return old_v2(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/wandb/integration/keras/keras.py\", line 174, in new_v2\n",
      "    return old_v2(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/wandb/integration/keras/keras.py\", line 174, in new_v2\n",
      "    return old_v2(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py\", line 102, in convert_to_eager_tensor\n",
      "    return ops.EagerTensor(value, ctx.device_name, dtype)\n",
      "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 298, in _run_job\n",
      "    self._function()\n",
      "  File \"/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/ipykernel_16718/85754476.py\", line 3, in trainer\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 2994, in __exit__\n",
      "    self._finish(exit_code)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 1848, in _finish\n",
      "    hook.call()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/wandb/sdk/wandb_init.py\", line 418, in _jupyter_teardown\n",
      "    ipython.display_pub.publish = ipython.display_pub._orig_publish\n",
      "AttributeError: 'ZMQDisplayPublisher' object has no attribute '_orig_publish'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 303, in _run_job\n",
      "    wandb.finish(exit_code=1)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 3671, in finish\n",
      "    wandb.run.finish(exit_code=exit_code, quiet=quiet)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 370, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 333, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 1835, in finish\n",
      "    return self._finish(exit_code, quiet)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 1848, in _finish\n",
      "    hook.call()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/wandb/sdk/wandb_init.py\", line 418, in _jupyter_teardown\n",
      "    ipython.display_pub.publish = ipython.display_pub._orig_publish\n",
      "AttributeError: 'ZMQDisplayPublisher' object has no attribute '_orig_publish'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">unique-sweep-2</strong> at: <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/xu7u3yy7\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/xu7u3yy7</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230202_160523-xu7u3yy7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:xu7u3yy7). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/hjalmarjacobvinje/Documents/deep-learning-for-option-pricing/LSTM-MLP_notebooks/wandb/run-20230202_160532-xu7u3yy7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/xu7u3yy7\" target=\"_blank\">unique-sweep-2</a></strong> to <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page: <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/sweeps/98bxt6oq\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/sweeps/98bxt6oq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/sweeps/98bxt6oq\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/sweeps/98bxt6oq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/xu7u3yy7\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/xu7u3yy7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">unique-sweep-2</strong> at: <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/xu7u3yy7\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/xu7u3yy7</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230202_160532-xu7u3yy7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type float).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 17\u001b[0m\n\u001b[1;32m      1\u001b[0m config \u001b[39m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mLSTM_units\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m4\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mMLP_units\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m100\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mTest data\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     16\u001b[0m }\n\u001b[0;32m---> 17\u001b[0m trainer(config \u001b[39m=\u001b[39;49m config, project \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mDeep learning for option pricing - test area\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[23], line 24\u001b[0m, in \u001b[0;36mtrainer\u001b[0;34m(train_x, train_y, val_x, val_y, config, project)\u001b[0m\n\u001b[1;32m     11\u001b[0m early_stopping \u001b[39m=\u001b[39m EarlyStopping(\n\u001b[1;32m     12\u001b[0m     monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     13\u001b[0m     mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     14\u001b[0m     min_delta \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mMin_delta,\n\u001b[1;32m     15\u001b[0m     patience \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mPatience,\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m wandb_callback \u001b[39m=\u001b[39m WandbCallback(\n\u001b[1;32m     19\u001b[0m     monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     20\u001b[0m     mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     21\u001b[0m     save_model\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     22\u001b[0m )\n\u001b[0;32m---> 24\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     25\u001b[0m     train_x,\n\u001b[1;32m     26\u001b[0m     train_y,\n\u001b[1;32m     27\u001b[0m     batch_size \u001b[39m=\u001b[39;49m config\u001b[39m.\u001b[39;49mMinibatch_size,\n\u001b[1;32m     28\u001b[0m     validation_data \u001b[39m=\u001b[39;49m (val_x, val_y),\n\u001b[1;32m     29\u001b[0m     epochs \u001b[39m=\u001b[39;49m \u001b[39m1000\u001b[39;49m,\n\u001b[1;32m     30\u001b[0m     callbacks \u001b[39m=\u001b[39;49m [early_stopping, wandb_callback] \n\u001b[1;32m     31\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/wandb/integration/keras/keras.py:174\u001b[0m, in \u001b[0;36mpatch_tf_keras.<locals>.new_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[39mfor\u001b[39;00m cbk \u001b[39min\u001b[39;00m cbks:\n\u001b[1;32m    173\u001b[0m         set_wandb_attrs(cbk, val_data)\n\u001b[0;32m--> 174\u001b[0m \u001b[39mreturn\u001b[39;00m old_v2(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/wandb/integration/keras/keras.py:174\u001b[0m, in \u001b[0;36mpatch_tf_keras.<locals>.new_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[39mfor\u001b[39;00m cbk \u001b[39min\u001b[39;00m cbks:\n\u001b[1;32m    173\u001b[0m         set_wandb_attrs(cbk, val_data)\n\u001b[0;32m--> 174\u001b[0m \u001b[39mreturn\u001b[39;00m old_v2(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/wandb/integration/keras/keras.py:174\u001b[0m, in \u001b[0;36mpatch_tf_keras.<locals>.new_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[39mfor\u001b[39;00m cbk \u001b[39min\u001b[39;00m cbks:\n\u001b[1;32m    173\u001b[0m         set_wandb_attrs(cbk, val_data)\n\u001b[0;32m--> 174\u001b[0m \u001b[39mreturn\u001b[39;00m old_v2(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'LSTM_units': 4,\n",
    "    'MLP_units': 100,\n",
    "    'LSTM_timesteps': 20,\n",
    "    'LSTM_layers': 4,\n",
    "    'MLP_layers': 4,\n",
    "    'Bn_momentum': 0.9,\n",
    "    'Lr': 1e-4,\n",
    "    'Lr_decay': 1,\n",
    "    'Minibatch_size': 10,\n",
    "    'Min_delta': 0.1,\n",
    "    'Patience': 20,\n",
    "    'Num_features': 4,\n",
    "    'Architecture': 'LSTM-MLP v.0.1',\n",
    "    'Dataset': 'Test data',\n",
    "}\n",
    "trainer(config = config, project = 'Deep learning for option pricing - test area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
