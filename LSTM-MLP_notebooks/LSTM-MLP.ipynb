{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights and Biases\n",
    "#!pip install -q wandb\n",
    "# Tensorflow\n",
    "#!pip install -q tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, LSTM, Concatenate, Dense, BatchNormalization, LeakyReLU\n",
    "from keras.activations import tanh\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from tensorflow import square, reduce_mean\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.math import multiply\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/hjalmarjacobvinje/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If running in colab, insert your wandb key here\n",
    "\n",
    "import config\n",
    "#Erlend\n",
    "#wandb.login(key=config.erlend_key)\n",
    "# Hjalmar\n",
    "wandb.login(key=config.hjalmar_key)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, split and normalize data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Quote_date</th>\n",
       "      <th>Expire_date</th>\n",
       "      <th>Price</th>\n",
       "      <th>Underlying_last</th>\n",
       "      <th>Strike</th>\n",
       "      <th>TTM</th>\n",
       "      <th>Moneyness</th>\n",
       "      <th>Underlying_return</th>\n",
       "      <th>Underlying_1</th>\n",
       "      <th>...</th>\n",
       "      <th>Underlying_82</th>\n",
       "      <th>Underlying_83</th>\n",
       "      <th>Underlying_84</th>\n",
       "      <th>Underlying_85</th>\n",
       "      <th>Underlying_86</th>\n",
       "      <th>Underlying_87</th>\n",
       "      <th>Underlying_88</th>\n",
       "      <th>Underlying_89</th>\n",
       "      <th>Underlying_90</th>\n",
       "      <th>R</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>2021-01-06</td>\n",
       "      <td>2701.855</td>\n",
       "      <td>3701.38</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.701380</td>\n",
       "      <td>-0.014623</td>\n",
       "      <td>0.006403</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015046</td>\n",
       "      <td>0.007654</td>\n",
       "      <td>-0.002161</td>\n",
       "      <td>0.006698</td>\n",
       "      <td>0.001618</td>\n",
       "      <td>0.010257</td>\n",
       "      <td>0.003357</td>\n",
       "      <td>0.010258</td>\n",
       "      <td>0.003477</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>2021-01-06</td>\n",
       "      <td>2598.795</td>\n",
       "      <td>3701.38</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.364891</td>\n",
       "      <td>-0.014623</td>\n",
       "      <td>0.006403</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015046</td>\n",
       "      <td>0.007654</td>\n",
       "      <td>-0.002161</td>\n",
       "      <td>0.006698</td>\n",
       "      <td>0.001618</td>\n",
       "      <td>0.010257</td>\n",
       "      <td>0.003357</td>\n",
       "      <td>0.010258</td>\n",
       "      <td>0.003477</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>2021-01-06</td>\n",
       "      <td>2500.195</td>\n",
       "      <td>3701.38</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.084483</td>\n",
       "      <td>-0.014623</td>\n",
       "      <td>0.006403</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015046</td>\n",
       "      <td>0.007654</td>\n",
       "      <td>-0.002161</td>\n",
       "      <td>0.006698</td>\n",
       "      <td>0.001618</td>\n",
       "      <td>0.010257</td>\n",
       "      <td>0.003357</td>\n",
       "      <td>0.010258</td>\n",
       "      <td>0.003477</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>2021-01-06</td>\n",
       "      <td>2400.290</td>\n",
       "      <td>3701.38</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.847215</td>\n",
       "      <td>-0.014623</td>\n",
       "      <td>0.006403</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015046</td>\n",
       "      <td>0.007654</td>\n",
       "      <td>-0.002161</td>\n",
       "      <td>0.006698</td>\n",
       "      <td>0.001618</td>\n",
       "      <td>0.010257</td>\n",
       "      <td>0.003357</td>\n",
       "      <td>0.010258</td>\n",
       "      <td>0.003477</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>2021-01-06</td>\n",
       "      <td>2300.300</td>\n",
       "      <td>3701.38</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.643843</td>\n",
       "      <td>-0.014623</td>\n",
       "      <td>0.006403</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015046</td>\n",
       "      <td>0.007654</td>\n",
       "      <td>-0.002161</td>\n",
       "      <td>0.006698</td>\n",
       "      <td>0.001618</td>\n",
       "      <td>0.010257</td>\n",
       "      <td>0.003357</td>\n",
       "      <td>0.010258</td>\n",
       "      <td>0.003477</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4058478</th>\n",
       "      <td>4058478</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>2025-12-19</td>\n",
       "      <td>6.950</td>\n",
       "      <td>3839.81</td>\n",
       "      <td>8400.0</td>\n",
       "      <td>1085</td>\n",
       "      <td>0.457120</td>\n",
       "      <td>-0.002300</td>\n",
       "      <td>0.017343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003593</td>\n",
       "      <td>-0.000262</td>\n",
       "      <td>-0.010776</td>\n",
       "      <td>0.003305</td>\n",
       "      <td>-0.007857</td>\n",
       "      <td>-0.011077</td>\n",
       "      <td>-0.006737</td>\n",
       "      <td>-0.033678</td>\n",
       "      <td>0.014386</td>\n",
       "      <td>4.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4058479</th>\n",
       "      <td>4058479</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>2025-12-19</td>\n",
       "      <td>5.950</td>\n",
       "      <td>3839.81</td>\n",
       "      <td>8600.0</td>\n",
       "      <td>1085</td>\n",
       "      <td>0.446490</td>\n",
       "      <td>-0.002300</td>\n",
       "      <td>0.017343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003593</td>\n",
       "      <td>-0.000262</td>\n",
       "      <td>-0.010776</td>\n",
       "      <td>0.003305</td>\n",
       "      <td>-0.007857</td>\n",
       "      <td>-0.011077</td>\n",
       "      <td>-0.006737</td>\n",
       "      <td>-0.033678</td>\n",
       "      <td>0.014386</td>\n",
       "      <td>4.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4058480</th>\n",
       "      <td>4058480</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>2025-12-19</td>\n",
       "      <td>5.450</td>\n",
       "      <td>3839.81</td>\n",
       "      <td>8800.0</td>\n",
       "      <td>1085</td>\n",
       "      <td>0.436342</td>\n",
       "      <td>-0.002300</td>\n",
       "      <td>0.017343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003593</td>\n",
       "      <td>-0.000262</td>\n",
       "      <td>-0.010776</td>\n",
       "      <td>0.003305</td>\n",
       "      <td>-0.007857</td>\n",
       "      <td>-0.011077</td>\n",
       "      <td>-0.006737</td>\n",
       "      <td>-0.033678</td>\n",
       "      <td>0.014386</td>\n",
       "      <td>4.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4058481</th>\n",
       "      <td>4058481</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>2025-12-19</td>\n",
       "      <td>4.500</td>\n",
       "      <td>3839.81</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>1085</td>\n",
       "      <td>0.426646</td>\n",
       "      <td>-0.002300</td>\n",
       "      <td>0.017343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003593</td>\n",
       "      <td>-0.000262</td>\n",
       "      <td>-0.010776</td>\n",
       "      <td>0.003305</td>\n",
       "      <td>-0.007857</td>\n",
       "      <td>-0.011077</td>\n",
       "      <td>-0.006737</td>\n",
       "      <td>-0.033678</td>\n",
       "      <td>0.014386</td>\n",
       "      <td>4.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4058482</th>\n",
       "      <td>4058482</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>2025-12-19</td>\n",
       "      <td>4.025</td>\n",
       "      <td>3839.81</td>\n",
       "      <td>9200.0</td>\n",
       "      <td>1085</td>\n",
       "      <td>0.417371</td>\n",
       "      <td>-0.002300</td>\n",
       "      <td>0.017343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003593</td>\n",
       "      <td>-0.000262</td>\n",
       "      <td>-0.010776</td>\n",
       "      <td>0.003305</td>\n",
       "      <td>-0.007857</td>\n",
       "      <td>-0.011077</td>\n",
       "      <td>-0.006737</td>\n",
       "      <td>-0.033678</td>\n",
       "      <td>0.014386</td>\n",
       "      <td>4.22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4058483 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0  Quote_date Expire_date     Price  Underlying_last  \\\n",
       "0                 0  2021-01-04  2021-01-06  2701.855          3701.38   \n",
       "1                 1  2021-01-04  2021-01-06  2598.795          3701.38   \n",
       "2                 2  2021-01-04  2021-01-06  2500.195          3701.38   \n",
       "3                 3  2021-01-04  2021-01-06  2400.290          3701.38   \n",
       "4                 4  2021-01-04  2021-01-06  2300.300          3701.38   \n",
       "...             ...         ...         ...       ...              ...   \n",
       "4058478     4058478  2022-12-30  2025-12-19     6.950          3839.81   \n",
       "4058479     4058479  2022-12-30  2025-12-19     5.950          3839.81   \n",
       "4058480     4058480  2022-12-30  2025-12-19     5.450          3839.81   \n",
       "4058481     4058481  2022-12-30  2025-12-19     4.500          3839.81   \n",
       "4058482     4058482  2022-12-30  2025-12-19     4.025          3839.81   \n",
       "\n",
       "         Strike   TTM  Moneyness  Underlying_return  Underlying_1  ...  \\\n",
       "0        1000.0     2   3.701380          -0.014623      0.006403  ...   \n",
       "1        1100.0     2   3.364891          -0.014623      0.006403  ...   \n",
       "2        1200.0     2   3.084483          -0.014623      0.006403  ...   \n",
       "3        1300.0     2   2.847215          -0.014623      0.006403  ...   \n",
       "4        1400.0     2   2.643843          -0.014623      0.006403  ...   \n",
       "...         ...   ...        ...                ...           ...  ...   \n",
       "4058478  8400.0  1085   0.457120          -0.002300      0.017343  ...   \n",
       "4058479  8600.0  1085   0.446490          -0.002300      0.017343  ...   \n",
       "4058480  8800.0  1085   0.436342          -0.002300      0.017343  ...   \n",
       "4058481  9000.0  1085   0.426646          -0.002300      0.017343  ...   \n",
       "4058482  9200.0  1085   0.417371          -0.002300      0.017343  ...   \n",
       "\n",
       "         Underlying_82  Underlying_83  Underlying_84  Underlying_85  \\\n",
       "0             0.015046       0.007654      -0.002161       0.006698   \n",
       "1             0.015046       0.007654      -0.002161       0.006698   \n",
       "2             0.015046       0.007654      -0.002161       0.006698   \n",
       "3             0.015046       0.007654      -0.002161       0.006698   \n",
       "4             0.015046       0.007654      -0.002161       0.006698   \n",
       "...                ...            ...            ...            ...   \n",
       "4058478      -0.003593      -0.000262      -0.010776       0.003305   \n",
       "4058479      -0.003593      -0.000262      -0.010776       0.003305   \n",
       "4058480      -0.003593      -0.000262      -0.010776       0.003305   \n",
       "4058481      -0.003593      -0.000262      -0.010776       0.003305   \n",
       "4058482      -0.003593      -0.000262      -0.010776       0.003305   \n",
       "\n",
       "         Underlying_86  Underlying_87  Underlying_88  Underlying_89  \\\n",
       "0             0.001618       0.010257       0.003357       0.010258   \n",
       "1             0.001618       0.010257       0.003357       0.010258   \n",
       "2             0.001618       0.010257       0.003357       0.010258   \n",
       "3             0.001618       0.010257       0.003357       0.010258   \n",
       "4             0.001618       0.010257       0.003357       0.010258   \n",
       "...                ...            ...            ...            ...   \n",
       "4058478      -0.007857      -0.011077      -0.006737      -0.033678   \n",
       "4058479      -0.007857      -0.011077      -0.006737      -0.033678   \n",
       "4058480      -0.007857      -0.011077      -0.006737      -0.033678   \n",
       "4058481      -0.007857      -0.011077      -0.006737      -0.033678   \n",
       "4058482      -0.007857      -0.011077      -0.006737      -0.033678   \n",
       "\n",
       "         Underlying_90     R  \n",
       "0             0.003477  0.09  \n",
       "1             0.003477  0.09  \n",
       "2             0.003477  0.09  \n",
       "3             0.003477  0.09  \n",
       "4             0.003477  0.09  \n",
       "...                ...   ...  \n",
       "4058478       0.014386  4.22  \n",
       "4058479       0.014386  4.22  \n",
       "4058480       0.014386  4.22  \n",
       "4058481       0.014386  4.22  \n",
       "4058482       0.014386  4.22  \n",
       "\n",
       "[4058483 rows x 100 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "google_colab = False\n",
    "\n",
    "if google_colab:\n",
    "    import tensorflow as tf\n",
    "    # Pring info\n",
    "    gpu_info = !nvidia-smi\n",
    "    gpu_info = '\\n'.join(gpu_info)\n",
    "    if gpu_info.find('failed') >= 0:\n",
    "        print('Not connected to a GPU')\n",
    "    else:\n",
    "        print(gpu_info)\n",
    "    \n",
    "    from psutil import virtual_memory\n",
    "    ram_gb = virtual_memory().total / 1e9\n",
    "    print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "    if ram_gb < 20:\n",
    "        print('Not using a high-RAM runtime')\n",
    "    else:\n",
    "        print('You are using a high-RAM runtime!')\n",
    "\n",
    "    # Code to read csv file into Colaboratory:\n",
    "    !pip install -U -q PyDrive\n",
    "    from pydrive.auth import GoogleAuth\n",
    "    from pydrive.drive import GoogleDrive\n",
    "    from google.colab import auth\n",
    "    from oauth2client.client import GoogleCredentials\n",
    "    # Authenticate and create the PyDrive client.\n",
    "    auth.authenticate_user()\n",
    "    gauth = GoogleAuth()\n",
    "    gauth.credentials = GoogleCredentials.get_application_default()\n",
    "    drive = GoogleDrive(gauth)\n",
    "    id = \"1tSZeYgiZh_gL6CKUYufNlb1C6X4vODY3\"\n",
    "    downloaded = drive.CreateFile({'id':id}) \n",
    "    downloaded.GetContentFile('2021_2022.csv')  \n",
    "    df_read = pd.read_csv('2021_2022.csv')\n",
    "else:\n",
    "    file = \"../data/processed_data/2021_2022.csv\"\n",
    "    df_read = pd.read_csv(file)\n",
    "\n",
    "display(df_read)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1841485, 90, 1), (1841485, 4)\n",
      "Val shape: (202710, 90, 1), (202710, 4)\n"
     ]
    }
   ],
   "source": [
    "df = df_read\n",
    "del df_read\n",
    "\n",
    "# Format settings\n",
    "max_timesteps = 90\n",
    "moneyness = False # Moneyness = True WIP\n",
    "bs_vars = ['Moneyness', 'TTM', 'R'] if moneyness else ['Underlying_last', 'Strike', 'TTM', 'R']\n",
    "underlying_lags = ['Underlying_last'] + [f'Underlying_{i}' for i in range (1, max_timesteps)]\n",
    "\n",
    "# Create train, validation and test set split points\n",
    "train_start = datetime(2017,1,1) \n",
    "val_start = train_start + relativedelta(months=44) \n",
    "test_start = val_start + relativedelta(months=6)\n",
    "test_end = test_start + relativedelta(months=6)\n",
    "train_start = str(train_start.date())\n",
    "val_start = str(val_start.date())\n",
    "test_start = str(test_start.date())\n",
    "test_end = str(test_end.date())\n",
    "\n",
    "# Split train and validation data\n",
    "df_train = df[(df['Quote_date'] >= train_start) & ( df['Quote_date'] < val_start)]\n",
    "df_val = df[(df['Quote_date'] >= val_start) & ( df['Quote_date'] < test_start)]\n",
    "\n",
    "del df\n",
    "# Extract target values\n",
    "train_y = (df_train['Price']/df_train['Strike']).to_numpy() if moneyness else df_train['Price'].to_numpy()\n",
    "val_y = (df_val['Price']/df_val['Strike']).to_numpy() if moneyness else df_val['Price'].to_numpy()\n",
    "\n",
    "# If usining moneyness, extract strike\n",
    "if moneyness:\n",
    "    train_strike = df_train['Strike'].to_numpy()\n",
    "    val_strike = df_val['Strike'].to_numpy()\n",
    "\n",
    "# Convert dataframes to numpy arrays\n",
    "train_x = [df_train[underlying_lags].to_numpy(), df_train[bs_vars].to_numpy()]\n",
    "val_x = [df_val[underlying_lags].to_numpy(), df_val[bs_vars].to_numpy()]\n",
    "\n",
    "del df_train\n",
    "del df_val\n",
    "\n",
    "# Scale features based on training set\n",
    "underlying_scaler = MinMaxScaler()\n",
    "train_x[0] = underlying_scaler.fit_transform(train_x[0])\n",
    "val_x[0] = underlying_scaler.transform(val_x[0])\n",
    "\n",
    "bs_scaler = MinMaxScaler()\n",
    "train_x[1] = bs_scaler.fit_transform(train_x[1])\n",
    "val_x[1] = bs_scaler.transform(val_x[1])\n",
    "\n",
    "# Shuffle training set\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(len(train_x[0]))\n",
    "train_x = [train_x[0][shuffle], train_x[1][shuffle]]\n",
    "train_y = train_y[shuffle]\n",
    "if moneyness:\n",
    "    train_strike = train_strike[shuffle]\n",
    "\n",
    "# Reshape data to fit LSTM\n",
    "train_x = [train_x[0].reshape(len(train_x[0]), max_timesteps,1), train_x[1]]\n",
    "val_x = [val_x[0].reshape(len(val_x[0]), max_timesteps, 1), val_x[1]]\n",
    "\n",
    "print(f'Train shape: {train_x[0].shape}, {train_x[1].shape}')\n",
    "print(f'Val shape: {val_x[0].shape}, {val_x[1].shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(config):\n",
    "    '''Builds an LSTM-MLP model of minimum 2 layers sequentially from a given config dictionary'''\n",
    "\n",
    "    # Input layers\n",
    "    underlying_history = Input((config.LSTM_timesteps,1))\n",
    "    bs_vars = Input((config.Num_features,))\n",
    "\n",
    "    # LSTM layers\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(\n",
    "        units = config.LSTM_units,\n",
    "        activation = tanh,\n",
    "        input_shape = (config.LSTM_timesteps, 1),\n",
    "        return_sequences = True\n",
    "    ))\n",
    "\n",
    "    for _ in range(config.LSTM_layers - 2):\n",
    "        model.add(LSTM(\n",
    "            units = config.LSTM_units,\n",
    "            activation = tanh,\n",
    "            return_sequences = True\n",
    "        ))\n",
    "    \n",
    "    model.add(LSTM(\n",
    "        units = config.LSTM_units,\n",
    "        activation = tanh,\n",
    "        return_sequences = False\n",
    "    ))\n",
    "\n",
    "    # MLP layers\n",
    "    layers = Concatenate()([model(underlying_history), bs_vars])\n",
    "    \n",
    "    for _ in range(config.MLP_layers - 1):\n",
    "        layers = Dense(config.MLP_units)(layers)\n",
    "        layers = BatchNormalization(momentum=config.Bn_momentum)(layers)\n",
    "        layers = LeakyReLU()(layers)\n",
    "\n",
    "    output = Dense(1, activation='relu')(layers)\n",
    "\n",
    "    # Exponential decaying learning rate\n",
    "    lr_schedule = ExponentialDecay(\n",
    "        initial_learning_rate = config.Lr,\n",
    "        decay_steps = int(len(train_x[0])/config.Minibatch_size),\n",
    "        decay_rate=config.Lr_decay\n",
    "    )\n",
    "\n",
    "    # Compile model\n",
    "    model = Model(inputs=[underlying_history, bs_vars], outputs=output)\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=lr_schedule))\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter search setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Malformed sweep config detected! This may cause your sweep to behave in unexpected ways.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m To avoid this, please fix the sweep config schema violations below:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m   Violation 1. Lr uses log_uniform, where min/max specify base-e exponents. Use log_uniform_values to specify limit values.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m   Violation 2. Lr_decay uses log_uniform, where min/max specify base-e exponents. Use log_uniform_values to specify limit values.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: tx3zgah9\n",
      "Sweep URL: https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/sweeps/tx3zgah9\n"
     ]
    }
   ],
   "source": [
    "# Configuring the sweep hyperparameter search space\n",
    "sweep_configuration = {\n",
    "    'method': 'random',\n",
    "    'name': 'LSTM-MLP v3.2: unfilted data, 2017-2022, single LSTM',\n",
    "    'metric': {\n",
    "        'goal': 'minimize', \n",
    "        'name': 'val_loss'\n",
    "\t\t},\n",
    "    'parameters': {\n",
    "        'LSTM_units': {\n",
    "            'values': [4, 8, 16, 32]},\n",
    "        'MLP_units': {\n",
    "            'values': [50, 100, 200, 400, 600]},\n",
    "        'LSTM_timesteps': {\n",
    "            'values': [10, 20, 40, 60, 90, 150]},\n",
    "        'LSTM_layers': {\n",
    "            'distribution': 'int_uniform',\n",
    "            'max': 8, 'min': 2},\n",
    "        'MLP_layers': {\n",
    "            'distribution': 'int_uniform',\n",
    "            'max': 8, 'min': 2},\n",
    "        'Bn_momentum': {\n",
    "            'values': [0.1, 0.4, 0.7, 0.99]},\n",
    "        'Lr': {\n",
    "            'distribution': 'log_uniform',\n",
    "            'max': log(0.1), 'min': log(0.0001)},\n",
    "        'Lr_decay': {\n",
    "            'distribution': 'log_uniform',\n",
    "            'max': log(1), 'min': log(0.8)},        \n",
    "        'Minibatch_size': {\n",
    "            'value': 4096},\n",
    "        'Min_delta': {\n",
    "            'value': 0.01 if moneyness else 1},\n",
    "        'Patience': {\n",
    "            'value': 20},\n",
    "        'Num_features': {\n",
    "            'value': 3 if moneyness else 4},\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize sweep and creating sweepID\n",
    "\n",
    "# If new sweep, uncomment the line below and comment the line after it\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project='Deep learning for option pricing') \n",
    "#sweep_id = '98bxt6oq'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WIP\n",
    "class MSE_LossCallback(Callback):\n",
    "    def __init__(self, train_x, train_y, train_strike, val_x, val_y, val_strike):\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "        self.train_strike = train_strike\n",
    "        self.val_x = val_x\n",
    "        self.val_y = val_y\n",
    "        self.val_strike = val_strike\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        train_pred = self.model(train_x)\n",
    "        val_pred = self.model(val_x)\n",
    "\n",
    "        train_mse = reduce_mean(square(multiply(train_pred[:,0] - self.train_y, self.train_strike)))\n",
    "        val_mse = reduce_mean(square(multiply(val_pred[:,0] - self.val_y, self.val_strike)))\n",
    "\n",
    "        print(f' Training scaled MSE: {train_mse}, Validation scaled MSE: {val_mse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the training and validation MSE loss on the actual option price when using price/strike as the target\n",
    "def MSE_loss(model, train_x, train_y, train_strike, val_x, val_y, val_strike):\n",
    "    train_pred = model(train_x)\n",
    "    val_pred = model(val_x)\n",
    "\n",
    "    train_mse = reduce_mean(square((train_pred[:,0] - train_y)*train_strike))\n",
    "    val_mse = reduce_mean(square((val_pred[:,0] - val_y)*val_strike))\n",
    "\n",
    "    print(f' Training scaled MSE: {train_mse}, Validation scaled MSE: {val_mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from tensorflow.keras import backend as k\n",
    "\n",
    "class ClearMemory(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        gc.collect()\n",
    "        k.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(train_x = train_x, train_y = train_y, val_x = val_x, val_y = val_y, config = None, project = None):\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config, project = project):\n",
    "\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "\n",
    "        # Build model and create callbacks\n",
    "        model = create_model(config)\n",
    "\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            min_delta = config.Min_delta,\n",
    "            patience = config.Patience,\n",
    "        )\n",
    "        \n",
    "        wandb_callback = WandbCallback(\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            save_model=False\n",
    "        )\n",
    "\n",
    "        if moneyness:\n",
    "            mse_callback = MSE_LossCallback(train_x, train_y, train_strike, val_x, val_y, val_strike)\n",
    "\n",
    "        # Adapt sequence length to config\n",
    "        train_x_adjusted = [train_x[0][:, :config.LSTM_timesteps, :], train_x[1]]\n",
    "        val_x_adjusted = [val_x[0][:, :config.LSTM_timesteps, :], val_x[1]]\n",
    "        print(f'Train shape: {train_x_adjusted[0].shape}, {train_x_adjusted[0].shape}')\n",
    "        print(f'Val shape: {val_x_adjusted[0].shape}, {val_x_adjusted[0].shape}')\n",
    "\n",
    "        # Train model\n",
    "        model.fit(\n",
    "            train_x_adjusted,\n",
    "            train_y,\n",
    "            batch_size = config.Minibatch_size,\n",
    "            validation_data = (val_x_adjusted, val_y),\n",
    "            epochs = 1000,\n",
    "            callbacks = [early_stopping, wandb_callback, mse_callback, ClearMemory()] if moneyness else [early_stopping, wandb_callback],\n",
    "        )\n",
    "\n",
    "        if moneyness:\n",
    "            MSE_loss(model, train_x, train_y, train_strike, val_x, val_y, val_strike)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run full sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: s3bytpff with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBn_momentum: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_layers: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_timesteps: 90\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_units: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLr: 0.0005854385979916059\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLr_decay: 0.8409993709669317\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tMLP_layers: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tMLP_units: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tMin_delta: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tMinibatch_size: 4096\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tNum_features: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tPatience: 20\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/hjalmarjacobvinje/Documents/deep-learning-for-option-pricing/LSTM-MLP_notebooks/wandb/run-20230214_163615-s3bytpff</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/s3bytpff\" target=\"_blank\">stoic-sweep-1</a></strong> to <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page: <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/sweeps/tx3zgah9\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/sweeps/tx3zgah9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/sweeps/tx3zgah9\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/sweeps/tx3zgah9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/s3bytpff\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/s3bytpff</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-14 16:36:16.694116: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 90, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 16)           11712       ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 20)           0           ['sequential[0][0]',             \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 50)           1050        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 50)          200         ['dense[0][0]']                  \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 50)           0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 50)           2550        ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 50)          200         ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 50)           0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 50)           2550        ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 50)          200         ['dense_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 50)           0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 50)           2550        ['leaky_re_lu_2[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 50)          200         ['dense_3[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)      (None, 50)           0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 50)           2550        ['leaky_re_lu_3[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 50)          200         ['dense_4[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)      (None, 50)           0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 50)           2550        ['leaky_re_lu_4[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 50)          200         ['dense_5[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)      (None, 50)           0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 1)            51          ['leaky_re_lu_5[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 26,763\n",
      "Trainable params: 26,163\n",
      "Non-trainable params: 600\n",
      "__________________________________________________________________________________________________\n",
      "Train shape: (1841485, 90, 1), (1841485, 90, 1)\n",
      "Val shape: (202710, 90, 1), (202710, 90, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ConnectionError), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ConnectionError), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/450 [..............................] - ETA: 100:48:55 - loss: 1122298.1250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ConnectionError), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11/450 [..............................] - ETA: 1:25:03 - loss: 1092256.2500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ConnectionError), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 92/450 [=====>........................] - ETA: 2:22:47 - loss: 1083886.5000"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id=sweep_id, function=trainer, project='Deep learning for option pricing - test area', count = 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'LSTM_units': 8,\n",
    "    'MLP_units': 100,\n",
    "    'LSTM_timesteps': 20,\n",
    "    'LSTM_layers': 3,\n",
    "    'MLP_layers': 4,\n",
    "    'Bn_momentum': 0.99,\n",
    "    'Lr': 0.01,\n",
    "    'Lr_decay': 1,\n",
    "    'Minibatch_size': 4096,\n",
    "    'Min_delta': 0.01 if moneyness else 1,\n",
    "    'Patience': 20,\n",
    "    'Num_features': 3 if moneyness else 4, \n",
    "    'Architecture': 'LSTM-MLP v.0.1',\n",
    "    'Dataset': f'Moneyness. Train_start: {train_start}, val_start: {val_start}, test_start: {test_start}',\n",
    "}\n",
    "trainer(config = config, project = 'Deep learning for option pricing - test area')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
