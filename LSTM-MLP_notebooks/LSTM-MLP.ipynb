{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights and Biases\n",
    "#!pip install -q wandb\n",
    "# Tensorflow\n",
    "#!pip install -q tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, LSTM, Concatenate, Dense, BatchNormalization, LeakyReLU\n",
    "from keras.activations import tanh\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from tensorflow import square, reduce_mean\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.math import multiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33merlendrygg\u001b[0m (\u001b[33mavogadro\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Erlend/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If running in colab, insert your wandb key here\n",
    "\n",
    "import config\n",
    "#Erlend\n",
    "wandb.login(key=config.erlend_key)\n",
    "# Hjalmar\n",
    "#wandb.login(key=config.hjalmar_key)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, split and normalize data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_colab = False\n",
    "\n",
    "if google_colab:\n",
    "    import tensorflow as tf\n",
    "    # Pring info\n",
    "    gpu_info = !nvidia-smi\n",
    "    gpu_info = '\\n'.join(gpu_info)\n",
    "    if gpu_info.find('failed') >= 0:\n",
    "        print('Not connected to a GPU')\n",
    "    else:\n",
    "        print(gpu_info)\n",
    "    \n",
    "    from psutil import virtual_memory\n",
    "    ram_gb = virtual_memory().total / 1e9\n",
    "    print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "    if ram_gb < 20:\n",
    "        print('Not using a high-RAM runtime')\n",
    "    else:\n",
    "        print('You are using a high-RAM runtime!')\n",
    "\n",
    "    # Code to read csv file into Colaboratory:\n",
    "    !pip install -U -q PyDrive\n",
    "    from pydrive.auth import GoogleAuth\n",
    "    from pydrive.drive import GoogleDrive\n",
    "    from google.colab import auth\n",
    "    from oauth2client.client import GoogleCredentials\n",
    "    # Authenticate and create the PyDrive client.\n",
    "    auth.authenticate_user()\n",
    "    gauth = GoogleAuth()\n",
    "    gauth.credentials = GoogleCredentials.get_application_default()\n",
    "    drive = GoogleDrive(gauth)\n",
    "    id = \"1tSZeYgiZh_gL6CKUYufNlb1C6X4vODY3\"\n",
    "    downloaded = drive.CreateFile({'id':id}) \n",
    "    downloaded.GetContentFile('2021_2022.csv')  \n",
    "    df_read = pd.read_csv('2021_2022.csv')\n",
    "else:\n",
    "    file = \"../data/processed_data/2021_2022.csv\"\n",
    "    df_read = pd.read_csv(file)\n",
    "\n",
    "display(df_read)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (521839, 90, 1), (521839, 3)\n",
      "Val shape: (158533, 90, 1), (158533, 3)\n"
     ]
    }
   ],
   "source": [
    "df = df_read\n",
    "\n",
    "# Format settings\n",
    "max_timesteps = 90\n",
    "moneyness = True # Moneyness = True WIP\n",
    "bs_vars = ['Moneyness', 'TTM', 'R'] if moneyness else ['Underlying_last', 'Strike', 'TTM', 'R']\n",
    "underlying_lags = ['Underlying_last'] + [f'Underlying_{i}' for i in range (1, max_timesteps)]\n",
    "\n",
    "# Create train, validation and test set split points\n",
    "train_start = datetime(2021,6,1) # Should be (2021, 1, 1)\n",
    "val_start = train_start + relativedelta(months=3) # NB! Should be 11 months. Reduced to 3 for testing\n",
    "test_start = val_start + relativedelta(months=1)\n",
    "test_end = test_start + relativedelta(months=1)\n",
    "train_start = str(train_start.date())\n",
    "val_start = str(val_start.date())\n",
    "test_start = str(test_start.date())\n",
    "test_end = str(test_end.date())\n",
    "\n",
    "# Split train and validation data\n",
    "df_train = df[(df['Quote_date'] >= train_start) & ( df['Quote_date'] < val_start)]\n",
    "df_val = df[(df['Quote_date'] >= val_start) & ( df['Quote_date'] < test_start)]\n",
    "\n",
    "# Extract target values\n",
    "train_y = (df_train['Price']/df_train['Strike']).to_numpy() if moneyness else df_train['Price'].to_numpy()\n",
    "val_y = (df_val['Price']/df_val['Strike']).to_numpy() if moneyness else df_val['Price'].to_numpy()\n",
    "\n",
    "# If usining moneyness, extract strike\n",
    "if moneyness:\n",
    "    train_strike = df_train['Strike'].to_numpy()\n",
    "    val_strike = df_val['Strike'].to_numpy()\n",
    "\n",
    "# Convert dataframes to numpy arrays\n",
    "train_x = [df_train[underlying_lags].to_numpy(), df_train[bs_vars].to_numpy()]\n",
    "val_x = [df_val[underlying_lags].to_numpy(), df_val[bs_vars].to_numpy()]\n",
    "\n",
    "# Scale features based on training set\n",
    "underlying_scaler = MinMaxScaler()\n",
    "train_x[0] = underlying_scaler.fit_transform(train_x[0])\n",
    "val_x[0] = underlying_scaler.transform(val_x[0])\n",
    "\n",
    "bs_scaler = MinMaxScaler()\n",
    "train_x[1] = bs_scaler.fit_transform(train_x[1])\n",
    "val_x[1] = bs_scaler.transform(val_x[1])\n",
    "\n",
    "# Shuffle training set\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(len(train_x[0]))\n",
    "train_x = [train_x[0][shuffle], train_x[1][shuffle]]\n",
    "train_y = train_y[shuffle]\n",
    "if moneyness:\n",
    "    train_strike = train_strike[shuffle]\n",
    "\n",
    "# Reshape data to fit LSTM\n",
    "train_x = [train_x[0].reshape(len(train_x[0]), max_timesteps,1), train_x[1]]\n",
    "val_x = [val_x[0].reshape(len(val_x[0]), max_timesteps, 1), val_x[1]]\n",
    "\n",
    "print(f'Train shape: {train_x[0].shape}, {train_x[1].shape}')\n",
    "print(f'Val shape: {val_x[0].shape}, {val_x[1].shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(config):\n",
    "    '''Builds an LSTM-MLP model of minimum 2 layers sequentially from a given config dictionary'''\n",
    "\n",
    "    # Input layers\n",
    "    underlying_history = Input((config.LSTM_timesteps,1))\n",
    "    bs_vars = Input((config.Num_features,))\n",
    "\n",
    "    # LSTM layers\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(\n",
    "        units = config.LSTM_units,\n",
    "        activation = tanh,\n",
    "        input_shape = (config.LSTM_timesteps, 1),\n",
    "        return_sequences = True\n",
    "    ))\n",
    "\n",
    "    for _ in range(config.LSTM_layers - 2):\n",
    "        model.add(LSTM(\n",
    "            units = config.LSTM_units,\n",
    "            activation = tanh,\n",
    "            return_sequences = True\n",
    "        ))\n",
    "    \n",
    "    model.add(LSTM(\n",
    "        units = config.LSTM_units,\n",
    "        activation = tanh,\n",
    "        return_sequences = False\n",
    "    ))\n",
    "\n",
    "    # MLP layers\n",
    "    layers = Concatenate()([model(underlying_history), bs_vars])\n",
    "    \n",
    "    for _ in range(config.MLP_layers - 1):\n",
    "        layers = Dense(config.MLP_units)(layers)\n",
    "        layers = BatchNormalization(momentum=config.Bn_momentum)(layers)\n",
    "        layers = LeakyReLU()(layers)\n",
    "\n",
    "    output = Dense(1, activation='relu')(layers)\n",
    "\n",
    "    # Exponential decaying learning rate\n",
    "    lr_schedule = ExponentialDecay(\n",
    "        initial_learning_rate = config.Lr,\n",
    "        decay_steps = int(len(train_x[0])/config.Minibatch_size),\n",
    "        decay_rate=config.Lr_decay\n",
    "    )\n",
    "\n",
    "    # Compile model\n",
    "    model = Model(inputs=[underlying_history, bs_vars], outputs=output)\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=lr_schedule))\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter search setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: eex5s5of\n",
      "Sweep URL: https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/sweeps/eex5s5of\n"
     ]
    }
   ],
   "source": [
    "# Configuring the sweep hyperparameter search space\n",
    "sweep_configuration = {\n",
    "    'method': 'random',\n",
    "    'name': 'LSTM-MLP v.1.0 testing : 21-22 data',\n",
    "    'metric': {\n",
    "        'goal': 'minimize', \n",
    "        'name': 'val_loss'\n",
    "\t\t},\n",
    "    'parameters': {\n",
    "        'LSTM_units': {\n",
    "            'values': [8, 16, 32, 64, 96, 128]},\n",
    "        'MLP_units': {\n",
    "            'values': [32, 64, 96, 128]},\n",
    "        'LSTM_timesteps': {\n",
    "            'values': [20]},\n",
    "        'LSTM_layers': {\n",
    "            'distribution': 'int_uniform',\n",
    "            'max': 6, 'min': 2},\n",
    "        'MLP_layers': {\n",
    "            'distribution': 'int_uniform',\n",
    "            'max': 6, 'min': 2},\n",
    "        'Bn_momentum': {\n",
    "            'distribution': 'uniform',\n",
    "            'max': 1, 'min': 0},\n",
    "        'Lr': {\n",
    "            'distribution': 'uniform',\n",
    "            'max': 0.01, 'min': 0.0001},\n",
    "        'Lr_decay': {\n",
    "            'distribution': 'uniform',\n",
    "            'max': 1, 'min': 0.9},        \n",
    "        'Minibatch_size': {\n",
    "            'values': [1024, 2048, 4096]},\n",
    "        'Min_delta': {\n",
    "            'value': 1},\n",
    "        'Patience': {\n",
    "            'value': 20},\n",
    "        'Num_features': {\n",
    "            'value': 4},\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize sweep and creating sweepID\n",
    "\n",
    "# If new sweep, uncomment the line below and comment the line after it\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project='Deep learning for option pricing - test area') \n",
    "#sweep_id = '98bxt6oq'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WIP\n",
    "class MSE_LossCallback(Callback):\n",
    "    def __init__(self, train_x, train_y, train_strike, val_x, val_y, val_strike):\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "        self.train_strike = train_strike\n",
    "        self.val_x = val_x\n",
    "        self.val_y = val_y\n",
    "        self.val_strike = val_strike\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        train_pred = self.model(train_x)\n",
    "        val_pred = self.model(val_x)\n",
    "\n",
    "        train_mse = reduce_mean(square(multiply(train_pred[:,0] - self.train_y, self.train_strike)))\n",
    "        val_mse = reduce_mean(square(multiply(val_pred[:,0] - self.val_y, self.val_strike)))\n",
    "\n",
    "        print(f' Training scaled MSE: {train_mse}, Validation scaled MSE: {val_mse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the training and validation MSE loss on the actual option price when using price/strike as the target\n",
    "def MSE_loss(model, train_x, train_y, train_strike, val_x, val_y, val_strike):\n",
    "    train_pred = model(train_x)\n",
    "    val_pred = model(val_x)\n",
    "\n",
    "    train_mse = reduce_mean(square((train_pred[:,0] - train_y)*train_strike))\n",
    "    val_mse = reduce_mean(square((val_pred[:,0] - val_y)*val_strike))\n",
    "\n",
    "    print(f' Training scaled MSE: {train_mse}, Validation scaled MSE: {val_mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(train_x = train_x, train_y = train_y, val_x = val_x, val_y = val_y, config = None, project = None):\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config, project = project):\n",
    "\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "\n",
    "        # Build model and create callbacks\n",
    "        model = create_model(config)\n",
    "\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            min_delta = config.Min_delta,\n",
    "            patience = config.Patience,\n",
    "        )\n",
    "        \n",
    "        wandb_callback = WandbCallback(\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            save_model=False\n",
    "        )\n",
    "\n",
    "        if moneyness:\n",
    "            mse_callback = MSE_LossCallback(train_x, train_y, train_strike, val_x, val_y, val_strike)\n",
    "\n",
    "        # Adapt sequence length to config\n",
    "        train_x[0] = train_x[0][:, :config.LSTM_timesteps, :]\n",
    "        val_x[0] = val_x[0][:, :config.LSTM_timesteps, :]\n",
    "        print(f'Train shape: {train_x[0].shape}, {train_x[1].shape}')\n",
    "        print(f'Val shape: {val_x[0].shape}, {val_x[1].shape}')\n",
    "\n",
    "        # Train model\n",
    "        model.fit(\n",
    "            train_x,\n",
    "            train_y,\n",
    "            batch_size = config.Minibatch_size,\n",
    "            validation_data = (val_x, val_y),\n",
    "            epochs = 1000,\n",
    "            callbacks = [early_stopping, wandb_callback, mse_callback] if moneyness else [early_stopping, wandb_callback],\n",
    "        )\n",
    "\n",
    "        if moneyness:\n",
    "            MSE_loss(model, train_x, train_y, train_strike, val_x, val_y, val_strike)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run full sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: x1nwz702 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBn_momentum: 0.2115311974369981\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_layers: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_timesteps: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_units: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLr: 0.007252310655150062\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLr_decay: 0.931385828545152\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tMLP_layers: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tMLP_units: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tMin_delta: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tMinibatch_size: 1024\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tNum_features: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tPatience: 20\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id=sweep_id, function=trainer, project='Deep learning for option pricing - test area', count = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.13.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Erlend\\Google Drive\\NTNU\\5. Klasse\\Master\\Git-folder\\deep-learning-for-option-pricing\\LSTM-MLP_notebooks\\wandb\\run-20230211_153853-1ne6d58f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/1ne6d58f\" target=\"_blank\">sage-music-147</a></strong> to <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 20, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 8)            1408        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 11)           0           ['sequential[0][0]',             \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 100)          1200        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 100)         400         ['dense[0][0]']                  \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 100)          0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 100)          10100       ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 100)         400         ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 100)          0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 100)          10100       ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 100)         400         ['dense_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 100)          0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            101         ['leaky_re_lu_2[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 24,109\n",
      "Trainable params: 23,509\n",
      "Non-trainable params: 600\n",
      "__________________________________________________________________________________________________\n",
      "Train shape: (521839, 20, 1), (521839, 3)\n",
      "Val shape: (158533, 20, 1), (158533, 3)\n",
      "Epoch 1/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.0793 Training scaled MSE: 4428290.0, Validation scaled MSE: 4795840.0\n",
      "128/128 [==============================] - 48s 316ms/step - loss: 0.0793 - val_loss: 1.8144\n",
      "Epoch 2/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.0181 Training scaled MSE: 3197012.5, Validation scaled MSE: 3410441.0\n",
      "128/128 [==============================] - 39s 306ms/step - loss: 0.0181 - val_loss: 1.5036\n",
      "Epoch 3/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.0097 Training scaled MSE: 1542429.375, Validation scaled MSE: 1557186.25\n",
      "128/128 [==============================] - 38s 301ms/step - loss: 0.0097 - val_loss: 1.0881\n",
      "Epoch 4/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.0126 Training scaled MSE: 319144.375, Validation scaled MSE: 287697.75\n",
      "128/128 [==============================] - 38s 301ms/step - loss: 0.0126 - val_loss: 0.4787\n",
      "Epoch 5/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.0142 Training scaled MSE: 758565.1875, Validation scaled MSE: 759274.125\n",
      "128/128 [==============================] - 38s 301ms/step - loss: 0.0142 - val_loss: 0.1331\n",
      "Epoch 6/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.0139 Training scaled MSE: 495456.3125, Validation scaled MSE: 498487.34375\n",
      "128/128 [==============================] - 38s 296ms/step - loss: 0.0139 - val_loss: 0.0364\n",
      "Epoch 7/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.0105 Training scaled MSE: 256220.765625, Validation scaled MSE: 237797.96875\n",
      "128/128 [==============================] - 41s 319ms/step - loss: 0.0105 - val_loss: 0.0245\n",
      "Epoch 8/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.0081 Training scaled MSE: 492308.84375, Validation scaled MSE: 395289.75\n",
      "128/128 [==============================] - 40s 312ms/step - loss: 0.0081 - val_loss: 0.0436\n",
      "Epoch 9/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.0102 Training scaled MSE: 365448.21875, Validation scaled MSE: 343479.21875\n",
      "128/128 [==============================] - 38s 294ms/step - loss: 0.0102 - val_loss: 0.0429\n",
      "Epoch 10/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.0080 Training scaled MSE: 205618.8125, Validation scaled MSE: 179384.53125\n",
      "128/128 [==============================] - 39s 307ms/step - loss: 0.0080 - val_loss: 0.0256\n",
      "Epoch 11/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.0158 Training scaled MSE: 120398.2421875, Validation scaled MSE: 110220.7890625\n",
      "128/128 [==============================] - 39s 305ms/step - loss: 0.0158 - val_loss: 0.0121\n",
      "Epoch 12/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.0112 Training scaled MSE: 2228946.0, Validation scaled MSE: 2773353.75\n",
      "128/128 [==============================] - 75s 589ms/step - loss: 0.0112 - val_loss: 0.2163\n",
      "Epoch 13/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.0090 Training scaled MSE: 463721.03125, Validation scaled MSE: 484427.375\n",
      "128/128 [==============================] - 85s 665ms/step - loss: 0.0090 - val_loss: 0.0464\n",
      "Epoch 14/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.0125 Training scaled MSE: 445021.90625, Validation scaled MSE: 418267.78125\n",
      "128/128 [==============================] - 77s 607ms/step - loss: 0.0125 - val_loss: 0.0606\n",
      "Epoch 15/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.0084 Training scaled MSE: 278925.15625, Validation scaled MSE: 207774.296875\n",
      "128/128 [==============================] - 68s 535ms/step - loss: 0.0084 - val_loss: 0.0171\n",
      "Epoch 16/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.0148 Training scaled MSE: 118345.2734375, Validation scaled MSE: 124012.7890625\n",
      "128/128 [==============================] - 82s 643ms/step - loss: 0.0148 - val_loss: 0.0435\n",
      "Epoch 17/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.0060 Training scaled MSE: 19204.974609375, Validation scaled MSE: 18847.482421875\n",
      "128/128 [==============================] - 82s 641ms/step - loss: 0.0060 - val_loss: 0.0022\n",
      "Epoch 18/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.0074 Training scaled MSE: 1106102.0, Validation scaled MSE: 1000610.125\n",
      "128/128 [==============================] - 69s 539ms/step - loss: 0.0074 - val_loss: 0.0866\n",
      "Epoch 19/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.0077 Training scaled MSE: 798646.3125, Validation scaled MSE: 695041.125\n",
      "128/128 [==============================] - 68s 537ms/step - loss: 0.0077 - val_loss: 0.0492\n",
      "Epoch 20/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.0103 Training scaled MSE: 3861658.5, Validation scaled MSE: 3796034.75\n",
      "128/128 [==============================] - 75s 586ms/step - loss: 0.0103 - val_loss: 0.2382\n",
      "Epoch 21/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.0083 Training scaled MSE: 1413233.875, Validation scaled MSE: 1361596.75\n",
      "128/128 [==============================] - 61s 481ms/step - loss: 0.0083 - val_loss: 0.0959\n",
      "Epoch 22/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.0082 Training scaled MSE: 400893.625, Validation scaled MSE: 360857.8125\n",
      "128/128 [==============================] - 48s 375ms/step - loss: 0.0082 - val_loss: 0.0463\n",
      "Epoch 23/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.0076 Training scaled MSE: 792900.25, Validation scaled MSE: 724372.9375\n",
      "128/128 [==============================] - 43s 337ms/step - loss: 0.0076 - val_loss: 0.0576\n",
      "Epoch 24/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.0084 Training scaled MSE: 496112.5625, Validation scaled MSE: 459074.96875\n",
      "128/128 [==============================] - 44s 343ms/step - loss: 0.0084 - val_loss: 0.0710\n",
      "Epoch 25/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.0057 Training scaled MSE: 1345559.25, Validation scaled MSE: 1431844.625\n",
      "128/128 [==============================] - 46s 358ms/step - loss: 0.0057 - val_loss: 0.0992\n",
      "Epoch 26/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.0044 Training scaled MSE: 1596380.375, Validation scaled MSE: 1710435.375\n",
      "128/128 [==============================] - 40s 311ms/step - loss: 0.0044 - val_loss: 0.1177\n",
      "Epoch 27/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.0055 Training scaled MSE: 308140.0, Validation scaled MSE: 253670.96875\n",
      "128/128 [==============================] - 47s 367ms/step - loss: 0.0055 - val_loss: 0.0215\n",
      "Epoch 28/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.0074 Training scaled MSE: 546939.125, Validation scaled MSE: 461359.78125\n",
      "128/128 [==============================] - 39s 305ms/step - loss: 0.0074 - val_loss: 0.0547\n",
      "Epoch 29/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.0082 Training scaled MSE: 143800.390625, Validation scaled MSE: 139941.078125\n",
      "128/128 [==============================] - 43s 334ms/step - loss: 0.0082 - val_loss: 0.0271\n",
      "Epoch 30/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.0052 Training scaled MSE: 214011.21875, Validation scaled MSE: 199686.03125\n",
      "128/128 [==============================] - 40s 313ms/step - loss: 0.0052 - val_loss: 0.0248\n",
      "Epoch 31/1000\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.0057 Training scaled MSE: 154562.0, Validation scaled MSE: 142122.953125\n",
      "128/128 [==============================] - 37s 291ms/step - loss: 0.0057 - val_loss: 0.0129\n",
      " Training scaled MSE: 154562.0, Validation scaled MSE: 142122.953125\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>loss</td><td>█▂▁▂▂▂▂▁▂▁▂▂▁▂▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▅▃▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>16</td></tr><tr><td>best_val_loss</td><td>0.00217</td></tr><tr><td>epoch</td><td>30</td></tr><tr><td>loss</td><td>0.00569</td></tr><tr><td>val_loss</td><td>0.01294</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">sage-music-147</strong>: <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/1ne6d58f\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/1ne6d58f</a><br/>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230211_153853-1ne6d58f\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = {\n",
    "    'LSTM_units': 8,\n",
    "    'MLP_units': 100,\n",
    "    'LSTM_timesteps': 20,\n",
    "    'LSTM_layers': 3,\n",
    "    'MLP_layers': 4,\n",
    "    'Bn_momentum': 0.99,\n",
    "    'Lr': 0.01,\n",
    "    'Lr_decay': 1,\n",
    "    'Minibatch_size': 4096,\n",
    "    'Min_delta': 0.01 if moneyness else 50,\n",
    "    'Patience': 20,\n",
    "    'Num_features': 3 if moneyness else 4, \n",
    "    'Architecture': 'LSTM-MLP v.0.1',\n",
    "    'Dataset': f'Moneyness. Train_start: {train_start}, val_start: {val_start}, test_start: {test_start}',\n",
    "}\n",
    "trainer(config = config, project = 'Deep learning for option pricing - test area')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d22c5043c0ee61c8547bd88adb0f85d3d6a0a630c1da4282026f99271dac814"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
