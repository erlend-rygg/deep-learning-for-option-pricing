{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights and Biases\n",
    "#!pip install -q wandb\n",
    "# Tensorflow\n",
    "#!pip install -q tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, LSTM, Concatenate, Dense, BatchNormalization, LeakyReLU\n",
    "from keras.activations import tanh\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvinje\u001b[0m (\u001b[33mavogadro\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/hjalmarjacobvinje/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import config\n",
    "#Erlend\n",
    "#wandb.login(key=config.erlend_key)\n",
    "# Hjalmar\n",
    "wandb.login(key=config.hjalmar_key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, split and normalize data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Quote_date</th>\n",
       "      <th>Expire_date</th>\n",
       "      <th>Price</th>\n",
       "      <th>Underlying_last</th>\n",
       "      <th>Strike</th>\n",
       "      <th>TTM</th>\n",
       "      <th>Moneyness</th>\n",
       "      <th>Underlying_return</th>\n",
       "      <th>Underlying_1</th>\n",
       "      <th>...</th>\n",
       "      <th>Underlying_82</th>\n",
       "      <th>Underlying_83</th>\n",
       "      <th>Underlying_84</th>\n",
       "      <th>Underlying_85</th>\n",
       "      <th>Underlying_86</th>\n",
       "      <th>Underlying_87</th>\n",
       "      <th>Underlying_88</th>\n",
       "      <th>Underlying_89</th>\n",
       "      <th>Underlying_90</th>\n",
       "      <th>R</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>721776</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>3002.45</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.000990</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>721777</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>2800.60</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.334158</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>721778</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>2602.20</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.857850</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>721779</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>2402.45</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.500619</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>721780</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>2202.45</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.222772</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>721781</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>1998.15</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.000495</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>721782</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>1799.25</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.818632</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>721783</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>1601.70</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.667079</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>721784</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>1402.25</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>2600.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.538842</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>721785</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>1202.45</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>2800.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.428925</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>721786</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>1004.55</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.333663</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>721787</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>902.30</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>3100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.290642</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>721788</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>802.30</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.250309</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>721789</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>702.50</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>3300.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.212421</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>721790</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>602.70</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>3400.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.176762</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>721791</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>503.95</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.143140</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>721792</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>453.95</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>3550.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.127039</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>721793</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>403.60</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.111386</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>721794</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>353.70</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>3650.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.096162</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>721795</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>328.75</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>3675.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.088705</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>721796</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>303.75</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>3700.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.081349</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>721797</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>283.75</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>3720.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.075535</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>721798</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>278.75</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>3725.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.074091</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>721799</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>273.85</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>3730.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.072651</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>721800</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>263.85</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>3740.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.069783</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>721801</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>254.15</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.066931</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>721802</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>244.15</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>3760.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.064093</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>721803</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>234.20</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>3770.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.061271</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>721804</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>229.25</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>3775.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.059865</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>721805</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>224.25</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>3780.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.058463</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  Quote_date Expire_date    Price  Underlying_last  Strike  TTM  \\\n",
       "0       721776  2022-05-10  2022-05-11  3002.45          4000.99  1000.0    1   \n",
       "1       721777  2022-05-10  2022-05-11  2800.60          4000.99  1200.0    1   \n",
       "2       721778  2022-05-10  2022-05-11  2602.20          4000.99  1400.0    1   \n",
       "3       721779  2022-05-10  2022-05-11  2402.45          4000.99  1600.0    1   \n",
       "4       721780  2022-05-10  2022-05-11  2202.45          4000.99  1800.0    1   \n",
       "5       721781  2022-05-10  2022-05-11  1998.15          4000.99  2000.0    1   \n",
       "6       721782  2022-05-10  2022-05-11  1799.25          4000.99  2200.0    1   \n",
       "7       721783  2022-05-10  2022-05-11  1601.70          4000.99  2400.0    1   \n",
       "8       721784  2022-05-10  2022-05-11  1402.25          4000.99  2600.0    1   \n",
       "9       721785  2022-05-10  2022-05-11  1202.45          4000.99  2800.0    1   \n",
       "10      721786  2022-05-10  2022-05-11  1004.55          4000.99  3000.0    1   \n",
       "11      721787  2022-05-10  2022-05-11   902.30          4000.99  3100.0    1   \n",
       "12      721788  2022-05-10  2022-05-11   802.30          4000.99  3200.0    1   \n",
       "13      721789  2022-05-10  2022-05-11   702.50          4000.99  3300.0    1   \n",
       "14      721790  2022-05-10  2022-05-11   602.70          4000.99  3400.0    1   \n",
       "15      721791  2022-05-10  2022-05-11   503.95          4000.99  3500.0    1   \n",
       "16      721792  2022-05-10  2022-05-11   453.95          4000.99  3550.0    1   \n",
       "17      721793  2022-05-10  2022-05-11   403.60          4000.99  3600.0    1   \n",
       "18      721794  2022-05-10  2022-05-11   353.70          4000.99  3650.0    1   \n",
       "19      721795  2022-05-10  2022-05-11   328.75          4000.99  3675.0    1   \n",
       "20      721796  2022-05-10  2022-05-11   303.75          4000.99  3700.0    1   \n",
       "21      721797  2022-05-10  2022-05-11   283.75          4000.99  3720.0    1   \n",
       "22      721798  2022-05-10  2022-05-11   278.75          4000.99  3725.0    1   \n",
       "23      721799  2022-05-10  2022-05-11   273.85          4000.99  3730.0    1   \n",
       "24      721800  2022-05-10  2022-05-11   263.85          4000.99  3740.0    1   \n",
       "25      721801  2022-05-10  2022-05-11   254.15          4000.99  3750.0    1   \n",
       "26      721802  2022-05-10  2022-05-11   244.15          4000.99  3760.0    1   \n",
       "27      721803  2022-05-10  2022-05-11   234.20          4000.99  3770.0    1   \n",
       "28      721804  2022-05-10  2022-05-11   229.25          4000.99  3775.0    1   \n",
       "29      721805  2022-05-10  2022-05-11   224.25          4000.99  3780.0    1   \n",
       "\n",
       "    Moneyness  Underlying_return  Underlying_1  ...  Underlying_82  \\\n",
       "0    4.000990               7.73       -130.56  ...           1.57   \n",
       "1    3.334158               7.73       -130.56  ...           1.57   \n",
       "2    2.857850               7.73       -130.56  ...           1.57   \n",
       "3    2.500619               7.73       -130.56  ...           1.57   \n",
       "4    2.222772               7.73       -130.56  ...           1.57   \n",
       "5    2.000495               7.73       -130.56  ...           1.57   \n",
       "6    1.818632               7.73       -130.56  ...           1.57   \n",
       "7    1.667079               7.73       -130.56  ...           1.57   \n",
       "8    1.538842               7.73       -130.56  ...           1.57   \n",
       "9    1.428925               7.73       -130.56  ...           1.57   \n",
       "10   1.333663               7.73       -130.56  ...           1.57   \n",
       "11   1.290642               7.73       -130.56  ...           1.57   \n",
       "12   1.250309               7.73       -130.56  ...           1.57   \n",
       "13   1.212421               7.73       -130.56  ...           1.57   \n",
       "14   1.176762               7.73       -130.56  ...           1.57   \n",
       "15   1.143140               7.73       -130.56  ...           1.57   \n",
       "16   1.127039               7.73       -130.56  ...           1.57   \n",
       "17   1.111386               7.73       -130.56  ...           1.57   \n",
       "18   1.096162               7.73       -130.56  ...           1.57   \n",
       "19   1.088705               7.73       -130.56  ...           1.57   \n",
       "20   1.081349               7.73       -130.56  ...           1.57   \n",
       "21   1.075535               7.73       -130.56  ...           1.57   \n",
       "22   1.074091               7.73       -130.56  ...           1.57   \n",
       "23   1.072651               7.73       -130.56  ...           1.57   \n",
       "24   1.069783               7.73       -130.56  ...           1.57   \n",
       "25   1.066931               7.73       -130.56  ...           1.57   \n",
       "26   1.064093               7.73       -130.56  ...           1.57   \n",
       "27   1.061271               7.73       -130.56  ...           1.57   \n",
       "28   1.059865               7.73       -130.56  ...           1.57   \n",
       "29   1.058463               7.73       -130.56  ...           1.57   \n",
       "\n",
       "    Underlying_83  Underlying_84  Underlying_85  Underlying_86  Underlying_87  \\\n",
       "0          -65.91          13.02          43.68          -6.56         -19.84   \n",
       "1          -65.91          13.02          43.68          -6.56         -19.84   \n",
       "2          -65.91          13.02          43.68          -6.56         -19.84   \n",
       "3          -65.91          13.02          43.68          -6.56         -19.84   \n",
       "4          -65.91          13.02          43.68          -6.56         -19.84   \n",
       "5          -65.91          13.02          43.68          -6.56         -19.84   \n",
       "6          -65.91          13.02          43.68          -6.56         -19.84   \n",
       "7          -65.91          13.02          43.68          -6.56         -19.84   \n",
       "8          -65.91          13.02          43.68          -6.56         -19.84   \n",
       "9          -65.91          13.02          43.68          -6.56         -19.84   \n",
       "10         -65.91          13.02          43.68          -6.56         -19.84   \n",
       "11         -65.91          13.02          43.68          -6.56         -19.84   \n",
       "12         -65.91          13.02          43.68          -6.56         -19.84   \n",
       "13         -65.91          13.02          43.68          -6.56         -19.84   \n",
       "14         -65.91          13.02          43.68          -6.56         -19.84   \n",
       "15         -65.91          13.02          43.68          -6.56         -19.84   \n",
       "16         -65.91          13.02          43.68          -6.56         -19.84   \n",
       "17         -65.91          13.02          43.68          -6.56         -19.84   \n",
       "18         -65.91          13.02          43.68          -6.56         -19.84   \n",
       "19         -65.91          13.02          43.68          -6.56         -19.84   \n",
       "20         -65.91          13.02          43.68          -6.56         -19.84   \n",
       "21         -65.91          13.02          43.68          -6.56         -19.84   \n",
       "22         -65.91          13.02          43.68          -6.56         -19.84   \n",
       "23         -65.91          13.02          43.68          -6.56         -19.84   \n",
       "24         -65.91          13.02          43.68          -6.56         -19.84   \n",
       "25         -65.91          13.02          43.68          -6.56         -19.84   \n",
       "26         -65.91          13.02          43.68          -6.56         -19.84   \n",
       "27         -65.91          13.02          43.68          -6.56         -19.84   \n",
       "28         -65.91          13.02          43.68          -6.56         -19.84   \n",
       "29         -65.91          13.02          43.68          -6.56         -19.84   \n",
       "\n",
       "    Underlying_88  Underlying_89  Underlying_90     R  \n",
       "0           -4.39         -92.55          -2.38  0.57  \n",
       "1           -4.39         -92.55          -2.38  0.57  \n",
       "2           -4.39         -92.55          -2.38  0.57  \n",
       "3           -4.39         -92.55          -2.38  0.57  \n",
       "4           -4.39         -92.55          -2.38  0.57  \n",
       "5           -4.39         -92.55          -2.38  0.57  \n",
       "6           -4.39         -92.55          -2.38  0.57  \n",
       "7           -4.39         -92.55          -2.38  0.57  \n",
       "8           -4.39         -92.55          -2.38  0.57  \n",
       "9           -4.39         -92.55          -2.38  0.57  \n",
       "10          -4.39         -92.55          -2.38  0.57  \n",
       "11          -4.39         -92.55          -2.38  0.57  \n",
       "12          -4.39         -92.55          -2.38  0.57  \n",
       "13          -4.39         -92.55          -2.38  0.57  \n",
       "14          -4.39         -92.55          -2.38  0.57  \n",
       "15          -4.39         -92.55          -2.38  0.57  \n",
       "16          -4.39         -92.55          -2.38  0.57  \n",
       "17          -4.39         -92.55          -2.38  0.57  \n",
       "18          -4.39         -92.55          -2.38  0.57  \n",
       "19          -4.39         -92.55          -2.38  0.57  \n",
       "20          -4.39         -92.55          -2.38  0.57  \n",
       "21          -4.39         -92.55          -2.38  0.57  \n",
       "22          -4.39         -92.55          -2.38  0.57  \n",
       "23          -4.39         -92.55          -2.38  0.57  \n",
       "24          -4.39         -92.55          -2.38  0.57  \n",
       "25          -4.39         -92.55          -2.38  0.57  \n",
       "26          -4.39         -92.55          -2.38  0.57  \n",
       "27          -4.39         -92.55          -2.38  0.57  \n",
       "28          -4.39         -92.55          -2.38  0.57  \n",
       "29          -4.39         -92.55          -2.38  0.57  \n",
       "\n",
       "[30 rows x 100 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file = \"../data/processed_data/2022.csv\"\n",
    "df_read = pd.read_csv(file)\n",
    "display(df_read)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (918922, 20, 1), (918922, 4)\n",
      "Val shape: (311065, 20, 1), (311065, 4)\n"
     ]
    }
   ],
   "source": [
    "df = df_read\n",
    "\n",
    "# Formating settings\n",
    "val_start = \"2022-11-01\"\n",
    "num_features = 4\n",
    "time_steps = 20\n",
    "bs_vars = ['Underlying_last', 'Strike', 'TTM', 'R']\n",
    "underlying_lags = ['Underlying_last'] + [f'Underlying_{i}' for i in range (1, time_steps)]\n",
    "\n",
    "# Split train and validation data\n",
    "df_train = df[df['Quote_date'] < val_start]\n",
    "df_val = df[df['Quote_date'] >= val_start]\n",
    "\n",
    "# Extract target values\n",
    "train_y = df_train['Price'].to_numpy()\n",
    "val_y = df_val['Price'].to_numpy()\n",
    "\n",
    "# Convert dataframes to numpy arrays\n",
    "train_x = [df_train[underlying_lags].to_numpy(), df_train[bs_vars].to_numpy()]\n",
    "val_x = [df_val[underlying_lags].to_numpy(), df_val[bs_vars].to_numpy()]\n",
    "\n",
    "# Scale features based on training set\n",
    "underlying_scaler = MinMaxScaler()\n",
    "train_x[0] = underlying_scaler.fit_transform(train_x[0])\n",
    "val_x[0] = underlying_scaler.transform(val_x[0])\n",
    "\n",
    "bs_scaler = MinMaxScaler()\n",
    "train_x[1] = bs_scaler.fit_transform(train_x[1])\n",
    "val_x[1] = bs_scaler.transform(val_x[1])\n",
    "\n",
    "# Shuffle training set\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(len(train_x[0]))\n",
    "train_x = [train_x[0][shuffle], train_x[1][shuffle]]\n",
    "train_y = train_y[shuffle]\n",
    "\n",
    "# Reshape data to fit LSTM\n",
    "train_x = [train_x[0].reshape(len(train_x[0]),time_steps,1), train_x[1]]\n",
    "val_x = [val_x[0].reshape(len(val_x[0]), time_steps, 1), val_x[1]]\n",
    "\n",
    "print(f'Train shape: {train_x[0].shape}, {train_x[1].shape}')\n",
    "print(f'Val shape: {val_x[0].shape}, {val_x[1].shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(config):\n",
    "    '''Builds an LSTM-MLP model of minimum 2 layers sequentially from a given config dictionary'''\n",
    "\n",
    "    # Input layers\n",
    "    underlying_history = Input((config.LSTM_timesteps,1))\n",
    "    bs_vars = Input((config.Num_features,))\n",
    "\n",
    "    # LSTM layers\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(\n",
    "        units = config.LSTM_units,\n",
    "        activation = tanh,\n",
    "        input_shape = (config.LSTM_timesteps, 1),\n",
    "        return_sequences = True\n",
    "    ))\n",
    "\n",
    "    for _ in range(config.LSTM_layers - 2):\n",
    "        model.add(LSTM(\n",
    "            units = config.LSTM_units,\n",
    "            activation = tanh,\n",
    "            return_sequences = True\n",
    "        ))\n",
    "    \n",
    "    model.add(LSTM(\n",
    "        units = config.LSTM_units,\n",
    "        activation = tanh,\n",
    "        return_sequences = False\n",
    "    ))\n",
    "\n",
    "    # MLP layers\n",
    "    layers = Concatenate()([model(underlying_history), bs_vars])\n",
    "    \n",
    "    for _ in range(config.MLP_layers - 1):\n",
    "        layers = Dense(config.MLP_units)(layers)\n",
    "        layers = BatchNormalization(momentum=config.Bn_momentum)(layers)\n",
    "        layers = LeakyReLU()(layers)\n",
    "\n",
    "    output = Dense(1, activation='relu')(layers)\n",
    "\n",
    "    # Exponential decaying learning rate\n",
    "    lr_schedule = ExponentialDecay(\n",
    "        initial_learning_rate = config.Lr,\n",
    "        decay_steps = int(len(train_x[0])/config.Minibatch_size),\n",
    "        decay_rate=config.Lr_decay\n",
    "    )\n",
    "\n",
    "    # Compile model\n",
    "    model = Model(inputs=[underlying_history, bs_vars], outputs=output)\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=lr_schedule))\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter search setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: ybzdubg7\n",
      "Sweep URL: https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/sweeps/ybzdubg7\n"
     ]
    }
   ],
   "source": [
    "# Configuring the sweep hyperparameter search space\n",
    "sweep_configuration = {\n",
    "    'method': 'random',\n",
    "    'name': 'LSTM-MLP v.1.0 testing : 21-22 data',\n",
    "    'metric': {\n",
    "        'goal': 'minimize', \n",
    "        'name': 'val_loss'\n",
    "\t\t},\n",
    "    'parameters': {\n",
    "        'LSTM_units': {\n",
    "            'values': [8, 16, 32, 64, 96, 128]},\n",
    "        'MLP_units': {\n",
    "            'values': [32, 64, 96, 128]},\n",
    "        'LSTM_timesteps': {\n",
    "            'values': [20]},\n",
    "        'LSTM_layers': {\n",
    "            'distribution': 'int_uniform',\n",
    "            'max': 6, 'min': 2},\n",
    "        'MLP_layers': {\n",
    "            'distribution': 'int_uniform',\n",
    "            'max': 6, 'min': 2},\n",
    "        'Bn_momentum': {\n",
    "            'distribution': 'uniform',\n",
    "            'max': 1, 'min': 0},\n",
    "        'Lr': {\n",
    "            'distribution': 'uniform',\n",
    "            'max': 0.01, 'min': 0.0001},\n",
    "        'Lr_decay': {\n",
    "            'distribution': 'uniform',\n",
    "            'max': 1, 'min': 0.9},        \n",
    "        'Minibatch_size': {\n",
    "            'values': [1024, 2048, 4096]},\n",
    "        'Min_delta': {\n",
    "            'value': 1},\n",
    "        'Patience': {\n",
    "            'value': 20},\n",
    "        'Num_features': {\n",
    "            'value': 4},\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize sweep and creating sweepID\n",
    "\n",
    "# If new sweep, uncomment the line below and comment the line after it\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project='Deep learning for option pricing - test area') \n",
    "#sweep_id = '98bxt6oq'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(train_x = train_x, train_y = train_y, val_x = val_x, val_y = val_y, config = None, project = None):\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config, project = project):\n",
    "\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "\n",
    "        # Build model and create callbacks\n",
    "        model = create_model(config)\n",
    "\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            min_delta = config.Min_delta,\n",
    "            patience = config.Patience,\n",
    "        )\n",
    "        \n",
    "        wandb_callback = WandbCallback(\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            save_model=False\n",
    "        )\n",
    "\n",
    "        # Adapt sequence length to config\n",
    "        train_x[0] = train_x[0][:, :config.LSTM_timesteps, :]\n",
    "        val_x[0] = val_x[0][:, :config.LSTM_timesteps, :]\n",
    "        print(f'Train shape: {train_x[0].shape}, {train_x[1].shape}')\n",
    "        print(f'Val shape: {val_x[0].shape}, {val_x[1].shape}')\n",
    "\n",
    "        # Train model\n",
    "        model.fit(\n",
    "            train_x,\n",
    "            train_y,\n",
    "            batch_size = config.Minibatch_size,\n",
    "            validation_data = (val_x, val_y),\n",
    "            epochs = 1000,\n",
    "            callbacks = [early_stopping, wandb_callback] \n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run full sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: x1nwz702 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBn_momentum: 0.2115311974369981\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_layers: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_timesteps: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_units: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLr: 0.007252310655150062\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLr_decay: 0.931385828545152\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tMLP_layers: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tMLP_units: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tMin_delta: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tMinibatch_size: 1024\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tNum_features: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tPatience: 20\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/hjalmarjacobvinje/Documents/deep-learning-for-option-pricing/LSTM-MLP_notebooks/wandb/run-20230207_211403-x1nwz702</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/x1nwz702\" target=\"_blank\">smooth-sweep-1</a></strong> to <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page: <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/sweeps/ybzdubg7\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/sweeps/ybzdubg7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/sweeps/ybzdubg7\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/sweeps/ybzdubg7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/x1nwz702\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/x1nwz702</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 20, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 64)           115968      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 68)           0           ['sequential[0][0]',             \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          8832        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 128)         512         ['dense[0][0]']                  \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 128)          0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 128)          16512       ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 128)         512         ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 128)          0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 128)          16512       ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 128)         512         ['dense_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 128)          0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 128)          16512       ['leaky_re_lu_2[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 128)         512         ['dense_3[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)      (None, 128)          0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 1)            129         ['leaky_re_lu_3[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 176,513\n",
      "Trainable params: 175,489\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "Train shape: (918922, 20, 1), (918922, 4)\n",
      "Val shape: (311065, 20, 1), (311065, 4)\n",
      "Epoch 1/1000\n",
      "898/898 [==============================] - 3195s 4s/step - loss: 89862.3047 - val_loss: 768.2710\n",
      "Epoch 2/1000\n",
      "898/898 [==============================] - 351s 390ms/step - loss: 1453.5273 - val_loss: 646.8732\n",
      "Epoch 3/1000\n",
      "898/898 [==============================] - 346s 385ms/step - loss: 1333.1545 - val_loss: 1266.7584\n",
      "Epoch 4/1000\n",
      "898/898 [==============================] - 346s 385ms/step - loss: 1404.3755 - val_loss: 1543.6741\n",
      "Epoch 5/1000\n",
      "898/898 [==============================] - 371s 413ms/step - loss: 1457.4291 - val_loss: 1395.6392\n",
      "Epoch 6/1000\n",
      "898/898 [==============================] - 350s 390ms/step - loss: 1415.0502 - val_loss: 319.3226\n",
      "Epoch 7/1000\n",
      "898/898 [==============================] - 348s 388ms/step - loss: 1303.1407 - val_loss: 1790.8051\n",
      "Epoch 8/1000\n",
      "898/898 [==============================] - 4829s 5s/step - loss: 1312.5432 - val_loss: 1458.0983\n",
      "Epoch 9/1000\n",
      "898/898 [==============================] - 334s 371ms/step - loss: 1490.7477 - val_loss: 549.4606\n",
      "Epoch 10/1000\n",
      "898/898 [==============================] - 320s 356ms/step - loss: 1281.6539 - val_loss: 1021.4601\n",
      "Epoch 11/1000\n",
      "898/898 [==============================] - 4091s 5s/step - loss: 1256.8662 - val_loss: 4848.3633\n",
      "Epoch 12/1000\n",
      "898/898 [==============================] - 16466s 18s/step - loss: 1302.0778 - val_loss: 588.9904\n",
      "Epoch 13/1000\n",
      "898/898 [==============================] - 14144s 16s/step - loss: 1263.6561 - val_loss: 3052.4429\n",
      "Epoch 14/1000\n",
      "898/898 [==============================] - 344s 382ms/step - loss: 1103.0442 - val_loss: 2864.6431\n",
      "Epoch 15/1000\n",
      "461/898 [==============>...............] - ETA: 4:45 - loss: 1214.8164"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ConnectionError), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "898/898 [==============================] - 3373s 4s/step - loss: 1178.8127 - val_loss: 2081.3750\n",
      "Epoch 16/1000\n",
      "898/898 [==============================] - 470s 524ms/step - loss: 1030.3053 - val_loss: 4746.7500\n",
      "Epoch 17/1000\n",
      "414/898 [============>.................] - ETA: 9:46 - loss: 1035.6847"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ConnectionError), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "658/898 [====================>.........] - ETA: 4:52 - loss: 1022.8640"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ConnectionError), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/898 [=====================>........] - ETA: 4:37 - loss: 1001.6516"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ConnectionError), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "898/898 [==============================] - 1393s 2s/step - loss: 1035.3822 - val_loss: 1996.6251\n",
      "Epoch 18/1000\n",
      "898/898 [==============================] - 433s 481ms/step - loss: 977.9628 - val_loss: 918.9311\n",
      "Epoch 19/1000\n",
      "147/898 [===>..........................] - ETA: 4:55 - loss: 965.1594"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id=sweep_id, function=trainer, project='Deep learning for option pricing - test area', count = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href=\"https://wandb.me/wandb-init\" target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d308a875ec2e4d7094edcb5de36ab4c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016742161466390827, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/hjalmarjacobvinje/Documents/deep-learning-for-option-pricing/LSTM-MLP_notebooks/wandb/run-20230207_211303-pop6udf4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/pop6udf4\" target=\"_blank\">honest-sweep-1</a></strong> to <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page: <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/sweeps/a2ayoc3v\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/sweeps/a2ayoc3v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/sweeps/a2ayoc3v\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/sweeps/a2ayoc3v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/pop6udf4\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/pop6udf4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 40, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 96)           111744      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 100)          0           ['sequential[0][0]',             \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 32)           3232        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32)          128         ['dense[0][0]']                  \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 32)           0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 32)           1056        ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32)          128         ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 32)           0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 32)           1056        ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32)          128         ['dense_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 32)           0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 32)           1056        ['leaky_re_lu_2[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 32)          128         ['dense_3[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)      (None, 32)           0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 1)            33          ['leaky_re_lu_3[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 118,689\n",
      "Trainable params: 118,433\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n",
      "Train shape: (918922, 20, 1), (918922, 4)\n",
      "Val shape: (311065, 20, 1), (311065, 4)\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50aa73bae2bc4fc8adb442de82199963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">honest-sweep-1</strong> at: <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/pop6udf4\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/pop6udf4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230207_211303-pop6udf4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, 40, 1), found shape=(None, 20, 1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 17\u001b[0m\n\u001b[1;32m      1\u001b[0m config \u001b[39m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mLSTM_units\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m8\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mMLP_units\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m100\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m2022: Val split 01.11 with shuffle\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     16\u001b[0m }\n\u001b[0;32m---> 17\u001b[0m trainer(config \u001b[39m=\u001b[39;49m config, project \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mDeep learning for option pricing - test area\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[11], line 32\u001b[0m, in \u001b[0;36mtrainer\u001b[0;34m(train_x, train_y, val_x, val_y, config, project)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mVal shape: \u001b[39m\u001b[39m{\u001b[39;00mval_x[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mval_x[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[39m# Train model\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     33\u001b[0m     train_x,\n\u001b[1;32m     34\u001b[0m     train_y,\n\u001b[1;32m     35\u001b[0m     batch_size \u001b[39m=\u001b[39;49m config\u001b[39m.\u001b[39;49mMinibatch_size,\n\u001b[1;32m     36\u001b[0m     validation_data \u001b[39m=\u001b[39;49m (val_x, val_y),\n\u001b[1;32m     37\u001b[0m     epochs \u001b[39m=\u001b[39;49m \u001b[39m1000\u001b[39;49m,\n\u001b[1;32m     38\u001b[0m     callbacks \u001b[39m=\u001b[39;49m [early_stopping, wandb_callback] \n\u001b[1;32m     39\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/wandb/integration/keras/keras.py:174\u001b[0m, in \u001b[0;36mpatch_tf_keras.<locals>.new_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[39mfor\u001b[39;00m cbk \u001b[39min\u001b[39;00m cbks:\n\u001b[1;32m    173\u001b[0m         set_wandb_attrs(cbk, val_data)\n\u001b[0;32m--> 174\u001b[0m \u001b[39mreturn\u001b[39;00m old_v2(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/wandb/integration/keras/keras.py:174\u001b[0m, in \u001b[0;36mpatch_tf_keras.<locals>.new_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[39mfor\u001b[39;00m cbk \u001b[39min\u001b[39;00m cbks:\n\u001b[1;32m    173\u001b[0m         set_wandb_attrs(cbk, val_data)\n\u001b[0;32m--> 174\u001b[0m \u001b[39mreturn\u001b[39;00m old_v2(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/wandb/integration/keras/keras.py:174\u001b[0m, in \u001b[0;36mpatch_tf_keras.<locals>.new_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[39mfor\u001b[39;00m cbk \u001b[39min\u001b[39;00m cbks:\n\u001b[1;32m    173\u001b[0m         set_wandb_attrs(cbk, val_data)\n\u001b[0;32m--> 174\u001b[0m \u001b[39mreturn\u001b[39;00m old_v2(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/wk/x86_p6511l95p594k6qnb98h0000gn/T/__autograph_generated_file4to0cf6p.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, 40, 1), found shape=(None, 20, 1)\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'LSTM_units': 8,\n",
    "    'MLP_units': 100,\n",
    "    'LSTM_timesteps': 20,\n",
    "    'LSTM_layers': 3,\n",
    "    'MLP_layers': 4,\n",
    "    'Bn_momentum': 0.99,\n",
    "    'Lr': 0.01,\n",
    "    'Lr_decay': 1,\n",
    "    'Minibatch_size': 4096,\n",
    "    'Min_delta': 1,\n",
    "    'Patience': 20,\n",
    "    'Num_features': 4,\n",
    "    'Architecture': 'LSTM-MLP v.0.1',\n",
    "    'Dataset': '2022: Val split 01.11 with shuffle',\n",
    "}\n",
    "trainer(config = config, project = 'Deep learning for option pricing - test area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
