{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights and Biases\n",
    "#!pip install -q wandb\n",
    "# Tensorflow\n",
    "#!pip install -q tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, LSTM, Concatenate, Dense, BatchNormalization, LeakyReLU\n",
    "from keras.activations import tanh\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/hjalmarjacobvinje/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import config\n",
    "#Erlend\n",
    "#wandb.login(key=config.erlend_key)\n",
    "# Hjalmar\n",
    "wandb.login(key=config.hjalmar_key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, split and normalize data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Quote_date</th>\n",
       "      <th>Expire_date</th>\n",
       "      <th>Price</th>\n",
       "      <th>Underlying_last</th>\n",
       "      <th>Strike</th>\n",
       "      <th>TTM</th>\n",
       "      <th>Moneyness</th>\n",
       "      <th>Underlying_return</th>\n",
       "      <th>Underlying_1</th>\n",
       "      <th>...</th>\n",
       "      <th>Underlying_82</th>\n",
       "      <th>Underlying_83</th>\n",
       "      <th>Underlying_84</th>\n",
       "      <th>Underlying_85</th>\n",
       "      <th>Underlying_86</th>\n",
       "      <th>Underlying_87</th>\n",
       "      <th>Underlying_88</th>\n",
       "      <th>Underlying_89</th>\n",
       "      <th>Underlying_90</th>\n",
       "      <th>R</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>721776</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>3002.45</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.000990</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>721777</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>2800.60</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.334158</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>721778</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>2602.20</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.857850</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>721779</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>2402.45</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.500619</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>721780</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>2202.45</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.222772</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>721781</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>1998.15</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.000495</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>721782</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>1799.25</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.818632</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>721783</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>1601.70</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.667079</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>721784</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>1402.25</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>2600.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.538842</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>721785</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>1202.45</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>2800.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.428925</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>721786</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>1004.55</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.333663</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>721787</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>902.30</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>3100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.290642</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>721788</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>802.30</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.250309</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>721789</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>702.50</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>3300.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.212421</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>721790</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>602.70</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>3400.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.176762</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>721791</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>503.95</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.143140</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>721792</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>453.95</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>3550.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.127039</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>721793</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>403.60</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.111386</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>721794</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>353.70</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>3650.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.096162</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>721795</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>328.75</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>3675.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.088705</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>721796</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>303.75</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>3700.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.081349</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>721797</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>283.75</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>3720.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.075535</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>721798</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>278.75</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>3725.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.074091</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>721799</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>273.85</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>3730.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.072651</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>721800</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>263.85</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>3740.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.069783</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>721801</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>254.15</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.066931</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>721802</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>244.15</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>3760.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.064093</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>721803</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>234.20</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>3770.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.061271</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>721804</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>229.25</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>3775.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.059865</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>721805</td>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>224.25</td>\n",
       "      <td>4000.99</td>\n",
       "      <td>3780.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.058463</td>\n",
       "      <td>7.73</td>\n",
       "      <td>-130.56</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>-65.91</td>\n",
       "      <td>13.02</td>\n",
       "      <td>43.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-92.55</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  Quote_date Expire_date    Price  Underlying_last  Strike  TTM  \\\n",
       "0       721776  2022-05-10  2022-05-11  3002.45          4000.99  1000.0    1   \n",
       "1       721777  2022-05-10  2022-05-11  2800.60          4000.99  1200.0    1   \n",
       "2       721778  2022-05-10  2022-05-11  2602.20          4000.99  1400.0    1   \n",
       "3       721779  2022-05-10  2022-05-11  2402.45          4000.99  1600.0    1   \n",
       "4       721780  2022-05-10  2022-05-11  2202.45          4000.99  1800.0    1   \n",
       "5       721781  2022-05-10  2022-05-11  1998.15          4000.99  2000.0    1   \n",
       "6       721782  2022-05-10  2022-05-11  1799.25          4000.99  2200.0    1   \n",
       "7       721783  2022-05-10  2022-05-11  1601.70          4000.99  2400.0    1   \n",
       "8       721784  2022-05-10  2022-05-11  1402.25          4000.99  2600.0    1   \n",
       "9       721785  2022-05-10  2022-05-11  1202.45          4000.99  2800.0    1   \n",
       "10      721786  2022-05-10  2022-05-11  1004.55          4000.99  3000.0    1   \n",
       "11      721787  2022-05-10  2022-05-11   902.30          4000.99  3100.0    1   \n",
       "12      721788  2022-05-10  2022-05-11   802.30          4000.99  3200.0    1   \n",
       "13      721789  2022-05-10  2022-05-11   702.50          4000.99  3300.0    1   \n",
       "14      721790  2022-05-10  2022-05-11   602.70          4000.99  3400.0    1   \n",
       "15      721791  2022-05-10  2022-05-11   503.95          4000.99  3500.0    1   \n",
       "16      721792  2022-05-10  2022-05-11   453.95          4000.99  3550.0    1   \n",
       "17      721793  2022-05-10  2022-05-11   403.60          4000.99  3600.0    1   \n",
       "18      721794  2022-05-10  2022-05-11   353.70          4000.99  3650.0    1   \n",
       "19      721795  2022-05-10  2022-05-11   328.75          4000.99  3675.0    1   \n",
       "20      721796  2022-05-10  2022-05-11   303.75          4000.99  3700.0    1   \n",
       "21      721797  2022-05-10  2022-05-11   283.75          4000.99  3720.0    1   \n",
       "22      721798  2022-05-10  2022-05-11   278.75          4000.99  3725.0    1   \n",
       "23      721799  2022-05-10  2022-05-11   273.85          4000.99  3730.0    1   \n",
       "24      721800  2022-05-10  2022-05-11   263.85          4000.99  3740.0    1   \n",
       "25      721801  2022-05-10  2022-05-11   254.15          4000.99  3750.0    1   \n",
       "26      721802  2022-05-10  2022-05-11   244.15          4000.99  3760.0    1   \n",
       "27      721803  2022-05-10  2022-05-11   234.20          4000.99  3770.0    1   \n",
       "28      721804  2022-05-10  2022-05-11   229.25          4000.99  3775.0    1   \n",
       "29      721805  2022-05-10  2022-05-11   224.25          4000.99  3780.0    1   \n",
       "\n",
       "    Moneyness  Underlying_return  Underlying_1  ...  Underlying_82  \\\n",
       "0    4.000990               7.73       -130.56  ...           1.57   \n",
       "1    3.334158               7.73       -130.56  ...           1.57   \n",
       "2    2.857850               7.73       -130.56  ...           1.57   \n",
       "3    2.500619               7.73       -130.56  ...           1.57   \n",
       "4    2.222772               7.73       -130.56  ...           1.57   \n",
       "5    2.000495               7.73       -130.56  ...           1.57   \n",
       "6    1.818632               7.73       -130.56  ...           1.57   \n",
       "7    1.667079               7.73       -130.56  ...           1.57   \n",
       "8    1.538842               7.73       -130.56  ...           1.57   \n",
       "9    1.428925               7.73       -130.56  ...           1.57   \n",
       "10   1.333663               7.73       -130.56  ...           1.57   \n",
       "11   1.290642               7.73       -130.56  ...           1.57   \n",
       "12   1.250309               7.73       -130.56  ...           1.57   \n",
       "13   1.212421               7.73       -130.56  ...           1.57   \n",
       "14   1.176762               7.73       -130.56  ...           1.57   \n",
       "15   1.143140               7.73       -130.56  ...           1.57   \n",
       "16   1.127039               7.73       -130.56  ...           1.57   \n",
       "17   1.111386               7.73       -130.56  ...           1.57   \n",
       "18   1.096162               7.73       -130.56  ...           1.57   \n",
       "19   1.088705               7.73       -130.56  ...           1.57   \n",
       "20   1.081349               7.73       -130.56  ...           1.57   \n",
       "21   1.075535               7.73       -130.56  ...           1.57   \n",
       "22   1.074091               7.73       -130.56  ...           1.57   \n",
       "23   1.072651               7.73       -130.56  ...           1.57   \n",
       "24   1.069783               7.73       -130.56  ...           1.57   \n",
       "25   1.066931               7.73       -130.56  ...           1.57   \n",
       "26   1.064093               7.73       -130.56  ...           1.57   \n",
       "27   1.061271               7.73       -130.56  ...           1.57   \n",
       "28   1.059865               7.73       -130.56  ...           1.57   \n",
       "29   1.058463               7.73       -130.56  ...           1.57   \n",
       "\n",
       "    Underlying_83  Underlying_84  Underlying_85  Underlying_86  Underlying_87  \\\n",
       "0          -65.91          13.02          43.68          -6.56         -19.84   \n",
       "1          -65.91          13.02          43.68          -6.56         -19.84   \n",
       "2          -65.91          13.02          43.68          -6.56         -19.84   \n",
       "3          -65.91          13.02          43.68          -6.56         -19.84   \n",
       "4          -65.91          13.02          43.68          -6.56         -19.84   \n",
       "5          -65.91          13.02          43.68          -6.56         -19.84   \n",
       "6          -65.91          13.02          43.68          -6.56         -19.84   \n",
       "7          -65.91          13.02          43.68          -6.56         -19.84   \n",
       "8          -65.91          13.02          43.68          -6.56         -19.84   \n",
       "9          -65.91          13.02          43.68          -6.56         -19.84   \n",
       "10         -65.91          13.02          43.68          -6.56         -19.84   \n",
       "11         -65.91          13.02          43.68          -6.56         -19.84   \n",
       "12         -65.91          13.02          43.68          -6.56         -19.84   \n",
       "13         -65.91          13.02          43.68          -6.56         -19.84   \n",
       "14         -65.91          13.02          43.68          -6.56         -19.84   \n",
       "15         -65.91          13.02          43.68          -6.56         -19.84   \n",
       "16         -65.91          13.02          43.68          -6.56         -19.84   \n",
       "17         -65.91          13.02          43.68          -6.56         -19.84   \n",
       "18         -65.91          13.02          43.68          -6.56         -19.84   \n",
       "19         -65.91          13.02          43.68          -6.56         -19.84   \n",
       "20         -65.91          13.02          43.68          -6.56         -19.84   \n",
       "21         -65.91          13.02          43.68          -6.56         -19.84   \n",
       "22         -65.91          13.02          43.68          -6.56         -19.84   \n",
       "23         -65.91          13.02          43.68          -6.56         -19.84   \n",
       "24         -65.91          13.02          43.68          -6.56         -19.84   \n",
       "25         -65.91          13.02          43.68          -6.56         -19.84   \n",
       "26         -65.91          13.02          43.68          -6.56         -19.84   \n",
       "27         -65.91          13.02          43.68          -6.56         -19.84   \n",
       "28         -65.91          13.02          43.68          -6.56         -19.84   \n",
       "29         -65.91          13.02          43.68          -6.56         -19.84   \n",
       "\n",
       "    Underlying_88  Underlying_89  Underlying_90     R  \n",
       "0           -4.39         -92.55          -2.38  0.57  \n",
       "1           -4.39         -92.55          -2.38  0.57  \n",
       "2           -4.39         -92.55          -2.38  0.57  \n",
       "3           -4.39         -92.55          -2.38  0.57  \n",
       "4           -4.39         -92.55          -2.38  0.57  \n",
       "5           -4.39         -92.55          -2.38  0.57  \n",
       "6           -4.39         -92.55          -2.38  0.57  \n",
       "7           -4.39         -92.55          -2.38  0.57  \n",
       "8           -4.39         -92.55          -2.38  0.57  \n",
       "9           -4.39         -92.55          -2.38  0.57  \n",
       "10          -4.39         -92.55          -2.38  0.57  \n",
       "11          -4.39         -92.55          -2.38  0.57  \n",
       "12          -4.39         -92.55          -2.38  0.57  \n",
       "13          -4.39         -92.55          -2.38  0.57  \n",
       "14          -4.39         -92.55          -2.38  0.57  \n",
       "15          -4.39         -92.55          -2.38  0.57  \n",
       "16          -4.39         -92.55          -2.38  0.57  \n",
       "17          -4.39         -92.55          -2.38  0.57  \n",
       "18          -4.39         -92.55          -2.38  0.57  \n",
       "19          -4.39         -92.55          -2.38  0.57  \n",
       "20          -4.39         -92.55          -2.38  0.57  \n",
       "21          -4.39         -92.55          -2.38  0.57  \n",
       "22          -4.39         -92.55          -2.38  0.57  \n",
       "23          -4.39         -92.55          -2.38  0.57  \n",
       "24          -4.39         -92.55          -2.38  0.57  \n",
       "25          -4.39         -92.55          -2.38  0.57  \n",
       "26          -4.39         -92.55          -2.38  0.57  \n",
       "27          -4.39         -92.55          -2.38  0.57  \n",
       "28          -4.39         -92.55          -2.38  0.57  \n",
       "29          -4.39         -92.55          -2.38  0.57  \n",
       "\n",
       "[30 rows x 100 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file = \"../data/processed_data/2022.csv\"\n",
    "df_read = pd.read_csv(file)\n",
    "display(df_read.head(30))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (918922, 20, 1), (918922, 4)\n",
      "Val shape: (311065, 20, 1), (311065, 4)\n"
     ]
    }
   ],
   "source": [
    "df = df_read\n",
    "\n",
    "# Formating settings\n",
    "val_start = \"2022-11-01\"\n",
    "num_features = 4\n",
    "time_steps = 20\n",
    "bs_vars = ['Underlying_last', 'Strike', 'TTM', 'R']\n",
    "underlying_lags = ['Underlying_last'] + [f'Underlying_{i}' for i in range (1, time_steps)]\n",
    "\n",
    "# Split train and validation data\n",
    "df_train = df[df['Quote_date'] < val_start]\n",
    "df_val = df[df['Quote_date'] >= val_start]\n",
    "\n",
    "# Extract target values\n",
    "train_y = df_train['Price'].to_numpy()\n",
    "val_y = df_val['Price'].to_numpy()\n",
    "\n",
    "# Convert dataframes to numpy arrays\n",
    "train_x = [df_train[underlying_lags].to_numpy(), df_train[bs_vars].to_numpy()]\n",
    "val_x = [df_val[underlying_lags].to_numpy(), df_val[bs_vars].to_numpy()]\n",
    "\n",
    "# Scale features based on training set\n",
    "underlying_scaler = MinMaxScaler()\n",
    "train_x[0] = underlying_scaler.fit_transform(train_x[0])\n",
    "val_x[0] = underlying_scaler.transform(val_x[0])\n",
    "\n",
    "bs_scaler = MinMaxScaler()\n",
    "train_x[1] = bs_scaler.fit_transform(train_x[1])\n",
    "val_x[1] = bs_scaler.transform(val_x[1])\n",
    "\n",
    "# Shuffle training set\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(len(train_x[0]))\n",
    "train_x = [train_x[0][shuffle], train_x[1][shuffle]]\n",
    "train_y = train_y[shuffle]\n",
    "\n",
    "# Reshape data to fit LSTM\n",
    "train_x = [train_x[0].reshape(len(train_x[0]),time_steps,1), train_x[1]]\n",
    "val_x = [val_x[0].reshape(len(val_x[0]), time_steps, 1), val_x[1]]\n",
    "\n",
    "print(f'Train shape: {train_x[0].shape}, {train_x[1].shape}')\n",
    "print(f'Val shape: {val_x[0].shape}, {val_x[1].shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(config):\n",
    "    '''Builds an LSTM-MLP model of minimum 2 layers sequentially from a given config dictionary'''\n",
    "\n",
    "    # Input layers\n",
    "    underlying_history = Input((config.LSTM_timesteps,1))\n",
    "    bs_vars = Input((config.Num_features,))\n",
    "\n",
    "    # LSTM layers\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(\n",
    "        units = config.LSTM_units,\n",
    "        activation = tanh,\n",
    "        input_shape = (config.LSTM_timesteps, 1),\n",
    "        return_sequences = True\n",
    "    ))\n",
    "\n",
    "    for _ in range(config.LSTM_layers - 2):\n",
    "        model.add(LSTM(\n",
    "            units = config.LSTM_units,\n",
    "            activation = tanh,\n",
    "            return_sequences = True\n",
    "        ))\n",
    "    \n",
    "    model.add(LSTM(\n",
    "        units = config.LSTM_units,\n",
    "        activation = tanh,\n",
    "        return_sequences = False\n",
    "    ))\n",
    "\n",
    "    # MLP layers\n",
    "    layers = Concatenate()([model(underlying_history), bs_vars])\n",
    "    \n",
    "    for _ in range(config.MLP_layers - 1):\n",
    "        layers = Dense(config.MLP_units)(layers)\n",
    "        layers = BatchNormalization(momentum=config.Bn_momentum)(layers)\n",
    "        layers = LeakyReLU()(layers)\n",
    "\n",
    "    output = Dense(1, activation='relu')(layers)\n",
    "\n",
    "    # Exponential decaying learning rate\n",
    "    lr_schedule = ExponentialDecay(\n",
    "        initial_learning_rate = config.Lr,\n",
    "        decay_steps = int(len(train_x[0])/config.Minibatch_size),\n",
    "        decay_rate=config.Lr_decay\n",
    "    )\n",
    "\n",
    "    # Compile model\n",
    "    model = Model(inputs=[underlying_history, bs_vars], outputs=output)\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=lr_schedule))\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter search setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 9ftanbyq\n",
      "Sweep URL: https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/sweeps/9ftanbyq\n"
     ]
    }
   ],
   "source": [
    "# Configuring the sweep hyperparameter search space\n",
    "sweep_configuration = {\n",
    "    'method': 'random',\n",
    "    'name': 'LSTM-MLP v.1.0 testing',\n",
    "    'metric': {\n",
    "        'goal': 'minimize', \n",
    "        'name': 'val_loss'\n",
    "\t\t},\n",
    "    'parameters': {\n",
    "        'LSTM_units': {\n",
    "            'values': [8, 16, 32, 64, 96, 128]},\n",
    "        'MLP_units': {\n",
    "            'values': [32, 64, 96, 128]},\n",
    "        'LSTM_timesteps': {\n",
    "            'values': [10, 20]},\n",
    "        'LSTM_layers': {\n",
    "            'distribution': 'int_uniform',\n",
    "            'max': 6, 'min': 2},\n",
    "        'MLP_layers': {\n",
    "            'distribution': 'int_uniform',\n",
    "            'max': 6, 'min': 2},\n",
    "        'Bn_momentum': {\n",
    "            'distribution': 'uniform',\n",
    "            'max': 1, 'min': 0},\n",
    "        'Lr': {\n",
    "            'distribution': 'uniform',\n",
    "            'max': 0.01, 'min': 0.0001},\n",
    "        'Lr_decay': {\n",
    "            'distribution': 'uniform',\n",
    "            'max': 1, 'min': 0.9},        \n",
    "        'Minibatch_size': {\n",
    "            'values': [1024, 2048, 4096]},\n",
    "        'Min_delta': {\n",
    "            'value': 1},\n",
    "        'Patience': {\n",
    "            'value': 20},\n",
    "        'Num_features': {\n",
    "            'value': 4},\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize sweep and creating sweepID\n",
    "\n",
    "# If new sweep, uncomment the line below and comment the line after it\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project='Deep learning for option pricing - test area') \n",
    "#sweep_id = '98bxt6oq'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(train_x = train_x, train_y = train_y, val_x = val_x, val_y = val_y, config = None, project = None):\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config, project = project):\n",
    "\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "\n",
    "        # Build model and create callbacks\n",
    "        model = create_model(config)\n",
    "\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            min_delta = config.Min_delta,\n",
    "            patience = config.Patience,\n",
    "        )\n",
    "        \n",
    "        wandb_callback = WandbCallback(\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            save_model=False\n",
    "        )\n",
    "\n",
    "        # Adapt sequence length to config\n",
    "        train_x[0] = train_x[0][:, :config.LSTM_timesteps, :]\n",
    "        val_x[0] = val_x[0][:, :config.LSTM_timesteps, :]\n",
    "        print(f'Train shape: {train_x[0].shape}, {train_x[1].shape}')\n",
    "        print(f'Val shape: {val_x[0].shape}, {val_x[1].shape}')\n",
    "\n",
    "        # Train model\n",
    "        model.fit(\n",
    "            train_x,\n",
    "            train_y,\n",
    "            batch_size = config.Minibatch_size,\n",
    "            validation_data = (val_x, val_y),\n",
    "            epochs = 1000,\n",
    "            callbacks = [early_stopping, wandb_callback] \n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run full sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kl9uu6aa with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tBn_momentum: 0.014417146220246035\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_timesteps: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLSTM_units: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLr: 0.0054796574930675605\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tLr_decay: 0.9664435825241704\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tMLP_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tMLP_units: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tMin_delta: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tMinibatch_size: 1024\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tNum_features: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tPatience: 20\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id=sweep_id, function=trainer, project='Deep learning for option pricing - test area', count = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.13.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Erlend\\Google Drive\\NTNU\\5. Klasse\\Master\\Git-folder\\deep-learning-for-option-pricing\\LSTM-MLP_notebooks\\wandb\\run-20230207_123606-30brlfcr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/30brlfcr\" target=\"_blank\">woven-lion-80</a></strong> to <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 20, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 8)            1408        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 12)           0           ['sequential[0][0]',             \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 100)          1300        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 100)         400         ['dense[0][0]']                  \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 100)          0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 100)          10100       ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 100)         400         ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 100)          0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 100)          10100       ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 100)         400         ['dense_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 100)          0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            101         ['leaky_re_lu_2[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 24,209\n",
      "Trainable params: 23,609\n",
      "Non-trainable params: 600\n",
      "__________________________________________________________________________________________________\n",
      "Train shape: (1452255, 20, 1), (1452255, 4)\n",
      "Val shape: (311065, 20, 1), (311065, 4)\n",
      "Epoch 1/1000\n",
      "355/355 [==============================] - 82s 213ms/step - loss: 227251.4375 - val_loss: 194344.3438\n",
      "Epoch 2/1000\n",
      "355/355 [==============================] - 65s 183ms/step - loss: 474.4754 - val_loss: 8041.1001\n",
      "Epoch 3/1000\n",
      "355/355 [==============================] - 63s 177ms/step - loss: 472.3555 - val_loss: 28381.0332\n",
      "Epoch 4/1000\n",
      "355/355 [==============================] - 64s 182ms/step - loss: 493.7442 - val_loss: 118136.7031\n",
      "Epoch 5/1000\n",
      "355/355 [==============================] - 57s 161ms/step - loss: 496.0572 - val_loss: 35079.3477\n",
      "Epoch 6/1000\n",
      "355/355 [==============================] - 56s 158ms/step - loss: 428.2709 - val_loss: 8320.0557\n",
      "Epoch 7/1000\n",
      "355/355 [==============================] - 57s 160ms/step - loss: 393.6819 - val_loss: 24273.2500\n",
      "Epoch 8/1000\n",
      "355/355 [==============================] - 64s 179ms/step - loss: 447.3439 - val_loss: 26465.7852\n",
      "Epoch 9/1000\n",
      "355/355 [==============================] - 80s 226ms/step - loss: 477.1388 - val_loss: 892.9767\n",
      "Epoch 10/1000\n",
      "355/355 [==============================] - 74s 208ms/step - loss: 443.5166 - val_loss: 20383.4570\n",
      "Epoch 11/1000\n",
      "355/355 [==============================] - 75s 211ms/step - loss: 476.6283 - val_loss: 3994.0474\n",
      "Epoch 12/1000\n",
      "355/355 [==============================] - 80s 225ms/step - loss: 447.2078 - val_loss: 2879.4412\n",
      "Epoch 13/1000\n",
      "355/355 [==============================] - 79s 223ms/step - loss: 488.3635 - val_loss: 4333.6914\n",
      "Epoch 14/1000\n",
      "355/355 [==============================] - 77s 217ms/step - loss: 430.5404 - val_loss: 29619.6465\n",
      "Epoch 15/1000\n",
      "355/355 [==============================] - 78s 219ms/step - loss: 530.3374 - val_loss: 10875.7090\n",
      "Epoch 16/1000\n",
      "355/355 [==============================] - 79s 222ms/step - loss: 486.3429 - val_loss: 15483.7510\n",
      "Epoch 17/1000\n",
      "355/355 [==============================] - 75s 211ms/step - loss: 503.0679 - val_loss: 322795.0625\n",
      "Epoch 18/1000\n",
      "355/355 [==============================] - 76s 215ms/step - loss: 498.5128 - val_loss: 4782.7451\n",
      "Epoch 19/1000\n",
      "355/355 [==============================] - 85s 241ms/step - loss: 468.4017 - val_loss: 711.2976\n",
      "Epoch 20/1000\n",
      "355/355 [==============================] - 85s 239ms/step - loss: 502.0860 - val_loss: 4848.7100\n",
      "Epoch 21/1000\n",
      "355/355 [==============================] - 83s 233ms/step - loss: 508.9420 - val_loss: 1462.7319\n",
      "Epoch 22/1000\n",
      "355/355 [==============================] - 138s 388ms/step - loss: 451.0058 - val_loss: 29380.2402\n",
      "Epoch 23/1000\n",
      "355/355 [==============================] - 86s 242ms/step - loss: 517.1207 - val_loss: 10699.0078\n",
      "Epoch 24/1000\n",
      "355/355 [==============================] - 86s 243ms/step - loss: 461.2269 - val_loss: 1517.8308\n",
      "Epoch 25/1000\n",
      "355/355 [==============================] - 87s 244ms/step - loss: 504.4256 - val_loss: 10513.8828\n",
      "Epoch 26/1000\n",
      "355/355 [==============================] - 87s 244ms/step - loss: 471.1577 - val_loss: 5945.6787\n",
      "Epoch 27/1000\n",
      "355/355 [==============================] - 82s 232ms/step - loss: 480.0703 - val_loss: 9894.0664\n",
      "Epoch 28/1000\n",
      "355/355 [==============================] - 88s 247ms/step - loss: 477.5972 - val_loss: 2048.1921\n",
      "Epoch 29/1000\n",
      "355/355 [==============================] - 87s 244ms/step - loss: 424.0838 - val_loss: 735.6666\n",
      "Epoch 30/1000\n",
      "355/355 [==============================] - 86s 243ms/step - loss: 462.2721 - val_loss: 1180.0521\n",
      "Epoch 31/1000\n",
      "355/355 [==============================] - 86s 243ms/step - loss: 451.7561 - val_loss: 498.6244\n",
      "Epoch 32/1000\n",
      "355/355 [==============================] - 82s 230ms/step - loss: 501.2716 - val_loss: 7325.4185\n",
      "Epoch 33/1000\n",
      "355/355 [==============================] - 80s 227ms/step - loss: 433.8280 - val_loss: 1553.6085\n",
      "Epoch 34/1000\n",
      "355/355 [==============================] - 87s 245ms/step - loss: 478.0516 - val_loss: 4188.3994\n",
      "Epoch 35/1000\n",
      "355/355 [==============================] - 88s 249ms/step - loss: 439.7157 - val_loss: 1014.1069\n",
      "Epoch 36/1000\n",
      "355/355 [==============================] - 88s 247ms/step - loss: 421.3161 - val_loss: 8202.8496\n",
      "Epoch 37/1000\n",
      "355/355 [==============================] - 89s 250ms/step - loss: 514.5905 - val_loss: 14672.0059\n",
      "Epoch 38/1000\n",
      "355/355 [==============================] - 88s 248ms/step - loss: 462.6916 - val_loss: 1117.5245\n",
      "Epoch 39/1000\n",
      "355/355 [==============================] - 89s 250ms/step - loss: 507.5510 - val_loss: 5179.5312\n",
      "Epoch 40/1000\n",
      "355/355 [==============================] - 88s 248ms/step - loss: 473.2314 - val_loss: 10093.3779\n",
      "Epoch 41/1000\n",
      "355/355 [==============================] - 89s 250ms/step - loss: 444.4245 - val_loss: 1840.4677\n",
      "Epoch 42/1000\n",
      "355/355 [==============================] - 88s 247ms/step - loss: 448.1315 - val_loss: 4814.8911\n",
      "Epoch 43/1000\n",
      "355/355 [==============================] - 89s 250ms/step - loss: 433.8468 - val_loss: 2949.1570\n",
      "Epoch 44/1000\n",
      "355/355 [==============================] - 90s 252ms/step - loss: 489.1543 - val_loss: 15036.3730\n",
      "Epoch 45/1000\n",
      "355/355 [==============================] - 89s 252ms/step - loss: 521.8607 - val_loss: 400.2130\n",
      "Epoch 46/1000\n",
      "355/355 [==============================] - 89s 252ms/step - loss: 521.3358 - val_loss: 1224.6058\n",
      "Epoch 47/1000\n",
      "355/355 [==============================] - 89s 251ms/step - loss: 475.3169 - val_loss: 7957.5820\n",
      "Epoch 48/1000\n",
      "355/355 [==============================] - 88s 249ms/step - loss: 426.7780 - val_loss: 852.2328\n",
      "Epoch 49/1000\n",
      "355/355 [==============================] - 87s 246ms/step - loss: 453.4919 - val_loss: 1049.4094\n",
      "Epoch 50/1000\n",
      "355/355 [==============================] - 87s 245ms/step - loss: 404.7739 - val_loss: 970.6992\n",
      "Epoch 51/1000\n",
      "355/355 [==============================] - 87s 246ms/step - loss: 481.2761 - val_loss: 8349.8418\n",
      "Epoch 52/1000\n",
      "355/355 [==============================] - 88s 248ms/step - loss: 479.6592 - val_loss: 311.6687\n",
      "Epoch 53/1000\n",
      "355/355 [==============================] - 87s 244ms/step - loss: 409.8933 - val_loss: 2815.3296\n",
      "Epoch 54/1000\n",
      "355/355 [==============================] - 88s 248ms/step - loss: 474.1137 - val_loss: 1729.8138\n",
      "Epoch 55/1000\n",
      "355/355 [==============================] - 88s 247ms/step - loss: 415.7791 - val_loss: 613.2369\n",
      "Epoch 56/1000\n",
      "355/355 [==============================] - 87s 246ms/step - loss: 441.0002 - val_loss: 4207.3071\n",
      "Epoch 57/1000\n",
      "355/355 [==============================] - 88s 247ms/step - loss: 507.4158 - val_loss: 1194.2615\n",
      "Epoch 58/1000\n",
      "355/355 [==============================] - 88s 247ms/step - loss: 418.9736 - val_loss: 2422.4272\n",
      "Epoch 59/1000\n",
      "355/355 [==============================] - 88s 249ms/step - loss: 406.8038 - val_loss: 1141.5482\n",
      "Epoch 60/1000\n",
      "355/355 [==============================] - 83s 234ms/step - loss: 463.2537 - val_loss: 1415.8501\n",
      "Epoch 61/1000\n",
      "355/355 [==============================] - 1419s 4s/step - loss: 509.9047 - val_loss: 2562.0278\n",
      "Epoch 62/1000\n",
      "355/355 [==============================] - 77s 217ms/step - loss: 504.8528 - val_loss: 15742.5244\n",
      "Epoch 63/1000\n",
      "355/355 [==============================] - 39s 111ms/step - loss: 452.6597 - val_loss: 1052.1387\n",
      "Epoch 64/1000\n",
      "355/355 [==============================] - 45s 126ms/step - loss: 442.7572 - val_loss: 2963.8674\n",
      "Epoch 65/1000\n",
      "355/355 [==============================] - 45s 126ms/step - loss: 428.1247 - val_loss: 535.8888\n",
      "Epoch 66/1000\n",
      "355/355 [==============================] - 48s 135ms/step - loss: 460.9793 - val_loss: 557.7028\n",
      "Epoch 67/1000\n",
      "355/355 [==============================] - 50s 140ms/step - loss: 410.4386 - val_loss: 4313.2124\n",
      "Epoch 68/1000\n",
      "355/355 [==============================] - 55s 156ms/step - loss: 413.8075 - val_loss: 1429.6615\n",
      "Epoch 69/1000\n",
      "355/355 [==============================] - 57s 161ms/step - loss: 440.8260 - val_loss: 482.5964\n",
      "Epoch 70/1000\n",
      "355/355 [==============================] - 57s 161ms/step - loss: 490.0808 - val_loss: 306.8202\n",
      "Epoch 71/1000\n",
      "355/355 [==============================] - 57s 160ms/step - loss: 463.9520 - val_loss: 1481.3186\n",
      "Epoch 72/1000\n",
      "355/355 [==============================] - 54s 152ms/step - loss: 450.5670 - val_loss: 1344.7737\n",
      "Epoch 73/1000\n",
      "355/355 [==============================] - 54s 151ms/step - loss: 429.7532 - val_loss: 1318.8152\n",
      "Epoch 74/1000\n",
      "355/355 [==============================] - 52s 148ms/step - loss: 504.5970 - val_loss: 1145.9813\n",
      "Epoch 75/1000\n",
      "355/355 [==============================] - 56s 157ms/step - loss: 399.9900 - val_loss: 1310.1387\n",
      "Epoch 76/1000\n",
      "355/355 [==============================] - 56s 158ms/step - loss: 434.5365 - val_loss: 12243.3672\n",
      "Epoch 77/1000\n",
      "355/355 [==============================] - 57s 159ms/step - loss: 490.2596 - val_loss: 651.0121\n",
      "Epoch 78/1000\n",
      "355/355 [==============================] - 53s 150ms/step - loss: 468.4704 - val_loss: 5090.4053\n",
      "Epoch 79/1000\n",
      "355/355 [==============================] - 55s 156ms/step - loss: 433.4182 - val_loss: 669.8524\n",
      "Epoch 80/1000\n",
      "355/355 [==============================] - 59s 167ms/step - loss: 490.4784 - val_loss: 2610.3911\n",
      "Epoch 81/1000\n",
      "355/355 [==============================] - 63s 178ms/step - loss: 458.1244 - val_loss: 424.7125\n",
      "Epoch 82/1000\n",
      "355/355 [==============================] - 60s 170ms/step - loss: 429.4602 - val_loss: 293.6133\n",
      "Epoch 83/1000\n",
      "355/355 [==============================] - 68s 191ms/step - loss: 430.9532 - val_loss: 1192.0078\n",
      "Epoch 84/1000\n",
      "355/355 [==============================] - 91s 258ms/step - loss: 436.7829 - val_loss: 8281.8896\n",
      "Epoch 85/1000\n",
      "355/355 [==============================] - 90s 254ms/step - loss: 451.0257 - val_loss: 2566.0015\n",
      "Epoch 86/1000\n",
      "355/355 [==============================] - 89s 251ms/step - loss: 467.9313 - val_loss: 1053.2496\n",
      "Epoch 87/1000\n",
      "355/355 [==============================] - 87s 245ms/step - loss: 532.1520 - val_loss: 434.2608\n",
      "Epoch 88/1000\n",
      "355/355 [==============================] - 88s 247ms/step - loss: 496.7449 - val_loss: 1141.6506\n",
      "Epoch 89/1000\n",
      "355/355 [==============================] - 94s 264ms/step - loss: 468.7637 - val_loss: 1826.3696\n",
      "Epoch 90/1000\n",
      "355/355 [==============================] - 87s 244ms/step - loss: 472.5748 - val_loss: 3400.1597\n",
      "Epoch 91/1000\n",
      "355/355 [==============================] - 86s 243ms/step - loss: 463.0588 - val_loss: 4943.6714\n",
      "Epoch 92/1000\n",
      "355/355 [==============================] - 87s 244ms/step - loss: 437.7230 - val_loss: 1504.6399\n",
      "Epoch 93/1000\n",
      "355/355 [==============================] - 87s 244ms/step - loss: 405.3413 - val_loss: 638.5938\n",
      "Epoch 94/1000\n",
      "355/355 [==============================] - 88s 247ms/step - loss: 432.0475 - val_loss: 12842.6348\n",
      "Epoch 95/1000\n",
      "355/355 [==============================] - 87s 246ms/step - loss: 406.6276 - val_loss: 1575.5344\n",
      "Epoch 96/1000\n",
      "355/355 [==============================] - 87s 245ms/step - loss: 403.8755 - val_loss: 9268.0146\n",
      "Epoch 97/1000\n",
      "355/355 [==============================] - 87s 246ms/step - loss: 397.7638 - val_loss: 3296.1489\n",
      "Epoch 98/1000\n",
      "355/355 [==============================] - 87s 246ms/step - loss: 413.9491 - val_loss: 624.9629\n",
      "Epoch 99/1000\n",
      "355/355 [==============================] - 88s 247ms/step - loss: 454.4109 - val_loss: 5426.9785\n",
      "Epoch 100/1000\n",
      "355/355 [==============================] - 88s 247ms/step - loss: 451.9129 - val_loss: 3327.4910\n",
      "Epoch 101/1000\n",
      "355/355 [==============================] - 89s 250ms/step - loss: 436.2657 - val_loss: 1758.1342\n",
      "Epoch 102/1000\n",
      "355/355 [==============================] - 89s 252ms/step - loss: 386.8758 - val_loss: 3117.2615\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f95c1ab094bd48c3b46a4fdcc56c4a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.265 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.012505…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▁▂▁▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>81</td></tr><tr><td>best_val_loss</td><td>293.61331</td></tr><tr><td>epoch</td><td>101</td></tr><tr><td>loss</td><td>386.87576</td></tr><tr><td>val_loss</td><td>3117.26147</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">woven-lion-80</strong>: <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/30brlfcr\" target=\"_blank\">https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20test%20area/runs/30brlfcr</a><br/>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230207_123606-30brlfcr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = {\n",
    "    'LSTM_units': 8,\n",
    "    'MLP_units': 100,\n",
    "    'LSTM_timesteps': 20,\n",
    "    'LSTM_layers': 3,\n",
    "    'MLP_layers': 4,\n",
    "    'Bn_momentum': 0.99,\n",
    "    'Lr': 0.01,\n",
    "    'Lr_decay': 1,\n",
    "    'Minibatch_size': 4096,\n",
    "    'Min_delta': 1,\n",
    "    'Patience': 20,\n",
    "    'Num_features': 4,\n",
    "    'Architecture': 'LSTM-MLP v.0.1',\n",
    "    'Dataset': '2022: Val split 01.11 with shuffle',\n",
    "}\n",
    "trainer(config = config, project = 'Deep learning for option pricing - test area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d22c5043c0ee61c8547bd88adb0f85d3d6a0a630c1da4282026f99271dac814"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
