{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2MNO3kPTnkd"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlTtOWVUD6YH"
      },
      "source": [
        "### Model decisions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oOQkqhqFTnkg"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3.1 -> 23.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "\n",
            "[notice] A new release of pip available: 22.3.1 -> 23.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Weights and Biases\n",
        "!pip install -q wandb\n",
        "# Tensorflow\n",
        "!pip install -q tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4PZBtROGD6YJ"
      },
      "outputs": [],
      "source": [
        "previous_loading = False\n",
        "google_colab = False\n",
        "moneyness = False\n",
        "lags = 30\n",
        "hyperparameter_search = False\n",
        "training_size = 500000\n",
        "val_size = 100000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ovc8FFP6D6YJ"
      },
      "outputs": [],
      "source": [
        "if google_colab == True:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8xVVTnPD6YJ"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Bfwvf6vGTnki"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, LSTM, Concatenate, Dense, BatchNormalization, LeakyReLU\n",
        "from keras.activations import tanh\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from tensorflow import square, reduce_mean\n",
        "from tensorflow.keras.losses import MSE\n",
        "from tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.math import multiply\n",
        "from tensorflow.keras.metrics import MeanSquaredError, RootMeanSquaredError\n",
        "from math import log\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1fduF6iTTnkj"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33merlendrygg\u001b[0m (\u001b[33mavogadro\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Erlend/.netrc\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# If running in colab, insert your wandb key here\n",
        "\n",
        "#import config\n",
        "#Erlend\n",
        "wandb.login(key=\"3cae81eb56be3190be5bb48c571e69933071df69\")\n",
        "# Hjalmar\n",
        "#wandb.login(key=\"b47bcf387a0571c5520c58a13be35cda8ada0a99\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJGDiGMLTnkl"
      },
      "source": [
        "# Load, split and normalize data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UI1j3mQWTnkl"
      },
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OKcl8WWJTnkl"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>Quote_date</th>\n",
              "      <th>Expire_date</th>\n",
              "      <th>Price</th>\n",
              "      <th>Underlying_last</th>\n",
              "      <th>Strike</th>\n",
              "      <th>TTM</th>\n",
              "      <th>R</th>\n",
              "      <th>Moneyness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2010-01-04</td>\n",
              "      <td>2010-01-07</td>\n",
              "      <td>207.490</td>\n",
              "      <td>1132.99</td>\n",
              "      <td>925.0</td>\n",
              "      <td>3</td>\n",
              "      <td>0.05</td>\n",
              "      <td>1.224854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2010-01-04</td>\n",
              "      <td>2010-01-07</td>\n",
              "      <td>182.500</td>\n",
              "      <td>1132.99</td>\n",
              "      <td>950.0</td>\n",
              "      <td>3</td>\n",
              "      <td>0.05</td>\n",
              "      <td>1.192621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2010-01-04</td>\n",
              "      <td>2010-01-07</td>\n",
              "      <td>157.500</td>\n",
              "      <td>1132.99</td>\n",
              "      <td>975.0</td>\n",
              "      <td>3</td>\n",
              "      <td>0.05</td>\n",
              "      <td>1.162041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2010-01-04</td>\n",
              "      <td>2010-01-07</td>\n",
              "      <td>132.600</td>\n",
              "      <td>1132.99</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>3</td>\n",
              "      <td>0.05</td>\n",
              "      <td>1.132990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2010-01-04</td>\n",
              "      <td>2010-01-07</td>\n",
              "      <td>107.705</td>\n",
              "      <td>1132.99</td>\n",
              "      <td>1025.0</td>\n",
              "      <td>3</td>\n",
              "      <td>0.05</td>\n",
              "      <td>1.105356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11895723</th>\n",
              "      <td>13536141</td>\n",
              "      <td>13536141</td>\n",
              "      <td>2022-12-30</td>\n",
              "      <td>2024-12-20</td>\n",
              "      <td>362.600</td>\n",
              "      <td>3839.81</td>\n",
              "      <td>4300.0</td>\n",
              "      <td>721</td>\n",
              "      <td>4.41</td>\n",
              "      <td>0.892979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11895724</th>\n",
              "      <td>13536142</td>\n",
              "      <td>13536142</td>\n",
              "      <td>2022-12-30</td>\n",
              "      <td>2024-12-20</td>\n",
              "      <td>319.150</td>\n",
              "      <td>3839.81</td>\n",
              "      <td>4400.0</td>\n",
              "      <td>721</td>\n",
              "      <td>4.41</td>\n",
              "      <td>0.872684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11895725</th>\n",
              "      <td>13536143</td>\n",
              "      <td>13536143</td>\n",
              "      <td>2022-12-30</td>\n",
              "      <td>2024-12-20</td>\n",
              "      <td>279.000</td>\n",
              "      <td>3839.81</td>\n",
              "      <td>4500.0</td>\n",
              "      <td>721</td>\n",
              "      <td>4.41</td>\n",
              "      <td>0.853291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11895726</th>\n",
              "      <td>13536144</td>\n",
              "      <td>13536144</td>\n",
              "      <td>2022-12-30</td>\n",
              "      <td>2024-12-20</td>\n",
              "      <td>241.950</td>\n",
              "      <td>3839.81</td>\n",
              "      <td>4600.0</td>\n",
              "      <td>721</td>\n",
              "      <td>4.41</td>\n",
              "      <td>0.834741</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11895727</th>\n",
              "      <td>13536145</td>\n",
              "      <td>13536145</td>\n",
              "      <td>2022-12-30</td>\n",
              "      <td>2024-12-20</td>\n",
              "      <td>208.800</td>\n",
              "      <td>3839.81</td>\n",
              "      <td>4700.0</td>\n",
              "      <td>721</td>\n",
              "      <td>4.41</td>\n",
              "      <td>0.816981</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11895728 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          Unnamed: 0  Unnamed: 0.1  Quote_date Expire_date    Price  \\\n",
              "0                  0             0  2010-01-04  2010-01-07  207.490   \n",
              "1                  1             1  2010-01-04  2010-01-07  182.500   \n",
              "2                  2             2  2010-01-04  2010-01-07  157.500   \n",
              "3                  3             3  2010-01-04  2010-01-07  132.600   \n",
              "4                  4             4  2010-01-04  2010-01-07  107.705   \n",
              "...              ...           ...         ...         ...      ...   \n",
              "11895723    13536141      13536141  2022-12-30  2024-12-20  362.600   \n",
              "11895724    13536142      13536142  2022-12-30  2024-12-20  319.150   \n",
              "11895725    13536143      13536143  2022-12-30  2024-12-20  279.000   \n",
              "11895726    13536144      13536144  2022-12-30  2024-12-20  241.950   \n",
              "11895727    13536145      13536145  2022-12-30  2024-12-20  208.800   \n",
              "\n",
              "          Underlying_last  Strike  TTM     R  Moneyness  \n",
              "0                 1132.99   925.0    3  0.05   1.224854  \n",
              "1                 1132.99   950.0    3  0.05   1.192621  \n",
              "2                 1132.99   975.0    3  0.05   1.162041  \n",
              "3                 1132.99  1000.0    3  0.05   1.132990  \n",
              "4                 1132.99  1025.0    3  0.05   1.105356  \n",
              "...                   ...     ...  ...   ...        ...  \n",
              "11895723          3839.81  4300.0  721  4.41   0.892979  \n",
              "11895724          3839.81  4400.0  721  4.41   0.872684  \n",
              "11895725          3839.81  4500.0  721  4.41   0.853291  \n",
              "11895726          3839.81  4600.0  721  4.41   0.834741  \n",
              "11895727          3839.81  4700.0  721  4.41   0.816981  \n",
              "\n",
              "[11895728 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "if google_colab:\n",
        "    import tensorflow as tf\n",
        "    # Print info\n",
        "    gpu_info = !nvidia-smi\n",
        "    gpu_info = '\\n'.join(gpu_info)\n",
        "    if gpu_info.find('failed') >= 0:\n",
        "        print('Not connected to a GPU')\n",
        "    else:\n",
        "        print(gpu_info)\n",
        "    \n",
        "    from psutil import virtual_memory\n",
        "    ram_gb = virtual_memory().total / 1e9\n",
        "    print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "    if ram_gb < 20:\n",
        "        print('Not using a high-RAM runtime')\n",
        "    else:\n",
        "        print('You are using a high-RAM runtime!')\n",
        "\n",
        "    # Code to read csv file into Colaboratory:\n",
        "    !pip install -U -q PyDrive\n",
        "    from pydrive.auth import GoogleAuth\n",
        "    from pydrive.drive import GoogleDrive\n",
        "    from google.colab import auth\n",
        "    from oauth2client.client import GoogleCredentials\n",
        "    # Authenticate and create the PyDrive client.\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)\n",
        "    id = \"17fLS2n9NC_t2n6rzOpf3LeUJy2iEbdRY\"\n",
        "    downloaded = drive.CreateFile({'id':id}) \n",
        "    downloaded.GetContentFile('2010-2022_filtered.csv')  \n",
        "    df_read = pd.read_csv('2010-2022_filtered.csv')\n",
        "else:\n",
        "    file = \"../data/processed_data/2010-2022_filtered.csv\"\n",
        "    df_read = pd.read_csv(file)\n",
        "\n",
        "display(df_read)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdbRyWt1D6YM"
      },
      "source": [
        "### Create lags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CpcmhVq6Tnkm"
      },
      "outputs": [],
      "source": [
        "df = df_read\n",
        "del df_read\n",
        "\n",
        "# Group the data by Quote Date and calculate the mean for Underlying Price\n",
        "df_agg = df.groupby('Quote_date').mean().reset_index()\n",
        "\n",
        "# Values to returns\n",
        "#df_agg[\"Underlying_return\"] = df_agg[\"Underlying_last\"].pct_change()\n",
        "\n",
        "# Add the Underlying Price Lag column\n",
        "for i in range(1, lags + 1):\n",
        "    df_agg['Underlying_' + str(i)] = df_agg['Underlying_last'].shift(i)\n",
        "\n",
        "df = pd.merge(df, df_agg[['Quote_date'] + ['Underlying_' + str(i) for i in range(1, lags + 1)]], on='Quote_date', how='left')\n",
        "\n",
        "df = df.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1fuXIvmTnkn"
      },
      "source": [
        "### Format input data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YKAFyCmTTnko"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------Dataframe dates--------------\n",
            "Train: 2014-01-03 - 2017-12-29\n",
            "Test: 2018-01-02 - 2018-06-29\n",
            "-------------------------------------------\n",
            "Train shape: (500000, 30, 1), (500000, 4)\n",
            "Val shape: (100000, 30, 1), (100000, 4)\n",
            "Test shape: (599413, 30, 1), (599413, 4)\n"
          ]
        }
      ],
      "source": [
        "# Format settings\n",
        "max_timesteps = lags\n",
        "bs_vars = ['Moneyness', 'TTM', 'R'] if moneyness else ['Underlying_last', 'Strike', 'TTM', 'R']\n",
        "underlying_lags = [f'Underlying_{i}' for i in range (max_timesteps - 1, 0, -1)] + ['Underlying_last']\n",
        "\n",
        "def create_rw_dataset(window_number = 0, df = None):\n",
        "    '''Creates dataset for a single rolling window period offsett by the window number'''\n",
        "\n",
        "    # Create train, validation and test set split points\n",
        "    test_months = 6\n",
        "    train_start = datetime(2014,1,1) + relativedelta(months=window_number * test_months)\n",
        "    #val_start = train_start + relativedelta(months=5*12)\n",
        "    #test_start = val_start + relativedelta(months = 1)\n",
        "    test_start = train_start + relativedelta(months=4*12)\n",
        "    test_end = test_start + relativedelta(months=test_months)\n",
        "    train_start = str(train_start.date())\n",
        "    #val_start = str(val_start.date())\n",
        "    test_start = str(test_start.date())\n",
        "    test_end = str(test_end.date())\n",
        "\n",
        "        \n",
        "    # Split train and validation data\n",
        "    df_train = df[(df['Quote_date'] >= train_start) & (df['Quote_date'] < test_start)]\n",
        "    #df_val = df[(df['Quote_date'] >= val_start) & (df['Quote_date'] < test_start)]\n",
        "    df_test = df[(df['Quote_date'] >= test_start) & (df['Quote_date'] < test_end)]\n",
        "\n",
        "    del df\n",
        "\n",
        "    # Extract target values\n",
        "    train_y = (df_train['Price'] / df_train['Strike']).to_numpy() if moneyness else df_train['Price'].to_numpy()\n",
        "    #val_y = (df_val['Price'] / df_val['Strike']).to_numpy() if moneyness else df_val['Price'].to_numpy()\n",
        "    test_y = (df_test['Price'] / df_test['Strike']).to_numpy() if moneyness else df_test['Price'].to_numpy()\n",
        "\n",
        "    # If usining moneyness, extract strike\n",
        "    if moneyness:\n",
        "        train_strike = df_train['Strike'].to_numpy()\n",
        "    #    val_strike = df_val['Strike'].to_numpy()\n",
        "        test_strike = df_test['Strike'].to_numpy()\n",
        "\n",
        "\n",
        "    # Print earliest and latest date in every dataframe used\n",
        "    print(\"--------------Dataframe dates--------------\")\n",
        "    print(f\"Train: {df_train['Quote_date'].min()} - {df_train['Quote_date'].max()}\")\n",
        "    #print(f\"Val: {df_val['Quote_date'].min()} - {df_val['Quote_date'].max()}\")\n",
        "    print(f\"Test: {df_test['Quote_date'].min()} - {df_test['Quote_date'].max()}\")\n",
        "    print(\"-------------------------------------------\")\n",
        "\n",
        "    # Convert dataframes to numpy arrays\n",
        "    train_x = [df_train[underlying_lags].to_numpy(), df_train[bs_vars].to_numpy()]\n",
        "    #val_x = [df_val[underlying_lags].to_numpy(), df_val[bs_vars].to_numpy()]\n",
        "    test_x = [df_test[underlying_lags].to_numpy(), df_test[bs_vars].to_numpy()]\n",
        "\n",
        "    del df_train\n",
        "    #del df_val\n",
        "\n",
        "    # Scale features based on training set\n",
        "    underlying_scaler = MinMaxScaler()\n",
        "    train_x[0] = underlying_scaler.fit_transform(train_x[0].flatten().reshape(-1, 1)).reshape(train_x[0].shape)\n",
        "    #val_x[0] = underlying_scaler.transform(val_x[0])\n",
        "    test_x[0] = underlying_scaler.transform(test_x[0].flatten().reshape(-1,1)).reshape(test_x[0].shape)\n",
        "\n",
        "    bs_scaler = MinMaxScaler()\n",
        "    train_x[1] = bs_scaler.fit_transform(train_x[1])\n",
        "    #val_x[1] = bs_scaler.transform(val_x[1])\n",
        "    test_x[1] = bs_scaler.transform(test_x[1])\n",
        "\n",
        "\n",
        "    # Shuffle training set\n",
        "    np.random.seed(0)\n",
        "    shuffle = np.random.permutation(len(train_x[0]))\n",
        "    train_x = [train_x[0][shuffle], train_x[1][shuffle]]\n",
        "    train_y = train_y[shuffle]\n",
        "\n",
        "    # Extract validation set\n",
        "    val_x = [train_x[0][training_size:training_size+val_size], train_x[1][training_size:training_size+val_size]]\n",
        "    val_y = train_y[training_size:training_size+val_size]\n",
        "\n",
        "    # Extract training set\n",
        "    train_x = [train_x[0][:training_size], train_x[1][:training_size]]\n",
        "    train_y = train_y[:training_size]\n",
        "\n",
        "    if moneyness:\n",
        "        train_strike = train_strike[shuffle][:training_size]\n",
        "\n",
        "    # Reshape data to fit LSTM\n",
        "    train_x = [train_x[0].reshape(len(train_x[0]), max_timesteps, 1), train_x[1]]\n",
        "    val_x = [val_x[0].reshape(len(val_x[0]), max_timesteps, 1), val_x[1]]\n",
        "    test_x = [test_x[0].reshape(len(test_x[0]), max_timesteps, 1), test_x[1]]\n",
        "\n",
        "    print(f'Train shape: {train_x[0].shape}, {train_x[1].shape}')\n",
        "    print(f'Val shape: {val_x[0].shape}, {val_x[1].shape}')\n",
        "    print(f'Test shape: {test_x[0].shape}, {test_x[1].shape}')\n",
        "\n",
        "    if moneyness:\n",
        "        return train_x, train_y, val_x, val_y, test_x, test_y, train_start, val_start, test_start, df_test, train_strike, val_strike, test_strike,\n",
        "    #return train_x, train_y, val_x, val_y, test_x, test_y, train_start, val_start, test_start, df_test\n",
        "    return train_x, train_y, val_x, val_y, test_x, test_y, train_start, test_start, df_test\n",
        "\n",
        "# Create the dataset for the first rolling window period\n",
        "if moneyness:\n",
        "    train_x, train_y, val_x, val_y, test_x, test_y, train_start, val_start, test_start, df_test, train_strike, val_strike, test_strike = create_rw_dataset(df=df)\n",
        "else:\n",
        "    #train_x, train_y, val_x, val_y, test_x, test_y, train_start, val_start, test_start, df_test = create_rw_dataset(df=df)\n",
        "    train_x, train_y, val_x, val_y, test_x, test_y, train_start, test_start, df_test = create_rw_dataset(df=df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inutSrd8Tnko"
      },
      "source": [
        "# Model construction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gewRG-rHTnkp"
      },
      "outputs": [],
      "source": [
        "def create_model(config):\n",
        "    '''Builds an LSTM-MLP model of minimum 2 layers sequentially from a given config dictionary'''\n",
        "\n",
        "    # Input layers\n",
        "    underlying_history = Input((config.LSTM_timesteps,1))\n",
        "    bs_vars = Input((config.Num_features,))\n",
        "\n",
        "    # LSTM layers\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(LSTM(\n",
        "        units = config.LSTM_units,\n",
        "        activation = tanh,\n",
        "        input_shape = (config.LSTM_timesteps, 1),\n",
        "        return_sequences = True\n",
        "    ))\n",
        "\n",
        "    for _ in range(config.LSTM_layers - 2):\n",
        "        model.add(LSTM(\n",
        "            units = config.LSTM_units,\n",
        "            activation = tanh,\n",
        "            return_sequences = True\n",
        "        ))\n",
        "    \n",
        "    model.add(LSTM(\n",
        "        units = config.Interface_units,\n",
        "        activation = tanh,\n",
        "        return_sequences = False\n",
        "    ))\n",
        "\n",
        "    # MLP layers\n",
        "    layers = Concatenate()([model(underlying_history), model(underlying_history), model(underlying_history), model(underlying_history), model(underlying_history), bs_vars])\n",
        "    \n",
        "    for _ in range(config.MLP_layers - 1):\n",
        "        layers = Dense(config.MLP_units)(layers)\n",
        "        layers = BatchNormalization(momentum=config.Bn_momentum)(layers)\n",
        "        layers = LeakyReLU()(layers)\n",
        "\n",
        "    output = Dense(1, activation='relu')(layers)\n",
        "\n",
        "    # Exponential decaying learning rate\n",
        "    lr_schedule = ExponentialDecay(\n",
        "        initial_learning_rate = config.Lr,\n",
        "        decay_steps = int(len(train_x[0])/config.Minibatch_size),\n",
        "        decay_rate=config.Lr_decay\n",
        "    )\n",
        "\n",
        "    # Compile model\n",
        "    model = Model(inputs=[underlying_history, bs_vars], outputs=output)\n",
        "    model.compile(loss='mse', optimizer=Adam(learning_rate=lr_schedule))\n",
        "\n",
        "    model.summary()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AogdJkrRTnkq"
      },
      "source": [
        "# Help functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5ogZE0y_Tnkq"
      },
      "outputs": [],
      "source": [
        "# Calculate the training and validation MSE loss on the actual option price when using price/strike as the target\n",
        "def MSE_loss(model, train_x, train_y, train_strike, val_x, val_y, val_strike):\n",
        "    train_pred = model(train_x)\n",
        "    val_pred = model(val_x)\n",
        "\n",
        "    train_mse = reduce_mean(square((train_pred[:,0] - train_y)*train_strike))\n",
        "    val_mse = reduce_mean(square((val_pred[:,0] - val_y)*val_strike))\n",
        "\n",
        "    print(f' Training scaled MSE: {train_mse}, Validation scaled MSE: {val_mse}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "xf-ICRDKTnkr"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "from tensorflow.keras import backend as k\n",
        "\n",
        "class ClearMemory(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        gc.collect()\n",
        "        k.clear_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4CRxFJrTnkr"
      },
      "source": [
        "## Creating trainer function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "EZsaCBmrTnkr"
      },
      "outputs": [],
      "source": [
        "def trainer(train_x = train_x, train_y = train_y, val_x = val_x, val_y = val_y, config = None, project = None, previous_checkpoint_path = None, checkpoint_path = None):\n",
        "    # Initialize a new wandb run\n",
        "    with wandb.init(config=config, project = project):\n",
        "\n",
        "        # If called by wandb.agent, as below,\n",
        "        # this config will be set by Sweep Controller\n",
        "        config = wandb.config\n",
        "\n",
        "        # Build model and create callbacks\n",
        "        if previous_checkpoint_path and os.path.exists(previous_checkpoint_path + \".h5\"):\n",
        "            model = load_model(previous_checkpoint_path + \".h5\")\n",
        "        else:\n",
        "            model = create_model(config)\n",
        "\n",
        "        early_stopping = EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            mode='min',\n",
        "            min_delta = config.Min_delta,\n",
        "            patience = config.Patience,\n",
        "        )\n",
        "        \n",
        "        wandb_callback = WandbCallback(\n",
        "            monitor='val_loss',\n",
        "            mode='min',\n",
        "            save_model=False\n",
        "        )\n",
        "        \n",
        "        # Check if the checkpoint folder exists\n",
        "        if checkpoint_path and not os.path.exists(checkpoint_path):\n",
        "            # Create the checkpoint folder if it does not exist\n",
        "            os.makedirs(checkpoint_path)\n",
        "        \n",
        "        checkpoint = ModelCheckpoint(\n",
        "            filepath=checkpoint_path + \".h5\",\n",
        "            monitor='val_loss',\n",
        "            mode='min',\n",
        "            save_best_only=True,\n",
        "            save_weights_only=False\n",
        "        )\n",
        "\n",
        "        # Adapt sequence length to config\n",
        "        train_x_adjusted = [train_x[0][:, :config.LSTM_timesteps, :], train_x[1]]\n",
        "        val_x_adjusted = [val_x[0][:, :config.LSTM_timesteps, :], val_x[1]]\n",
        "        print(f'Train shape: {train_x_adjusted[0].shape}, {train_x_adjusted[0].shape}')\n",
        "        print(f'Val shape: {val_x_adjusted[0].shape}, {val_x_adjusted[0].shape}')\n",
        "\n",
        "        # Train model\n",
        "        model.fit(\n",
        "            train_x_adjusted,\n",
        "            train_y,\n",
        "            batch_size = config.Minibatch_size,\n",
        "            validation_data = (val_x_adjusted, val_y),\n",
        "            epochs = 1000,\n",
        "            callbacks = [early_stopping, wandb_callback, checkpoint, ClearMemory()] if checkpoint_path else [early_stopping, wandb_callback, ClearMemory()],\n",
        "        )\n",
        "\n",
        "        if moneyness:\n",
        "            MSE_loss(model, train_x, train_y, train_strike, val_x, val_y, val_strike)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rqno7WFBTnkp"
      },
      "source": [
        "# Hyperparameter search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "__KmrL9HTnkp"
      },
      "outputs": [],
      "source": [
        "# Configuring the sweep hyperparameter search space\n",
        "sweep_configuration = {\n",
        "    'method': 'bayes',\n",
        "    'name': 'LSTM-MLP v4.0: fix nan issue',\n",
        "    'metric': {\n",
        "        'goal': 'minimize', \n",
        "        'name': 'val_loss'\n",
        "\t\t},\n",
        "    'parameters': {\n",
        "        'LSTM_units': {\n",
        "            'values': [4, 8, 16, 32]},\n",
        "        'Interface_units': {\n",
        "            'values': [4, 8, 16, 32]},\n",
        "        'MLP_units': {\n",
        "            'values': [50, 100, 200, 400, 600]},\n",
        "        'LSTM_timesteps': {\n",
        "            'values': [10, 20, 40, 60, 90, 150]},\n",
        "        'LSTM_layers': {\n",
        "            'distribution': 'int_uniform',\n",
        "            'max': 8, 'min': 2},\n",
        "        'MLP_layers': {\n",
        "            'distribution': 'int_uniform',\n",
        "            'max': 8, 'min': 2},\n",
        "        'Bn_momentum': {\n",
        "            'values': [0.1, 0.4, 0.7, 0.99]},\n",
        "        'Lr': {\n",
        "            'distribution': 'log_uniform',\n",
        "            'max': log(0.1), 'min': log(0.0001)},\n",
        "        'Lr_decay': {\n",
        "            'distribution': 'log_uniform',\n",
        "            'max': log(1), 'min': log(0.8)},        \n",
        "        'Minibatch_size': {\n",
        "            'value': 4096},\n",
        "        'Min_delta': {\n",
        "            'value': 0.01 if moneyness else 1},\n",
        "        'Patience': {\n",
        "            'value': 20},\n",
        "        'Num_features': {\n",
        "            'value': 3 if moneyness else 4},\n",
        "    }\n",
        "}\n",
        "\n",
        "# Initialize sweep and creating sweepID\n",
        "\n",
        "# If new sweep, uncomment the line below and comment the line after it\n",
        "#sweep_id = wandb.sweep(sweep=sweep_configuration, project='Deep learning for option pricing - test area') \n",
        "#sweep_id = '98bxt6oq'\n",
        "\n",
        "if hyperparameter_search:\n",
        "    wandb.agent(sweep_id=sweep_id, function=trainer, project='Deep learning for option pricing - test area', count = 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaS4BiSNTnks"
      },
      "source": [
        "# Rolling window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ifDAjXS3Tnks"
      },
      "outputs": [],
      "source": [
        "def calculate_error(predictions, original):\n",
        "    m = MeanSquaredError()\n",
        "    m.update_state(predictions, original)\n",
        "    print(\"MSE:\", m.result().numpy())\n",
        "    m = RootMeanSquaredError()\n",
        "    m.update_state(predictions, original)\n",
        "    print(\"RMSE:\", m.result().numpy())\n",
        "\n",
        "class config_object:\n",
        "    def __init__(self, config):\n",
        "        self.LSTM_units = config['LSTM_units']\n",
        "        self.Interface_units = config['Interface_units']\n",
        "        self.MLP_units = config['MLP_units']\n",
        "        self.LSTM_timesteps = config['LSTM_timesteps']\n",
        "        self.LSTM_layers = config['LSTM_layers']\n",
        "        self.MLP_layers = config['MLP_layers']\n",
        "        self.Bn_momentum = config['Bn_momentum']\n",
        "        self.Lr = config['Lr']\n",
        "        self.Lr_decay = config['Lr_decay']\n",
        "        self.Minibatch_size = config['Minibatch_size']\n",
        "        self.Min_delta = config['Min_delta']\n",
        "        self.Patience = config['Patience']\n",
        "        self.Num_features = config['Num_features']\n",
        "        self.Architecture = config['Architecture']\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "lRV8wlklTnks"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------Dataframe dates--------------\n",
            "Train: 2014-01-03 - 2017-12-29\n",
            "Test: 2018-01-02 - 2018-06-29\n",
            "-------------------------------------------\n",
            "Train shape: (500000, 30, 1), (500000, 4)\n",
            "Val shape: (100000, 30, 1), (100000, 4)\n",
            "Test shape: (599413, 30, 1), (599413, 4)\n",
            "-------------------------------------------\n",
            "Window 1 of 2\n",
            "Training start:  2013-01-01 Test start:  2018-01-01\n",
            "Loading previous checkpoint:  False\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.15.0 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\Erlend\\Google Drive\\NTNU\\5. Klasse\\Master\\Git-folder\\deep-learning-for-option-pricing\\LSTM-MLP_notebooks\\wandb\\run-20230426_131654-2nu4tiu3</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20rolling%20windows/runs/2nu4tiu3\" target=\"_blank\">lunar-water-855</a></strong> to <a href=\"https://wandb.ai/avogadro/Deep%20learning%20for%20option%20pricing%20-%20rolling%20windows\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 30, 1)]      0           []                               \n",
            "                                                                                                  \n",
            " sequential (Sequential)        (None, 16)           2016        ['input_1[0][0]',                \n",
            "                                                                  'input_1[0][0]',                \n",
            "                                                                  'input_1[0][0]',                \n",
            "                                                                  'input_1[0][0]',                \n",
            "                                                                  'input_1[0][0]']                \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 4)]          0           []                               \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 84)           0           ['sequential[0][0]',             \n",
            "                                                                  'sequential[1][0]',             \n",
            "                                                                  'sequential[2][0]',             \n",
            "                                                                  'sequential[3][0]',             \n",
            "                                                                  'sequential[4][0]',             \n",
            "                                                                  'input_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 400)          34000       ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 400)         1600        ['dense[0][0]']                  \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " leaky_re_lu (LeakyReLU)        (None, 400)          0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 400)          160400      ['leaky_re_lu[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 400)         1600        ['dense_1[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " leaky_re_lu_1 (LeakyReLU)      (None, 400)          0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 400)          160400      ['leaky_re_lu_1[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 400)         1600        ['dense_2[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " leaky_re_lu_2 (LeakyReLU)      (None, 400)          0           ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 400)          160400      ['leaky_re_lu_2[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 400)         1600        ['dense_3[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " leaky_re_lu_3 (LeakyReLU)      (None, 400)          0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 1)            401         ['leaky_re_lu_3[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 524,017\n",
            "Trainable params: 520,817\n",
            "Non-trainable params: 3,200\n",
            "__________________________________________________________________________________________________\n",
            "Train shape: (500000, 30, 1), (500000, 30, 1)\n",
            "Val shape: (100000, 30, 1), (100000, 30, 1)\n",
            "Epoch 1/1000\n",
            "  3/123 [..............................] - ETA: 4:31 - loss: 150361.9219 "
          ]
        }
      ],
      "source": [
        "num_windows = 2 #84\n",
        "\n",
        "config = {\n",
        "    'LSTM_units': 4,\n",
        "    'Interface_units': 16,\n",
        "    'MLP_units': 400,\n",
        "    'LSTM_timesteps': lags,\n",
        "    'LSTM_layers': 6,\n",
        "    'MLP_layers': 5,\n",
        "    'Bn_momentum': 0.7,\n",
        "    'Lr': 0.005,\n",
        "    'Lr_decay': 0.9,\n",
        "    'Minibatch_size': 4096,\n",
        "    'Min_delta': 0.01 if moneyness else 1,\n",
        "    'Patience': 20,\n",
        "    'Num_features': 3 if moneyness else 4, \n",
        "    'Architecture': 'LSTM-MLP v.E.1',\n",
        "}\n",
        "\n",
        "df_test_combined = pd.DataFrame()\n",
        "\n",
        "checkpoint_time = datetime.now().strftime(\"%m-%d_%H-%M\")\n",
        "\n",
        "previous_checkpoint_path = None\n",
        "\n",
        "for window in range(num_windows):\n",
        "    if moneyness:\n",
        "        train_x, train_y, val_x, val_y, test_x, test_y, train_start, val_start, test_start, df_test, train_strike, val_strike, test_strike = create_rw_dataset(df=df, window_number=window)\n",
        "    else:\n",
        "        #train_x, train_y, val_x, val_y, test_x, test_y, train_start, val_start, test_start, df_test = create_rw_dataset(df=df, window_number=window)\n",
        "        train_x, train_y, val_x, val_y, test_x, test_y, train_start, test_start, df_test = create_rw_dataset(df=df, window_number=window)\n",
        "\n",
        "    print('-------------------------------------------')\n",
        "    print(f'Window {window + 1} of {num_windows}')\n",
        "    #print(\"Training start: \", train_start, \"Validation start: \", val_start, \"Test start: \", test_start)\n",
        "    print(\"Training start: \", train_start, \"Test start: \", test_start)\n",
        "    print(\"Loading previous checkpoint: \", previous_loading)\n",
        "        \n",
        "    if google_colab:\n",
        "        checkpoint_path = f'/content/drive/MyDrive/01. Masters Thesis - Shared/05. Checkpoints/{checkpoint_time}/{train_start}/'\n",
        "    else:\n",
        "        checkpoint_path = f'./checkpoints/{checkpoint_time}/{train_start}/'\n",
        "\n",
        "    #config['Dataset'] = f'{train_start} - {val_start} - {test_start}'\n",
        "    config['Dataset'] = f'{train_start} - {test_start}'\n",
        "\n",
        "    trainer(train_x = train_x, train_y = train_y, val_x = val_x, val_y = val_y, config = config, project = 'Deep learning for option pricing - rolling windows',  previous_checkpoint_path = previous_checkpoint_path, checkpoint_path = checkpoint_path)\n",
        "    c_model = load_model(checkpoint_path + \".h5\")\n",
        "    \n",
        "    # Split test_x into five parts\n",
        "    part_size = len(test_x[0]) // 5\n",
        "    test_x_parts = []\n",
        "    for i in range(5):\n",
        "        subset_1 = test_x[0][i * part_size:(i + 1) * part_size]\n",
        "        subset_2 = test_x[1][i * part_size:(i + 1) * part_size]\n",
        "        test_x_parts.append((subset_1, subset_2))\n",
        "\n",
        "    # Adjust the last part to include any remaining data\n",
        "    test_x_parts[-1] = (test_x[0][(len(test_x_parts) - 1) * part_size:], test_x[1][(len(test_x_parts) - 1) * part_size:])\n",
        "\n",
        "    # Make predictions for each part\n",
        "    predictions_parts = [np.array(c_model(part)) for part in test_x_parts]\n",
        "\n",
        "    # Combine the predictions\n",
        "    predictions = np.concatenate(predictions_parts, axis=0)\n",
        "\n",
        "    print(f'--- Predictions for test_start {test_start} ---')\n",
        "    calculate_error(predictions, test_y)\n",
        "    print('-------------------------------------------')\n",
        "    \n",
        "    df_test[\"Prediction\"] = predictions\n",
        "    df_test_combined = pd.concat([df_test_combined, df_test[[\"Quote_date\", \"Price\", \"Prediction\"] + bs_vars]])\n",
        "\n",
        "    if previous_loading:\n",
        "       previous_checkpoint_path = checkpoint_path\n",
        "       \n",
        "\n",
        "print(f\"--- All model predictions ---\")\n",
        "calculate_error(df_test_combined[\"Prediction\"], df_test_combined[\"Price\"])\n",
        "print(\"-------------------------------------------\")\n",
        "\n",
        "if google_colab == False:\n",
        "    predictions_path = './predictions/'\n",
        "    if checkpoint_path and not os.path.exists(predictions_path):\n",
        "        os.makedirs(predictions_path)\n",
        "    df_test_combined.to_csv(f'{predictions_path}{datetime.now().strftime(\"%m-%d_%H-%M\")}.csv')\n",
        "else:\n",
        "  path = '/content/drive/MyDrive/01. Masters Thesis - Shared/06. Predictions/41.csv'\n",
        "  df_test_combined.to_csv(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a function to calculate RMSE\n",
        "def calculate_rmse(group):\n",
        "    rmse_model_1 = np.sqrt(mean_squared_error(group[\"Price\"], group[\"Model 1\"]))\n",
        "    return pd.Series({\"RMSE_Model_1\": rmse_model_1})\n",
        "\n",
        "# Group data by Quote_date and calculate RMSE for each group\n",
        "rmse_df = df.groupby(\"Quote_date\").apply(calculate_rmse).reset_index()\n",
        "\n",
        "# Plot the RMSE values for each model\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "sns.lineplot(data=rmse_df, x=\"Quote_date\", y=\"RMSE_Model_1\", label=\"Model\", ax=ax)\n",
        "\n",
        "# Set the interval for x-axis labels\n",
        "ax.xaxis.set_major_locator(ticker.MultipleLocator(base=10))\n",
        "\n",
        "plt.xlabel(\"Quote_date\")\n",
        "plt.ylabel(\"RMSE\")\n",
        "plt.title(\"RMSE for Models\")\n",
        "plt.legend()\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
