{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2MNO3kPTnkd"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlTtOWVUD6YH"
      },
      "source": [
        "### Model decisions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOQkqhqFTnkg"
      },
      "outputs": [],
      "source": [
        "# Weights and Biases\n",
        "!pip install -q wandb\n",
        "# Tensorflow\n",
        "!pip install -q tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PZBtROGD6YJ"
      },
      "outputs": [],
      "source": [
        "previous_loading = False\n",
        "google_colab = True\n",
        "moneyness = False\n",
        "lags = 10\n",
        "hyperparameter_search = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ovc8FFP6D6YJ"
      },
      "outputs": [],
      "source": [
        "if google_colab == True:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8xVVTnPD6YJ"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bfwvf6vGTnki"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, LSTM, Concatenate, Dense, BatchNormalization, LeakyReLU\n",
        "from keras.activations import tanh\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from tensorflow import square, reduce_mean\n",
        "from tensorflow.keras.losses import MSE\n",
        "from tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.math import multiply\n",
        "from tensorflow.keras.metrics import MeanSquaredError, RootMeanSquaredError\n",
        "from math import log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fduF6iTTnkj"
      },
      "outputs": [],
      "source": [
        "# If running in colab, insert your wandb key here\n",
        "\n",
        "#import config\n",
        "#Erlend\n",
        "#wandb.login(key=config.erlend_key)\n",
        "# Hjalmar\n",
        "wandb.login(key=\"b47bcf387a0571c5520c58a13be35cda8ada0a99\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJGDiGMLTnkl"
      },
      "source": [
        "# Load, split and normalize data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UI1j3mQWTnkl"
      },
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKcl8WWJTnkl"
      },
      "outputs": [],
      "source": [
        "if google_colab:\n",
        "    import tensorflow as tf\n",
        "    # Print info\n",
        "    gpu_info = !nvidia-smi\n",
        "    gpu_info = '\\n'.join(gpu_info)\n",
        "    if gpu_info.find('failed') >= 0:\n",
        "        print('Not connected to a GPU')\n",
        "    else:\n",
        "        print(gpu_info)\n",
        "    \n",
        "    from psutil import virtual_memory\n",
        "    ram_gb = virtual_memory().total / 1e9\n",
        "    print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "    if ram_gb < 20:\n",
        "        print('Not using a high-RAM runtime')\n",
        "    else:\n",
        "        print('You are using a high-RAM runtime!')\n",
        "\n",
        "    # Code to read csv file into Colaboratory:\n",
        "    !pip install -U -q PyDrive\n",
        "    from pydrive.auth import GoogleAuth\n",
        "    from pydrive.drive import GoogleDrive\n",
        "    from google.colab import auth\n",
        "    from oauth2client.client import GoogleCredentials\n",
        "    # Authenticate and create the PyDrive client.\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)\n",
        "    id = \"1eTbdlQx4KeKaBYn5g5kSI_PkFddMsLfH\"\n",
        "    downloaded = drive.CreateFile({'id':id}) \n",
        "    downloaded.GetContentFile('2014_2022_moneyness_and_TTM_filtered.csv')  \n",
        "    df_read = pd.read_csv('2014_2022_moneyness_and_TTM_filtered.csv')\n",
        "else:\n",
        "    file = \"../data/processed_data/2014_2022_moneyness_and_TTM_filtered.csv\"\n",
        "    df_read = pd.read_csv(file)\n",
        "\n",
        "display(df_read)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdbRyWt1D6YM"
      },
      "source": [
        "### Create lags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpcmhVq6Tnkm"
      },
      "outputs": [],
      "source": [
        "df = df_read\n",
        "del df_read\n",
        "\n",
        "# Group the data by Quote Date and calculate the mean for Underlying Price\n",
        "df_agg = df.groupby('Quote_date').mean().reset_index()\n",
        "\n",
        "# Values to returns\n",
        "df_agg[\"Underlying_return\"] = df_agg[\"Underlying_last\"].pct_change()\n",
        "\n",
        "# Add the Underlying Price Lag column\n",
        "for i in range(1, lags + 1):\n",
        "    df_agg['Underlying_' + str(i)] = df_agg['Underlying_return'].shift(i)\n",
        "\n",
        "df = pd.merge(df, df_agg[['Quote_date', \"Underlying_return\"] + ['Underlying_' + str(i) for i in range(1, lags + 1)]], on='Quote_date', how='left')\n",
        "\n",
        "# Filter df between 2014-01-01 and 2022-12-31\n",
        "df = df[(df[\"Quote_date\"] >= \"2014-01-01\") & (df[\"Quote_date\"] <= \"2022-12-31\")]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1fuXIvmTnkn"
      },
      "source": [
        "### Format input data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKAFyCmTTnko"
      },
      "outputs": [],
      "source": [
        "# Format settings\n",
        "max_timesteps = lags\n",
        "bs_vars = ['Moneyness', 'TTM', 'R'] if moneyness else ['Underlying_last', 'Strike', 'TTM', 'R']\n",
        "underlying_lags = ['Underlying_last'] + [f'Underlying_{i}' for i in range (1, max_timesteps)]\n",
        "\n",
        "def create_rw_dataset(window_number = 0, df = None):\n",
        "    '''Creates dataset for a single rolling window period offsett by the window number'''\n",
        "\n",
        "    # Create train, validation and test set split points\n",
        "    test_weeks = 2\n",
        "    train_start = datetime(2017,7,15) + relativedelta(weeks=window_number * test_weeks)\n",
        "    val_start = train_start + relativedelta(months=5)\n",
        "    test_start = val_start + relativedelta(weeks=1)\n",
        "    test_end = test_start + relativedelta(weeks=test_weeks)\n",
        "    train_start = str(train_start.date())\n",
        "    val_start = str(val_start.date())\n",
        "    test_start = str(test_start.date())\n",
        "    test_end = str(test_end.date())\n",
        "\n",
        "        \n",
        "    # Split train and validation data\n",
        "    df_train = df[(df['Quote_date'] >= train_start) & (df['Quote_date'] < val_start)]\n",
        "    df_val = df[(df['Quote_date'] >= val_start) & (df['Quote_date'] < test_start)]\n",
        "    df_test = df[(df['Quote_date'] >= test_start) & (df['Quote_date'] < test_end)]\n",
        "\n",
        "    del df\n",
        "\n",
        "    # Extract target values\n",
        "    train_y = (df_train['Price'] / df_train['Strike']).to_numpy() if moneyness else df_train['Price'].to_numpy()\n",
        "    val_y = (df_val['Price'] / df_val['Strike']).to_numpy() if moneyness else df_val['Price'].to_numpy()\n",
        "    test_y = (df_test['Price'] / df_test['Strike']).to_numpy() if moneyness else df_test['Price'].to_numpy()\n",
        "\n",
        "    # If usining moneyness, extract strike\n",
        "    if moneyness:\n",
        "        train_strike = df_train['Strike'].to_numpy()\n",
        "        val_strike = df_val['Strike'].to_numpy()\n",
        "        test_strike = df_test['Strike'].to_numpy()\n",
        "\n",
        "\n",
        "    # Print earliest and latest date in every dataframe used\n",
        "    print(\"--------------Dataframe dates--------------\")\n",
        "    print(f\"Train: {df_train['Quote_date'].min()} - {df_train['Quote_date'].max()}\")\n",
        "    print(f\"Val: {df_val['Quote_date'].min()} - {df_val['Quote_date'].max()}\")\n",
        "    print(f\"Test: {df_test['Quote_date'].min()} - {df_test['Quote_date'].max()}\")\n",
        "    print(\"-------------------------------------------\")\n",
        "\n",
        "    # Convert dataframes to numpy arrays\n",
        "    train_x = [df_train[underlying_lags].to_numpy(), df_train[bs_vars].to_numpy()]\n",
        "    val_x = [df_val[underlying_lags].to_numpy(), df_val[bs_vars].to_numpy()]\n",
        "    test_x = [df_test[underlying_lags].to_numpy(), df_test[bs_vars].to_numpy()]\n",
        "\n",
        "    del df_train\n",
        "    del df_val\n",
        "\n",
        "    # Scale features based on training set\n",
        "    underlying_scaler = MinMaxScaler()\n",
        "    train_x[0] = underlying_scaler.fit_transform(train_x[0])\n",
        "    val_x[0] = underlying_scaler.transform(val_x[0])\n",
        "    test_x[0] = underlying_scaler.transform(test_x[0])\n",
        "\n",
        "    bs_scaler = MinMaxScaler()\n",
        "    train_x[1] = bs_scaler.fit_transform(train_x[1])\n",
        "    val_x[1] = bs_scaler.transform(val_x[1])\n",
        "    test_x[1] = bs_scaler.transform(test_x[1])\n",
        "\n",
        "\n",
        "    # Shuffle training set\n",
        "    np.random.seed(0)\n",
        "    shuffle = np.random.permutation(len(train_x[0]))\n",
        "    train_x = [train_x[0][shuffle], train_x[1][shuffle]]\n",
        "    train_y = train_y[shuffle]\n",
        "    if moneyness:\n",
        "        train_strike = train_strike[shuffle]\n",
        "\n",
        "    # Reshape data to fit LSTM\n",
        "    train_x = [train_x[0].reshape(len(train_x[0]), max_timesteps, 1), train_x[1]]\n",
        "    val_x = [val_x[0].reshape(len(val_x[0]), max_timesteps, 1), val_x[1]]\n",
        "    test_x = [test_x[0].reshape(len(test_x[0]), max_timesteps, 1), test_x[1]]\n",
        "\n",
        "    print(f'Train shape: {train_x[0].shape}, {train_x[1].shape}')\n",
        "    print(f'Val shape: {val_x[0].shape}, {val_x[1].shape}')\n",
        "    print(f'Test shape: {test_x[0].shape}, {test_x[1].shape}')\n",
        "\n",
        "    if moneyness:\n",
        "        return train_x, train_y, val_x, val_y, test_x, test_y, train_start, val_start, test_start, df_test, train_strike, val_strike, test_strike,\n",
        "    return train_x, train_y, val_x, val_y, test_x, test_y, train_start, val_start, test_start, df_test\n",
        "\n",
        "# Create the dataset for the first rolling window period\n",
        "if moneyness:\n",
        "    train_x, train_y, val_x, val_y, test_x, test_y, train_start, val_start, test_start, df_test, train_strike, val_strike, test_strike = create_rw_dataset(df=df)\n",
        "else:\n",
        "    train_x, train_y, val_x, val_y, test_x, test_y, train_start, val_start, test_start, df_test = create_rw_dataset(df=df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inutSrd8Tnko"
      },
      "source": [
        "# Model construction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gewRG-rHTnkp"
      },
      "outputs": [],
      "source": [
        "def create_model(config):\n",
        "    '''Builds an LSTM-MLP model of minimum 2 layers sequentially from a given config dictionary'''\n",
        "\n",
        "    # Input layers\n",
        "    underlying_history = Input((config.LSTM_timesteps,1))\n",
        "    bs_vars = Input((config.Num_features,))\n",
        "\n",
        "    # LSTM layers\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(LSTM(\n",
        "        units = config.LSTM_units,\n",
        "        activation = tanh,\n",
        "        input_shape = (config.LSTM_timesteps, 1),\n",
        "        return_sequences = True\n",
        "    ))\n",
        "\n",
        "    for _ in range(config.LSTM_layers - 2):\n",
        "        model.add(LSTM(\n",
        "            units = config.LSTM_units,\n",
        "            activation = tanh,\n",
        "            return_sequences = True\n",
        "        ))\n",
        "    \n",
        "    model.add(LSTM(\n",
        "        units = config.Interface_units,\n",
        "        activation = tanh,\n",
        "        return_sequences = False\n",
        "    ))\n",
        "\n",
        "    # MLP layers\n",
        "    layers = Concatenate()([model(underlying_history), model(underlying_history), model(underlying_history), model(underlying_history), model(underlying_history), bs_vars])\n",
        "    \n",
        "    for _ in range(config.MLP_layers - 1):\n",
        "        layers = Dense(config.MLP_units)(layers)\n",
        "        layers = BatchNormalization(momentum=config.Bn_momentum)(layers)\n",
        "        layers = LeakyReLU()(layers)\n",
        "\n",
        "    output = Dense(1, activation='relu')(layers)\n",
        "\n",
        "    # Exponential decaying learning rate\n",
        "    lr_schedule = ExponentialDecay(\n",
        "        initial_learning_rate = config.Lr,\n",
        "        decay_steps = int(len(train_x[0])/config.Minibatch_size),\n",
        "        decay_rate=config.Lr_decay\n",
        "    )\n",
        "\n",
        "    # Compile model\n",
        "    model = Model(inputs=[underlying_history, bs_vars], outputs=output)\n",
        "    model.compile(loss='mse', optimizer=Adam(learning_rate=lr_schedule))\n",
        "\n",
        "    model.summary()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AogdJkrRTnkq"
      },
      "source": [
        "# Help functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ogZE0y_Tnkq"
      },
      "outputs": [],
      "source": [
        "# Calculate the training and validation MSE loss on the actual option price when using price/strike as the target\n",
        "def MSE_loss(model, train_x, train_y, train_strike, val_x, val_y, val_strike):\n",
        "    train_pred = model(train_x)\n",
        "    val_pred = model(val_x)\n",
        "\n",
        "    train_mse = reduce_mean(square((train_pred[:,0] - train_y)*train_strike))\n",
        "    val_mse = reduce_mean(square((val_pred[:,0] - val_y)*val_strike))\n",
        "\n",
        "    print(f' Training scaled MSE: {train_mse}, Validation scaled MSE: {val_mse}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xf-ICRDKTnkr"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "from tensorflow.keras import backend as k\n",
        "\n",
        "class ClearMemory(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        gc.collect()\n",
        "        k.clear_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4CRxFJrTnkr"
      },
      "source": [
        "## Creating trainer function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZsaCBmrTnkr"
      },
      "outputs": [],
      "source": [
        "def trainer(train_x = train_x, train_y = train_y, val_x = val_x, val_y = val_y, config = None, project = None, previous_checkpoint_path = None, checkpoint_path = None):\n",
        "    # Initialize a new wandb run\n",
        "    with wandb.init(config=config, project = project):\n",
        "\n",
        "        # If called by wandb.agent, as below,\n",
        "        # this config will be set by Sweep Controller\n",
        "        config = wandb.config\n",
        "\n",
        "        # Build model and create callbacks\n",
        "        if previous_checkpoint_path and os.path.exists(previous_checkpoint_path + \".h5\"):\n",
        "            model = load_model(previous_checkpoint_path + \".h5\")\n",
        "        else:\n",
        "            model = create_model(config)\n",
        "\n",
        "        early_stopping = EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            mode='min',\n",
        "            min_delta = config.Min_delta,\n",
        "            patience = config.Patience,\n",
        "        )\n",
        "        \n",
        "        wandb_callback = WandbCallback(\n",
        "            monitor='val_loss',\n",
        "            mode='min',\n",
        "            save_model=False\n",
        "        )\n",
        "        \n",
        "        # Check if the checkpoint folder exists\n",
        "        if checkpoint_path and not os.path.exists(checkpoint_path):\n",
        "            # Create the checkpoint folder if it does not exist\n",
        "            os.makedirs(checkpoint_path)\n",
        "        \n",
        "        checkpoint = ModelCheckpoint(\n",
        "            filepath=checkpoint_path + \".h5\",\n",
        "            monitor='val_loss',\n",
        "            mode='min',\n",
        "            save_best_only=True,\n",
        "            save_weights_only=False\n",
        "        )\n",
        "\n",
        "        # Adapt sequence length to config\n",
        "        train_x_adjusted = [train_x[0][:, :config.LSTM_timesteps, :], train_x[1]]\n",
        "        val_x_adjusted = [val_x[0][:, :config.LSTM_timesteps, :], val_x[1]]\n",
        "        print(f'Train shape: {train_x_adjusted[0].shape}, {train_x_adjusted[0].shape}')\n",
        "        print(f'Val shape: {val_x_adjusted[0].shape}, {val_x_adjusted[0].shape}')\n",
        "\n",
        "        # Train model\n",
        "        model.fit(\n",
        "            train_x_adjusted,\n",
        "            train_y,\n",
        "            batch_size = config.Minibatch_size,\n",
        "            validation_data = (val_x_adjusted, val_y),\n",
        "            epochs = 1000,\n",
        "            callbacks = [early_stopping, wandb_callback, checkpoint, ClearMemory()] if checkpoint_path else [early_stopping, wandb_callback, ClearMemory()],\n",
        "        )\n",
        "\n",
        "        if moneyness:\n",
        "            MSE_loss(model, train_x, train_y, train_strike, val_x, val_y, val_strike)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rqno7WFBTnkp"
      },
      "source": [
        "# Hyperparameter search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__KmrL9HTnkp"
      },
      "outputs": [],
      "source": [
        "# Configuring the sweep hyperparameter search space\n",
        "sweep_configuration = {\n",
        "    'method': 'bayes',\n",
        "    'name': 'LSTM-MLP v4.0: fix nan issue',\n",
        "    'metric': {\n",
        "        'goal': 'minimize', \n",
        "        'name': 'val_loss'\n",
        "\t\t},\n",
        "    'parameters': {\n",
        "        'LSTM_units': {\n",
        "            'values': [4, 8, 16, 32]},\n",
        "        'Interface_units': {\n",
        "            'values': [4, 8, 16, 32]},\n",
        "        'MLP_units': {\n",
        "            'values': [50, 100, 200, 400, 600]},\n",
        "        'LSTM_timesteps': {\n",
        "            'values': [10, 20, 40, 60, 90, 150]},\n",
        "        'LSTM_layers': {\n",
        "            'distribution': 'int_uniform',\n",
        "            'max': 8, 'min': 2},\n",
        "        'MLP_layers': {\n",
        "            'distribution': 'int_uniform',\n",
        "            'max': 8, 'min': 2},\n",
        "        'Bn_momentum': {\n",
        "            'values': [0.1, 0.4, 0.7, 0.99]},\n",
        "        'Lr': {\n",
        "            'distribution': 'log_uniform',\n",
        "            'max': log(0.1), 'min': log(0.0001)},\n",
        "        'Lr_decay': {\n",
        "            'distribution': 'log_uniform',\n",
        "            'max': log(1), 'min': log(0.8)},        \n",
        "        'Minibatch_size': {\n",
        "            'value': 4096},\n",
        "        'Min_delta': {\n",
        "            'value': 0.01 if moneyness else 1},\n",
        "        'Patience': {\n",
        "            'value': 20},\n",
        "        'Num_features': {\n",
        "            'value': 3 if moneyness else 4},\n",
        "    }\n",
        "}\n",
        "\n",
        "# Initialize sweep and creating sweepID\n",
        "\n",
        "# If new sweep, uncomment the line below and comment the line after it\n",
        "sweep_id = wandb.sweep(sweep=sweep_configuration, project='Deep learning for option pricing - test area') \n",
        "#sweep_id = '98bxt6oq'\n",
        "\n",
        "if hyperparameter_search:\n",
        "    wandb.agent(sweep_id=sweep_id, function=trainer, project='Deep learning for option pricing - test area', count = 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaS4BiSNTnks"
      },
      "source": [
        "# Rolling window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifDAjXS3Tnks"
      },
      "outputs": [],
      "source": [
        "def calculate_error(predictions, original):\n",
        "    m = MeanSquaredError()\n",
        "    m.update_state(predictions, original)\n",
        "    print(\"MSE:\", m.result().numpy())\n",
        "    m = RootMeanSquaredError()\n",
        "    m.update_state(predictions, original)\n",
        "    print(\"RMSE:\", m.result().numpy())\n",
        "\n",
        "class config_object:\n",
        "    def __init__(self, config):\n",
        "        self.LSTM_units = config['LSTM_units']\n",
        "        self.Interface_units = config['Interface_units']\n",
        "        self.MLP_units = config['MLP_units']\n",
        "        self.LSTM_timesteps = config['LSTM_timesteps']\n",
        "        self.LSTM_layers = config['LSTM_layers']\n",
        "        self.MLP_layers = config['MLP_layers']\n",
        "        self.Bn_momentum = config['Bn_momentum']\n",
        "        self.Lr = config['Lr']\n",
        "        self.Lr_decay = config['Lr_decay']\n",
        "        self.Minibatch_size = config['Minibatch_size']\n",
        "        self.Min_delta = config['Min_delta']\n",
        "        self.Patience = config['Patience']\n",
        "        self.Num_features = config['Num_features']\n",
        "        self.Architecture = config['Architecture']\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRV8wlklTnks"
      },
      "outputs": [],
      "source": [
        "num_windows = 12 #84\n",
        "\n",
        "config = {\n",
        "    'LSTM_units': 4,\n",
        "    'Interface_units': 16,\n",
        "    'MLP_units': 600,\n",
        "    'LSTM_timesteps': lags,\n",
        "    'LSTM_layers': 6,\n",
        "    'MLP_layers': 7,\n",
        "    'Bn_momentum': 0.7,\n",
        "    'Lr': 0.005,\n",
        "    'Lr_decay': 0.82,\n",
        "    'Minibatch_size': 4096,\n",
        "    'Min_delta': 0.01 if moneyness else 1,\n",
        "    'Patience': 20,\n",
        "    'Num_features': 3 if moneyness else 4, \n",
        "    'Architecture': 'LSTM-MLP v.5.1',\n",
        "}\n",
        "\n",
        "df_test_combined = pd.DataFrame()\n",
        "\n",
        "checkpoint_time = datetime.now().strftime(\"%m-%d_%H-%M\")\n",
        "\n",
        "previous_checkpoint_path = None\n",
        "\n",
        "for window in range(num_windows):\n",
        "    if moneyness:\n",
        "        train_x, train_y, val_x, val_y, test_x, test_y, train_start, val_start, test_start, df_test, train_strike, val_strike, test_strike = create_rw_dataset(df=df, window_number=window)\n",
        "    else:\n",
        "        train_x, train_y, val_x, val_y, test_x, test_y, train_start, val_start, test_start, df_test = create_rw_dataset(df=df, window_number=window)\n",
        "\n",
        "    print('-------------------------------------------')\n",
        "    print(f'Window {window + 1} of {num_windows}')\n",
        "    print(\"Training start: \", train_start, \"Validation start: \", val_start, \"Test start: \", test_start)\n",
        "    print(\"Loading previous checkpoint: \", previous_loading)\n",
        "        \n",
        "    if google_colab:\n",
        "        checkpoint_path = f'/content/drive/MyDrive/01. Masters Thesis - Shared/05. Checkpoints/{checkpoint_time}/{train_start}/'\n",
        "    else:\n",
        "        checkpoint_path = f'./checkpoints/{checkpoint_time}/{train_start}/'\n",
        "\n",
        "    config['Dataset'] = f'{train_start} - {val_start} - {test_start}'\n",
        "\n",
        "\n",
        "    trainer(train_x = train_x, train_y = train_y, val_x = val_x, val_y = val_y, config = config, project = 'Deep learning for option pricing - rolling windows',  previous_checkpoint_path = previous_checkpoint_path, checkpoint_path = checkpoint_path)\n",
        "    c_model = load_model(checkpoint_path + \".h5\")\n",
        "    predictions = np.array(c_model(test_x))\n",
        "\n",
        "    print(f'--- Predictions for test_start {test_start} ---')\n",
        "    calculate_error(predictions, test_y)\n",
        "    print('-------------------------------------------')\n",
        "    \n",
        "    df_test[\"Prediction\"] = predictions\n",
        "    df_test_combined = pd.concat([df_test_combined, df_test[[\"Quote_date\", \"Price\", \"Prediction\"] + bs_vars]])\n",
        "\n",
        "    if previous_loading:\n",
        "       previous_checkpoint_path = checkpoint_path\n",
        "       \n",
        "\n",
        "print(f\"--- All model predictions ---\")\n",
        "calculate_error(df_test_combined[\"Prediction\"], df_test_combined[\"Price\"])\n",
        "print(\"-------------------------------------------\")\n",
        "\n",
        "if google_colab == False:\n",
        "    predictions_path = './predictions/'\n",
        "    if checkpoint_path and not os.path.exists(predictions_path):\n",
        "        os.makedirs(predictions_path)\n",
        "    df_test_combined.to_csv(f'{predictions_path}{datetime.now().strftime(\"%m-%d_%H-%M\")}.csv')\n",
        "else:\n",
        "  path = '/content/drive/MyDrive/01. Masters Thesis - Shared/06. Predictions/10'\n",
        "  with open(path, 'w', encoding = 'utf-8-sig') as f:\n",
        "    df_test_combined.to_csv(f)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}